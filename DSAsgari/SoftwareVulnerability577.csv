"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","Cited by","DOI","Link","Abstract","Author Keywords","Index Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"Nguyen S.; Fu X.; Ogawa D.; Zheng Q.","Nguyen, Son (57191707007); Fu, Xiuju (7402205117); Ogawa, Daichi (58556760500); Zheng, Qin (56735209800)","57191707007; 7402205117; 58556760500; 56735209800","An application-oriented testing regime and multi-ship predictive modeling for vessel fuel consumption prediction","2023","Transportation Research Part E: Logistics and Transportation Review","177","","103261","","","","","10.1016/j.tre.2023.103261","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169297342&doi=10.1016%2fj.tre.2023.103261&partnerID=40&md5=53a7ab3641a26b25236ee7a41a5c4bec","Fuel consumption prediction (FCP) of vessels is the core of many decarbonization efforts by the maritime industry. Understanding the capability of machine learning (ML) FCP models is essential in various decision-making processes. However, the current model testing practice does not reflect their uncertainty and resilience in actual applications. To address this gap, this study proposes a testing regime that could provide insights into models’ behaviors, dependency on different features, and potential vulnerabilities to data uncertainties in the deployment phase. Two multi-ship FCP models were developed for testing, using extreme gradient boosting (XGB) and multi-layer perceptron artificial neural network (ANN) algorithms on noon reports of a container fleet operated globally in 2.5 years. Unlike previous studies, which explicitly indicated the superior ML algorithms, results from this study depicted a complicated situation with no decisive dominance of one algorithm over another, suggesting the potential of model combination and cooperation for optimal application performance. Aiding the FCP model development efforts, this study also includes findings regarding (1) the optimal configurations for ANN models, and (2) the reliance of FCP ML models and algorithms on different fuel consumption influencing factors. To our knowledge, this study is among the first to advocate a more comprehensive understanding of AI-based FCP models’ characteristics in realistic scenarios instead of simple selections based on accuracy indicators. © 2023 The Authors","Artificial neural network; Container shipping; Extreme gradient boosting; Fuel consumption prediction; Machine learning; Model testing; Testing scenarios","Adaptive boosting; Containers; Decision making; Fleet operations; Forecasting; Multilayer neural networks; Ships; Application-oriented; Container shippings; Extreme gradient boosting; Fuel consumption prediction; Gradient boosting; Machine learning algorithms; Machine-learning; Model testing; Prediction modelling; Testing scenario; algorithm; artificial neural network; container ship; fuel consumption; model test; prediction; shipping; software; vessel; Machine learning","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85169297342"
"Fu M.","Fu, Michael (57685110800)","57685110800","Toward More Effective Deep Learning-based Automated Software Vulnerability Prediction, Classification, and Repair","2023","Proceedings - International Conference on Software Engineering","","","","208","212","4","","10.1109/ICSE-Companion58688.2023.00057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171885706&doi=10.1109%2fICSE-Companion58688.2023.00057&partnerID=40&md5=c6b77f33f696b967e19236aa91e32119","Software vulnerabilities are prevalent in software systems and the unresolved vulnerable code may cause system failures or serious data breaches. To enhance security and prevent potential cyberattacks on software systems, it is critical to (1) early detect vulnerable code, (2) identify its vulnerability type, and (3) suggest corresponding repairs. Recently, deep learning-based approaches have been proposed to predict those tasks based on source code. In particular, software vulnerability prediction (SVP) detects vulnerable source code; software vulnerability clas-sification (SVC) identifies vulnerability types to explain detected vulnerable programs; neural machine translation (NMT)-based automated vulnerability repair (AVR) generates patches to repair detected vulnerable programs. However, existing SVPs require much effort to inspect their coarse-grained predictions; SVCs encounter an unresolved data imbalance issue; AVRs are still inaccurate. I hypothesize that by addressing the limitations of existing SVPs, SVCs and AVRs, we can improve the accuracy and effectiveness of DL-based approaches for the aforementioned three prediction tasks. To test this hypothesis, I will propose (1) a finer-grained SVP approach that can point out vulnerabilities at the line level; (2) an SVC approach that mitigates the data imbalance issue; (3) NMT-based AVR approaches to address limitations of previous NMT-based approaches. Finally, I propose integrating these novel approaches into an open-source software security framework to promote the adoption of the DL-powered security tool in the industry. © 2023 IEEE.","Cybersecurity; Software Security; Software Vulnerability","Deep learning; Open source software; Open systems; Repair; Cyber security; Cyber-attacks; Data imbalance; Learning-based approach; Software security; Software vulnerabilities; Software-systems; Source codes; System failures; Task-based; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85171885706"
"de Oliveira B.G.; Endo A.T.; Vergilio S.R.","de Oliveira, Bruno Gonçalves (57195312056); Endo, Andre Takeshi (25924829800); Vergilio, Silvia Regina (6506290092)","57195312056; 25924829800; 6506290092","Characterizing Security-Related Commits of JavaScript Engines","2023","International Conference on Enterprise Information Systems, ICEIS - Proceedings","2","","","86","97","11","","10.5220/0011966100003467","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160853182&doi=10.5220%2f0011966100003467&partnerID=40&md5=4cd2ea9f5e57c8ad76cae7328dc8c68d","JavaScript engines are security-critical components of Web browsers. Their different features bring challenges for practitioners that intend to detect and remove vulnerabilities. As these JavaScript engines are open-source projects, security insights could be drawn by analyzing the changes performed by developers. This paper aims to characterize security-related commits of open-source JavaScript engines. We identified and analyzed commits that involve some security aspects; they were selected from the widely used engines: V8, ChakraCore, JavaScriptCore, and Hermes. We compared the security-related commits with other commits using source code metrics and assessed how security-related commits modify specific modules of JavaScript engines. Finally, we classified a subset of commits and related them to potential vulnerabilities. The results showed that only six out of 44 metrics adopted in the literature are statistically different when comparing security-related commits to the others, for all engines. We also observed what files and, consequently, the modules, are more security-related modified. Certain vulnerabilities are more connected to security-related commits, such as Generic Crash, Type Confusion, Generic Leak, and Out-of-Bounds. The obtained results may help to advance vulnerability prediction and fuzzing of JavaScript engines, augmenting the security of the Internet. Copyright © 2023 by SCITEPRESS – Science and Technology Publications, Lda. Under CC license (CC BY-NC-ND 4.0)","JavaScript; Metrics; Mining Repositories; Security; Software Vulnerability","High level languages; Information systems; Information use; Open source software; Web browsers; Critical component; Javascript; Metric; Mining repositories; Open source projects; Open-source; Security; Security aspects; Security-critical; Software vulnerabilities; Engines","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85160853182"
"Kazemi F.; Jankowski R.","Kazemi, F. (57217047347); Jankowski, R. (7102473802)","57217047347; 7102473802","Machine learning-based prediction of seismic limit-state capacity of steel moment-resisting frames considering soil-structure interaction","2023","Computers and Structures","274","","106886","","","","","10.1016/j.compstruc.2022.106886","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139830049&doi=10.1016%2fj.compstruc.2022.106886&partnerID=40&md5=89ae5129e1f39e460916e7ad92deaffb","Regarding the unpredictable and complex nature of seismic excitations, there is a need for vulnerability assessment of newly constructed or existing structures. Predicting the seismic limit-state capacity of steel Moment-Resisting Frames (MRFs) can help designers to have a preliminary estimation and improve their views about the seismic performance of the designed structure. This study improved data-driven decision techniques in Python software, known as supervised Machine Learning (ML) algorithms, to find median IDA curves (M-IDAs) for predicting the seismic limit-state capacities of steel MRFs considering Soil-Structure Interaction (SSI) effects. For this purpose, Incremental Dynamic Analyses (IDAs) were performed on the steel MRFs from two to nine-story elevations modeled in Opensees subjected to three ground motion subsets of Far Fault (FF), near-fault Pulse-Like (PL) and No-Pulse (NP) suggested by FEMA-P695. The result of the analysis confirmed that there is no specific model for predicting the M-IDA curve of steel structures; therefore, the best developed ML algorithms to reduce a complex modeling process with high computational cost using 128,000 data points were proposed. To provide convenient access to prediction results, Graphical User Interface (GUI) was developed to predict Sa(T1) of seismic limit-state performance levels with a large database based on prediction models. © 2022 Elsevier Ltd","Data-driven decision techniques; Machine learning algorithm; Seismic limit-state capacity; Seismic vulnerability assessment; Soil-structure interaction; Supervised learning","Computer software; Graphical user interfaces; Interface states; Learning algorithms; Soil structure interactions; Soils; Structural frames; Supervised learning; Data driven decision; Data-driven decision technique; Limit state; Machine learning algorithms; Seismic limit-state capacity; Seismic vulnerability; Seismic vulnerability assessment; Soil-structure interaction; Steel moment resisting frame; Vulnerability assessments; Forecasting","Article","Final","","Scopus","2-s2.0-85139830049"
"Vermani K.; Noliya A.; Kumar S.; Dutta K.","Vermani, Kunal (58686249600); Noliya, Amandeep (57214154391); Kumar, Sunil (57220592072); Dutta, Kamlesh (36871512100)","58686249600; 57214154391; 57220592072; 36871512100","Ensemble Learning Based Malicious Node Detection in SDN-Based VANETs","2023","Journal of Information Systems Engineering and Business Intelligence","9","2","","136","146","10","","10.20473/jisebi.9.2.136-146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176128441&doi=10.20473%2fjisebi.9.2.136-146&partnerID=40&md5=a3604b7cbe1686073ca42fa4e3e70ad4","Background: The architecture of Software Defined Networking (SDN) integrated with Vehicular Ad-hoc Networks (VANETs) is considered a practical method for handling large-scale, dynamic, heterogeneous vehicular networks, since it offers flexibility, programmability, scalability, and a global understanding. However, the integration with VANETs introduces additional security vulnerabilities due to the deployment of a logically centralized control mechanism. These security attacks are classified as internal and external based on the nature of the attacker. The method adopted in this work facilitated the detection of internal position falsification attacks. Objective: This study aimed to investigate the performance of k-NN, SVM, Naïve Bayes, Logistic Regression, and Random Forest machine learning (ML) algorithms in detecting position falsification attacks using the Vehicular Reference Misbehavior (VeReMi) dataset. It also aimed to conduct a comparative analysis of two ensemble classification models, namely voting and stacking for final decision-making. These ensemble classification methods used the ML algorithms cooperatively to achieve improved classification. Methods: The simulations and evaluations were conducted using the Python programming language. VeReMi dataset was selected since it was an application-specific dataset for VANETs environment. Performance evaluation metrics, such as accuracy, precision, recall, F-measure, and prediction time were also used in the comparative studies. Results: This experimental study showed that Random Forest ML algorithm provided the best performance in detecting attacks among the ML algorithms. Voting and stacking were both used to enhance classification accuracy and reduce time required to identify an attack through predictions generated by k-NN, SVM, Naïve Bayes, Logistic Regression, and Random Forest classifiers. Conclusion: In terms of attack detection accuracy, both methods (voting and stacking) achieved the same level of accuracy as Random Forest. However, the detection of attack using stacking could be achieved in roughly less than half the time required by voting ensemble. © 2023 The Authors. Published by Universitas Airlangga.","Machine learning methods; Majority voting ensemble; SDN-based VANETs; Security attacks; Stacking ensemble classifiers; VANETs","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85176128441"
"Sun J.; Xing Z.; Lu Q.; Xu X.; Zhu L.; Hoang T.; Zhao D.","Sun, Jiamou (57205023522); Xing, Zhenchang (8347413500); Lu, Qinghua (56431802100); Xu, Xiwei (55706225900); Zhu, Liming (57191568259); Hoang, Thong (57210914382); Zhao, Dehai (56814028500)","57205023522; 8347413500; 56431802100; 55706225900; 57191568259; 57210914382; 56814028500","Silent Vulnerable Dependency Alert Prediction with Vulnerability Key Aspect Explanation","2023","Proceedings - International Conference on Software Engineering","","","","970","982","12","","10.1109/ICSE48619.2023.00089","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170829992&doi=10.1109%2fICSE48619.2023.00089&partnerID=40&md5=dffa9e915ce5194ef2fd05eea8bec372","Due to convenience, open-source software is widely used. For beneficial reasons, open-source maintainers often fix the vulnerabilities silently, exposing their users unaware of the updates to threats. Previous works all focus on black-box binary detection of the silent dependency alerts that suffer from high false-positive rates. Open-source software users need to analyze and explain AI prediction themselves. Explainable AI becomes remarkable as a complementary of black-box AI models, providing details in various forms to explain AI decisions. Noticing there is still no technique that can discover silent dependency alert on time, in this work, we propose a framework using an encoder-decoder model with a binary detector to provide explainable silent dependency alert prediction. Our model generates 4 types of vulnerability key aspects including vulnerability type, root cause, attack vector, and impact to enhance the trustworthiness and users' acceptance to alert prediction. By experiments with several models and inputs, we confirm CodeBERT with both commit messages and code changes achieves the best results. Our user study shows that explainable alert predictions can help users find silent dependency alert more easily than black-box predictions. To the best of our knowledge, this is the first research work on the application of Explainable AI in silent dependency alert prediction, which opens the door of the related domains. © 2023 IEEE.","","Network security; Open source software; Open systems; Attack vector; Binary detection; Black boxes; Encoder-decoder; False positive rates; Open-source; Open-source softwares; Root cause; User need; Users' acceptance; Forecasting","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85170829992"
"Kazemi F.; Asgarkhani N.; Jankowski R.","Kazemi, F. (57217047347); Asgarkhani, N. (57215197930); Jankowski, R. (7102473802)","57217047347; 57215197930; 7102473802","Machine learning-based seismic fragility and seismic vulnerability assessment of reinforced concrete structures","2023","Soil Dynamics and Earthquake Engineering","166","","107761","","","","","10.1016/j.soildyn.2023.107761","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145648277&doi=10.1016%2fj.soildyn.2023.107761&partnerID=40&md5=0229f34e5a80028114fb35bf0e8a077a","Many studies have been performed to put quantifying uncertainties into the seismic risk assessment of reinforced concrete (RC) buildings. This paper provides a risk-assessment support tool for purpose of retrofitting and potential design strategies of RC buildings. Machine Learning (ML) algorithms were developed in Python software by innovative methods of hyperparameter optimization, such as halving search, grid search, random search, fine-tuning method, and the k-fold cross-validation, to derive the seismic fragility curve for accelerating seismic risk assessment. Proposed ML methods significantly reduced the computational efforts compared to conventional procedure of seismic fragility assessment. The prediction results can be combined with considered hazard curves for the purpose of seismic risk assessment of RC buildings. To prepare the training dataset, Incremental Dynamic Analyses (IDAs) were performed on 165 RC frames to achieve 1121184 data points. Performance indicators showed that the algorithms of Artificial Neural Networks (ANNs), Extra-Trees Regressor (ETR), Extremely Randomized Tree Regressor (ERTR), Bagging Regressor (BR), Extreme Gradient Boosting (XGBoost), and Histogram-based Gradient Boosting Regression (HGBR) had higher performance, which achieved acceptable accuracy and fitted to actual curves. In addition, Graphical User Interface (GUI) was introduced as a practical tool yet reliable for seismic risk assessment of RC buildings. © 2023 Elsevier Ltd","Machine learning method; Reinforced concrete; Seismic fragility curve prediction; Seismic limit-state capacity; Seismic risk assessment; Seismic vulnerability assessment","Adaptive boosting; Computer software; Concrete buildings; Curve fitting; Graphical user interfaces; Machine learning; Neural networks; Python; Reinforced concrete; Seismology; Curve prediction; Limit state; Machine learning methods; Seismic fragility curve prediction; Seismic fragility curves; Seismic limit-state capacity; Seismic risk assessment; Seismic vulnerability; Seismic vulnerability assessment; Vulnerability assessments; concrete structure; dynamic analysis; machine learning; prediction; reinforced concrete; risk assessment; seismic hazard; seismic response; vulnerability; Risk assessment","Article","Final","","Scopus","2-s2.0-85145648277"
"Zhou X.; Duan B.; Wu X.; Wang P.","Zhou, Xu (56373491500); Duan, Bingjie (57867027700); Wu, Xugang (57202256517); Wang, Pengfei (57196033363)","56373491500; 57867027700; 57202256517; 57196033363","SAViP: Semantic-Aware Vulnerability Prediction for Binary Programs with Neural Networks","2023","Applied Sciences (Switzerland)","13","4","2271","","","","","10.3390/app13042271","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149264933&doi=10.3390%2fapp13042271&partnerID=40&md5=7326a71e7f93b0b6e7ac6a94f04c65a2","Vulnerability prediction, in which static analysis is leveraged to predict the vulnerabilities of binary programs, has become a popular research topic. Traditional vulnerability prediction methods depend on vulnerability patterns, which must be predefined by security experts in a time-consuming manner. The development of Artificial Intelligence (AI) has yielded new options for vulnerability prediction. Neural networks allow vulnerability patterns to be learned automatically. However, current works extract only one or two types of features and use traditional models such as word2vec, which results in the loss of much instruction-level information. In this paper, we propose a model named SAViP to predict vulnerabilities in binary programs. To fully extract binary information, we integrate three kinds of features: semantic, statistical, and structural features. For semantic features, we apply the Masked Language Model (MLM) pre-training task of the RoBERTa model to the assembly code to build our language model. Using this model, we innovatively combine the beginning token and the operation-code token to create the instruction embedding. For the statistical features, we design a 56-dimensional feature vector that contains 43 kinds of instructions. For the structural features, we improve the ability of the structure2vec network to obtain the characteristic of the network by emphasizing node self-attention. Through these optimizations, we significantly increase the accuracy of vulnerability prediction over existing methods. Our experiments show that SAViP achieves a recall of 77.85% and Top 100∼600 accuracies all above 95%. The results are 10% and 13% higher than those of the state-of-the-art V-Fuzz, respectively. © 2023 by the authors.","binary program; neural networks; software security; vulnerability prediction","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85149264933"
"Suneja S.; Zhuang Y.; Zheng Y.; Laredo J.; Morari A.; Khurana U.","Suneja, Sahil (55813198500); Zhuang, Yufan (57219684931); Zheng, Yunhui (55901243300); Laredo, Jim (57197580620); Morari, Alessandro (51665633100); Khurana, Udayan (8857468700)","55813198500; 57219684931; 55901243300; 57197580620; 51665633100; 8857468700","Incorporating Signal Awareness in Source Code Modeling: An Application to Vulnerability Detection","2023","ACM Transactions on Software Engineering and Methodology","32","6","145","","","","","10.1145/3597202","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175296686&doi=10.1145%2f3597202&partnerID=40&md5=412506514efa93e5984a2dfc5041bcb4","AI models of code have made significant progress over the past few years. However, many models are actually not learning task-relevant source code features. Instead, they often fit non-relevant but correlated data, leading to a lack of robustness and generalizability, and limiting the subsequent practical use of such models. In this work, we focus on improving the model quality through signal awareness, i.e., learning the relevant signals in the input for making predictions. We do so by leveraging the heterogeneity of code samples in terms of their signal-to-noise content. We perform an end-to-end exploration of model signal awareness, comprising: (i) uncovering the reliance of AI models of code on task-irrelevant signals, via prediction-preserving input minimization; (ii) improving models' signal awareness by incorporating the notion of code complexity during model training, via curriculum learning; (iii) improving models' signal awareness by generating simplified signal-preserving programs and augmenting them to the training dataset; and (iv) presenting a novel interpretation of the model learning behavior from the perspective of the dataset, using its code complexity distribution. We propose a new metric to measure model signal awareness, Signal-aware Recall, which captures how much of the model's performance is attributable to task-relevant signal learning. Using a software vulnerability detection use-case, our model probing approach uncovers a significant lack of signal awareness in the models, across three different neural network architectures and three datasets. Signal-aware Recall is observed to be in the sub-50s for models with traditional Recall in the high 90s, suggesting that the models are presumably picking up a lot of noise or dataset nuances while learning their logic. With our code-complexity-aware model learning enhancement techniques, we are able to assist the models toward more task-relevant learning, recording up-to 4.8× improvement in model signal awareness. Finally, we employ our model learning introspection approach to uncover the aspects of source code where the model is facing difficulty, and we analyze how our learning enhancement techniques alleviate it. © 2023 Copyright held by the owner/author(s).","Additional Key Words and PhrasesMachine learning; curriculum learning; data augmentation; explainability; neural networks; reliability; signal awareness","Codes (symbols); Complex networks; Computer programming languages; Learning systems; Network architecture; Network coding; Neural networks; Signal to noise ratio; Additional key word and phrasesmachine learning; Curriculum learning; Data augmentation; Explainability; Key words; Model signals; Neural-networks; Signal awareness; Source codes; Task relevant; Curricula","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85175296686"
"Yin J.; Tang M.; Cao J.; You M.; Wang H.; Alazab M.","Yin, Jiao (54884588500); Tang, MingJian (57215896761); Cao, Jinli (7403353999); You, Mingshan (57215898907); Wang, Hua (57215111932); Alazab, Mamoun (36661792200)","54884588500; 57215896761; 7403353999; 57215898907; 57215111932; 36661792200","Knowledge-Driven Cybersecurity Intelligence: Software Vulnerability Coexploitation Behavior Discovery","2023","IEEE Transactions on Industrial Informatics","19","4","","5593","5601","8","","10.1109/TII.2022.3192027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135205750&doi=10.1109%2fTII.2022.3192027&partnerID=40&md5=39eb7659eb86818259ca1cfad719452b","Coexploitation behavior, referring to multiple software vulnerabilities being exploited jointly by one or more exploits, brings enormous challenges to the prevention and remediation of cyberattacks. Leveraging the latest advances in graph-driven intelligence, this article formulates vulnerability coexploitation behavior discovery as a link prediction problem between vulnerability entities within a vulnerability knowledge graph. We propose a modality-aware graph convolutional network (MAGCN) module to embed multimodality entity attributes and topological graph connectivity features into a unified lower dimensional feature space to boost link prediction performance. We further design a graph knowledge transfer learning (GKTL) strategy to transfer knowledge between subgraphs extracted from the same knowledge graph. Experimental results on a real-world dataset containing coexploitation incidents between 1995 and 2021 show that MAGCN achieved 81.34% on the F1 score when applying the GKTL strategy, superior to other graph neural network modules, such as GCN, GraphSAGE, EdgeGCN, and GINGCN.  © 2005-2012 IEEE.","Coexploitation discovery; graph embedding; knowledge graph (KG); link prediction; transfer learning","Forecasting; Job analysis; Knowledge management; Network security; Topology; Australia; Co-exploitation discovery; Features extraction; Graph embeddings; Informatics; Knowledge graphs; Link prediction; Software; Task analysis; Transfer learning; Knowledge graph","Article","Final","","Scopus","2-s2.0-85135205750"
"Won D.-O.; Jang Y.-N.; Lee S.-W.","Won, Dong-Ok (56131784400); Jang, Yong-Nam (57671192800); Lee, Seong-Whan (7601390519)","56131784400; 57671192800; 7601390519","PlausMal-GAN: Plausible Malware Training Based on Generative Adversarial Networks for Analogous Zero-Day Malware Detection","2023","IEEE Transactions on Emerging Topics in Computing","11","1","","82","94","12","","10.1109/TETC.2022.3170544","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129596433&doi=10.1109%2fTETC.2022.3170544&partnerID=40&md5=bb4d8339bc3a6ec7014b4427d4a3f4f1","Zero-day malicious software (malware) refers to a previously unknown or newly discovered software vulnerability. The fundamental objective of this paper is to enhance detection for analogous zero-day malware by efficient learning to plausible generated data. To detect zero-day malware, we proposed a malware training framework based on the generated analogous malware data using generative adversarial networks (PlausMal-GAN). Thus, the PlausMal-GAN can suitably produce analogous zero-day malware images with high quality and high diversity from the existing malware data. The discriminator, as a detector, learns various malware features using both real and generated malware images. In terms of performance, the proposed framework showed higher and more stable performances for the analogous zero-day malware images, which can be assumed to be analogous zero-day malware data. We obtained reliable accuracy performances in the proposed PlausMal-GAN framework with representative GAN models (i.e., deep convolutional GAN, least-squares GAN, Wasserstein GAN with gradient penalty, and evolutionary GAN). These results indicate that the use of the proposed framework is beneficial for the detection and prediction of numerous and analogous zero-day malware data from noted malware when developing and updating malware detection systems.  © 2013 IEEE.","Analogous malware detection; generative adversarial networks; malware augmentation; malware data; zero-day malware","Big data; Generative adversarial networks; Linear programming; Zero-day attack; Analogous malware detection; Generator; Linear-programming; Malware augmentation; Malware data; Malware detection; Malwares; Training data; Zero-day malware; Malware","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85129596433"
"Kranthi S.; Kanchana M.; Suneetha M.","Kranthi, S. (57191371178); Kanchana, M. (57216477082); Suneetha, M. (58033193900)","57191371178; 57216477082; 58033193900","An intelligent intrusion prediction and prevention system for software defined internet of things cloud networks","2023","Peer-to-Peer Networking and Applications","16","1","","210","225","15","","10.1007/s12083-022-01374-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139410368&doi=10.1007%2fs12083-022-01374-9&partnerID=40&md5=06a34c63008d3f7adffd365aa2b75cbd","Securing digital data transmission or communication is essential for the growing smart cities. At the same, malicious vulnerabilities are also enhanced to break the security. Hence, securing data in a wireless or innovative environment is challenging. So, the current research article has aimed to design novel chimp-based Auto-encoder Networks (CbAN) for the Software-Defined-Network (SDN) Internet-of-Things (IoT) cloud network. Moreover, the SDN with Distributed-Denial-of-Service (DDoS) attack database has been considered in this research. Initially, the datasets were arranged in the tree structure then the error neglecting process was performed. Consequently, the error-less data is imported from the classification module of the auto-encoder tree. Incorporating the chimp fitness solution has offered a better feature extraction and classification outcome. After classifying the malicious features, it is neglected in the SDN environment. Finally, the proposed novel CbAN scheme has been executed in the python platform and has earned outstanding results than the previous work by yielding the highest intrusion forecasting accuracy. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Distributed-denial-of-service; Intrusion prevention; IoT users; Malicious and benign; Software-define-network","Classification (of information); Computer software; Denial-of-service attack; Forestry; Network coding; Network security; Trees (mathematics); Auto encoders; Cloud networks; Distributed denial of service; Internet-of-thing user; Intrusion prevention; Malicious and benign; Prediction systems; Prevention systems; Software-define-network; Software-defined networks; Internet of things","Article","Final","","Scopus","2-s2.0-85139410368"
"Luo Z.; Wang P.; Xie W.; Zhou X.; Wang B.","Luo, Zhenhao (57212578071); Wang, Pengfei (57196033363); Xie, Wei (57199001040); Zhou, Xu (56373491500); Wang, Baosheng (8250081400)","57212578071; 57196033363; 57199001040; 56373491500; 8250081400","IoTSim: Internet of Things-Oriented Binary Code Similarity Detection with Multiple Block Relations","2023","Sensors","23","18","7789","","","","","10.3390/s23187789","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173098547&doi=10.3390%2fs23187789&partnerID=40&md5=9b3a810047eed06ce8fd2cdbc7130b1e","Binary code similarity detection (BCSD) plays a crucial role in various computer security applications, including vulnerability detection, malware detection, and software component analysis. With the development of the Internet of Things (IoT), there are many binaries from different instruction architecture sets, which require BCSD approaches robust against different architectures. In this study, we propose a novel IoT-oriented binary code similarity detection approach. Our approach leverages a customized transformer-based language model with disentangled attention to capture relative position information. To mitigate out-of-vocabulary (OOV) challenges in the language model, we introduce a base-token prediction pre-training task aimed at capturing basic semantics for unseen tokens. During function embedding generation, we integrate directed jumps, data dependency, and address adjacency to capture multiple block relations. We then assign different weights to different relations and use multi-layer Graph Convolutional Networks (GCN) to generate function embeddings. We implemented the prototype of IoTSim. Our experimental results show that our proposed block relation matrix improves IoTSim with large margins. With a pool size of (Formula presented.), IoTSim achieves a recall@1 of 0.903 across architectures, outperforming the state-of-the-art approaches Trex, SAFE, and PalmTree. © 2023 by the authors.","binary code similarity detection; IoT security; vulnerability detection","Application programs; Binary codes; Computational linguistics; Embeddings; Network architecture; Network layers; Semantics; Binary code similarity detection; Code similarities; Computer security applications; Detection approach; Embeddings; Internet of thing security; Language model; Malware detection; Similarity detection; Vulnerability detection; article; attention; embedding; human; human experiment; internet of things; language model; prediction; recall; security; semantics; vocabulary; vulnerability; Internet of things","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85173098547"
"Guo J.; Long K.; Yang K.; Jiang K.; Lu L.; Wang C.","Guo, Junjun (55978093400); Long, Kai (57855746100); Yang, Kunpeng (58242293300); Jiang, Kaida (58242515300); Lu, Long (58241606100); Wang, Changyuan (36651948200)","55978093400; 57855746100; 58242293300; 58242515300; 58241606100; 36651948200","A novel prediction method for vulnerability outbreak trend","2023","Computers and Electrical Engineering","108","","108743","","","","","10.1016/j.compeleceng.2023.108743","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158869418&doi=10.1016%2fj.compeleceng.2023.108743&partnerID=40&md5=87dba918801912b1c2d60c032e71b81b","The trend prediction of software vulnerability can provide valuable threat intelligence in security event prevention. It is a challenging task for highly accurate prediction. To address this problem, a novel prediction method, STL-EEMD-ARIMA, is proposed, by incorporating Seasonal and Trend Decomposition using Loess (STL), Ensemble Empirical Mode Decomposition (EEMD), and Autoregressive Integrated Moving Average (ARIMA). Firstly, the trend and random fluctuation components are extracted from the vulnerability trend samples by using the STL method, respectively. Secondly, we use EEMD to decompose the trend and the residual to achieve Intrinsic Mode Functions (IMFs). Then, the IMFs are classified into high- and low-frequency components based on the data characteristics. Finally, the ultimate prediction result is obtained by ARIMA. The experimental results of the proposed method illustrate that the absolute error rate decreases by an average of 4.22% compared with the naive EEMD-ARIMA method. © 2023 The Author(s)","Prediction; Seasonal and trend decomposition; STL-EEMD-ARIMA; Vulnerability","Forecasting; Auto-regressive; Empirical Mode Decomposition; Intrinsic Mode functions; Moving averages; Prediction methods; Seasonal and trend decomposition; Software vulnerabilities; STL-ensemble empirical mode decomposition-autoregressive integrated moving average; Trend prediction; Vulnerability; Empirical mode decomposition","Article","Final","","Scopus","2-s2.0-85158869418"
"Pan S.; Bao L.; Xia X.; Lo D.; Li S.","Pan, Shengyi (57987555200); Bao, Lingfeng (56609745200); Xia, Xin (58275220100); Lo, David (35269388000); Li, Shanping (35275218400)","57987555200; 56609745200; 58275220100; 35269388000; 35275218400","Fine-grained Commit-level Vulnerability Type Prediction by CWE Tree Structure","2023","Proceedings - International Conference on Software Engineering","","","","957","969","12","","10.1109/ICSE48619.2023.00088","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171773162&doi=10.1109%2fICSE48619.2023.00088&partnerID=40&md5=97c94ce0a59a2f54e55b5c757057e046","Identifying security patches via code commits to allow early warnings and timely fixes for Open Source Software (OSS) has received increasing attention. However, the existing detection methods can only identify the presence of a patch (i.e., a binary classification) but fail to pinpoint the vulnerability type. In this work, we take the first step to categorize the security patches into fine-grained vulnerability types. Specifically, we use the Common Weakness Enumeration (CWE) as the label and perform fine-grained classification using categories at the third level of the CWE tree. We first formulate the task as a Hierarchical Multi-label Classification (HMC) problem, i.e., inferring a path (a sequence of CWE nodes) from the root of the CWE tree to the node at the target depth. We then propose an approach named TreeVul with a hierarchical and chained architecture, which manages to utilize the structure information of the CWE tree as prior knowledge of the classification task. We further propose a tree structure aware and beam search based inference algorithm for retrieving the optimal path with the highest merged probability. We collect a large security patch dataset from NVD, consisting of 6,541 commits from 1,560 GitHub OSS repositories. Experimental results show that Tree-vulsignificantly outperforms the best performing baselines, with improvements of 5.9%, 25.0%, and 7.7% in terms of weighted F1-score, macro F1-score, and MCC, respectively. We further conduct a user study and a case study to verify the practical value of TreeVul in enriching the binary patch detection results and improving the data quality of NVD, respectively. © 2023 IEEE.","CWE; Software Security; Vulnerability Type","Binary trees; Classification (of information); Inference engines; Large dataset; Open systems; Trees (mathematics); Common weakness enumeration; Enumeration trees; F1 scores; Fine grained; Open-source softwares; Security patches; Software security; Tree structures; Type predictions; Vulnerability type; Open source software","Conference paper","Final","","Scopus","2-s2.0-85171773162"
"Luo Z.; Zhang Y.; Wang Q.; Song W.","Luo, Zhiyong (57004112800); Zhang, Yu (58355316200); Wang, Qing (58355316300); Song, Weiwei (58355961100)","57004112800; 58355316200; 58355316300; 58355961100","Study of SDN intrusion intent identification algorithm based on Bayesian attack graph; [基于贝叶斯攻击图的 SDN 入侵意图识别算法的研究]","2023","Tongxin Xuebao/Journal on Communications","44","4","","216","225","9","","10.11959/j.issn.1000-436x.2023073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163036590&doi=10.11959%2fj.issn.1000-436x.2023073&partnerID=40&md5=ad94bef1501dc96b2301460dc9b03a60","Since the existing software defined network (SDN) security prediction methods do not consider the attack cost and the impact of controller vulnerabilities on SDN security, a Bayesian attack graph-based algorithm to assessing SDN intrusion intent was proposed. The PageRank algorithm was used to obtain the criticality of the device, and combining with the vulnerability value, attack cost, attack benefit and attack preference, an attack graph was constructed, and a risk assessment model was established to predict the intrusion path. Through experimental comparison, it is obvious that the proposed model can more accurately predict the intrusion path, effectively ensure the accuracy of security prediction, and provide a basis for SDN defense. © 2023 Editorial Board of Journal on Communications. All rights reserved.","attack graph; intrusion intention; PageRank algorithm; SDN security prediction","Graphic methods; Network security; Risk assessment; Attack graph; Bayesian; Identification algorithms; Intrusion intention; Network intrusions; Networks security; PageRank algorithm; Prediction methods; Software defined network security prediction; Software-defined networks; Forecasting","Article","Final","","Scopus","2-s2.0-85163036590"
"Chen W.; Ren Y.; Xiao Y.; Hou R.; Tian Z.","Chen, Weixiang (57211431003); Ren, Yitong (57214911758); Xiao, Yanjun (57826232300); Hou, Rui (56435520600); Tian, Zhihong (9636602700)","57211431003; 57214911758; 57826232300; 56435520600; 9636602700","A Research on Attack-path Prediction Method for APT Organization; [面向 APT 家族分析的攻击路径预测方法研究]","2023","Journal of Cyber Security","8","1","","1","13","12","","10.19363/J.cnki.cn10-1380/tn.2023.01.01","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160415600&doi=10.19363%2fJ.cnki.cn10-1380%2ftn.2023.01.01&partnerID=40&md5=fed77ef7494749d33036f9c1be5098b9","In recent years, attacks against government agencies, industrial facilities and large corporate networks have emerged one after another. Cyberspace Security has become an overall issue related to national stability, social stability and economic prosperity. Advanced persistent threat (APT) has gradually evolved into a complex of various social engineering attacks and zero-day vulnerability exploitation, and has become one of the most serious cyberspace security threats. The current research on APT focuses on finding reliable attack features and improving detection accuracy. Due to the complex and huge data, it is easy to hide APT features, it makes it more difficult to obtain reliable attack features. How to find APT attacks as soon as possible and attribute to the source of APT family is a hot issue for researchers. Based on this, this paper proposes an APT attack path restoration and prediction method. Firstly, referring to the idea of software gene, the APT malware gene model and gene similarity detection algorithm are designed to construct the malicious behavior gene library. The samples are genetically detected through the malicious behavior gene library to extract reliable malicious features and solve the problem of reliable data acquisition. Secondly, in order to solve the problem of APT attack path restoration and prediction, hidden Markov model (HMM) is used to restore and predict the attack path of APT malicious behavior chain. The characteristics generated by malicious behavior gene library are used to construct the malicious behavior chain and estimate the model parameters, and then restore and predict the APT attack path. The prediction accuracy can reach more than 90%. Finally, the family identification of malware is carried out by HMM and gene detection. The experimental results show that the gene characteristics and HMM parameter characteristics can guide the intrusion detection system to identify and classify malware to a certain extent. © 2023 Journal of Propulsion Technology. All rights reserved.","APT attack; attack path reconstruction and prediction; HMM; malware family classification; software gene","","Article","Final","","Scopus","2-s2.0-85160415600"
"Poozhithara J.J.; Asuncion H.U.; Lagesse B.","Poozhithara, Jeffy Jahfar (57219788014); Asuncion, Hazeline U. (23134725400); Lagesse, Brent (22957859100)","57219788014; 23134725400; 22957859100","Keyword Extraction From Specification Documents for Planning Security Mechanisms","2023","Proceedings - International Conference on Software Engineering","","","","1661","1673","12","","10.1109/ICSE48619.2023.00143","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171774776&doi=10.1109%2fICSE48619.2023.00143&partnerID=40&md5=cea2b495f687c7b793244cbd855d4b99","Software development companies heavily invest both time and money to provide post-production support to fix security vulnerabilities in their products. Current techniques identify vulnerabilities from source code using static and dynamic analyses. However, this does not help integrate security mechanisms early in the architectural design phase. We develop VDocScan, a technique for predicting vulnerabilities based on specification documents, even before the development stage. We evaluate VDocScan using an extensive dataset of CVE vulnerability reports mapped to over 3600 product documentations. An evaluation of 8 CWE vulnerability pillars shows that even interpretable whitebox classifiers predict vulnerabilities with up to 61.1% precision and 78% recall. Further, using strategies to improve the relevance of extracted keywords, addressing class imbalance, segregating products into categories such as Operating Systems, Web applications, and Hardware, and using blackbox ensemble models such as the random forest classifier improves the performance to 96% precision and 91.1% recall. The high precision and recall shows that VDocScan can anticipate vulnerabilities detected in a product's lifetime ahead of time during the Design phase to incorporate necessary security mechanisms. The performance is consistently high for vulnerabilities with the mode of introduction: architecture and design. © 2023 IEEE.","CVE; CWE; Documentation; Keyword Extraction; Security; Vulnerability Prediction","Extraction; Forestry; Product design; Software design; Specifications; CVE; CWE; Design phase; Documentation; Keywords extraction; Performance; Security; Security mechanism; Specifications document; Vulnerability prediction; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85171774776"
"Wang X.; Hui T.; Zhao L.; Cheng Y.","Wang, Xiaoke (57798804600); Hui, Tao (57702865500); Zhao, Lei (56461651900); Cheng, Yueqiang (36336830100)","57798804600; 57702865500; 56461651900; 36336830100","Input-Driven Dynamic Program Debloating for Code-Reuse Attack Mitigation","2023","ESEC/FSE 2023 - Proceedings of the 31st ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering","","","","934","946","12","","10.1145/3611643.3616274","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180551271&doi=10.1145%2f3611643.3616274&partnerID=40&md5=09b70910df5593c4d220a28ade3683d1","Modern software is bloated, especially for libraries. The unnecessary code not only brings severe vulnerabilities, but also assists attackers to construct exploits. To mitigate the damage of bloated libraries, researchers have proposed several debloating techniques to remove or restrict the invocation of unused code in a library. However, existing approaches either statically keep code for all expected inputs, which leave unused code for each concrete input, or rely on runtime context to dynamically determine the necessary code, which could be manipulated by attackers. In this paper, we propose Picup, a practical approach that dynamically customizes libraries for each input. Based on the observation that the behavior of a program mainly depends on the given input, we design Picup to predict the necessary library functions immediately after we get the input, which erases the unused code before attackers can affect the decision-making data. To achieve an effective prediction, we adopt a convolutional neural network (CNN) with attention mechanism to extract key bytes from the input and map them to library functions. We evaluate Picup on real-world benchmarks and popular applications. The results show that we can predict the necessary library functions with 97.56% accuracy, and reduce the code size by 87.55% on average with low overheads. These results indicate that Picup is a practical solution for secure and effective library debloating. © 2023 ACM.","attack mitigation; code debloating; software security","Benchmarking; Computer software reusability; Decision making; Forecasting; Attack mitigation; Attention mechanisms; Code debloating; Code reuse; Convolutional neural network; Decisions makings; Dynamic programs; Library functions; Runtime context; Software security; Libraries","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85180551271"
"Chow Y.W.; Schäfer M.; Pradel M.","Chow, Yiu Wai (58096208200); Schäfer, Max (26427858100); Pradel, Michael (25641744400)","58096208200; 26427858100; 25641744400","Beware of the Unexpected: Bimodal Taint Analysis","2023","ISSTA 2023 - Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis","","","","211","222","11","","10.1145/3597926.3598050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167705503&doi=10.1145%2f3597926.3598050&partnerID=40&md5=f4a30693beeb21a45cccc256451e7769","Static analysis is a powerful tool for detecting security vulnerabilities and other programming problems. Global taint tracking, in particular, can spot vulnerabilities arising from complicated data flow across multiple functions. However, precisely identifying which flows are problematic is challenging, and sometimes depends on factors beyond the reach of pure program analysis, such as conventions and informal knowledge. For example, learning that a parameter name of an API function locale ends up in a file path is surprising and potentially problematic. In contrast, it would be completely unsurprising to find that a parameter command passed to an API function execaCommand is eventually interpreted as part of an operating-system command. This paper presents Fluffy, a bimodal taint analysis that combines static analysis, which reasons about data flow, with machine learning, which probabilistically determines which flows are potentially problematic. The key idea is to let machine learning models predict from natural language information involved in a taint flow, such as API names, whether the flow is expected or unexpected, and to inform developers only about the latter. We present a general framework and instantiate it with four learned models, which offer different trade-offs between the need to annotate training data and the accuracy of predictions. We implement Fluffy on top of the CodeQL analysis framework and apply it to 250K JavaScript projects. Evaluating on five common vulnerability types, we find that Fluffy achieves an F1 score of 0.85 or more on four of them across a variety of datasets.  © 2023 ACM.","AI4SE; software security","Data flow analysis; Data transfer; Economic and social effects; Machine learning; AI4SE; API functions; Dataflow; Machine learning models; Machine-learning; Multiple function; Program analysis; Programming problem; Security vulnerabilities; Software security; Static analysis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85167705503"
"Devi V.K.; Asha S.; Umamaheswari E.; Bacanin N.","Devi, V. Kanchana (58172155400); Asha, S. (55340023300); Umamaheswari, E. (49964767000); Bacanin, Nebojsa (37028223900)","58172155400; 55340023300; 49964767000; 37028223900","A Comprehensive Review on Various Artificial Intelligence Based Techniques and Approaches for Cyber Security","2023","Smart Innovation, Systems and Technologies","361","","","303","314","11","","10.1007/978-981-99-3982-4_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171531696&doi=10.1007%2f978-981-99-3982-4_26&partnerID=40&md5=edca950063f846fc6ce24c40394871f7","Change is constant in Cyber Security world. In order to prevent cyber-attacks proactively new technologies must be incorporated. In which, Artificial Intelligence (AI) gains the attention towards Cyber Security (CS). In spite of viewing AI as a single, uniform feature, it is better to foresee it as a collection of several technologies that boosts up a common goal. It helps in analysing the vulnerabilities in the software, monitoring the network for detecting abnormalities, looking for patterns or anomalies in data. Cyber-attacks are monitored and defended by tying-up AI with cyber security to develop understanding and awareness, respond during real-time system and to extend progress in its overall performance. AI completes this kind of analysis in few seconds unlike other models or technologies. This paper presents the comprehensive analysis on various AI based Techniques and Approaches designed for CS. This review has been compared with other survey papers for better understanding. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Artificial Intelligence; Attacks; Cyber security; Mitigation; Prediction","Computer crime; Crime; Cyber attacks; Interactive computer systems; Network security; Real time systems; Attack; Comprehensive analysis; Cyber security; Cyber-attacks; Mitigation; Performance; Real - Time system; Software monitoring; Artificial intelligence","Conference paper","Final","","Scopus","2-s2.0-85171531696"
"Zhang A.; Zhang Y.; Xu Y.; Wang C.; Li S.","Zhang, Ao (58796090500); Zhang, Yiying (57192176739); Xu, Yao (58124759400); Wang, Cong (57192598286); Li, Siwei (58796003000)","58796090500; 57192176739; 58124759400; 57192598286; 58796003000","Machine Learning-based Fuzz Testing Techniques: A Survey","2023","IEEE Access","","","","1","1","0","","10.1109/ACCESS.2023.3347652","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181559106&doi=10.1109%2fACCESS.2023.3347652&partnerID=40&md5=22ae009d6ba869c5ad5ca936a5d5cdae","Fuzz testing is a vulnerability discovery technique that tests the robustness of target programs by providing them with unconventional data. With the rapid increase in software quantity, scale and complexity, traditional fuzzing has revealed issues such as incomplete logic coverage, low automation level and insufficient test cases. Machine learning, with its exceptional capabilities in data analysis and classification prediction, presents a promising approach for improve fuzzing. This paper investigates the latest research results in fuzzing and provides a systematic review of machine learning-based fuzzing techniques. Firstly, by outlining the workflow of fuzzing, it summarizes the optimization of different stages of fuzzing using machine learning. Specifically, it focuses on the application of machine learning in the preprocessing phase, test case generation phase, input selection phase and result analysis phase. Secondly, it mentally focuses on the optimization methods of machine learning in the process of mutation, generation and filtering of test cases and compares and analyzes its technical principles. Furthermore, it analyzes the performance gains brought by applying machine learning techniques to fuzzing, mainly including coverage, vulnerability detection capability, efficiency and effectiveness of test cases. Lastly, it concludes by summarizing the challenges and difficulties in combining machine learning with fuzzing and presents prospects for future trends in this field. Authors","Closed box; Fuzzing; Fuzzing; Glass box; Machine learning; Machine learning; Machine learning algorithms; Performance gain; Testing; Vulnerability discovery","Artificial intelligence; Learning systems; Software testing; Closed box; Fuzzing; Glass box; Machine learning algorithms; Machine-learning; Performance Gain; Test case; Vulnerability discovery; Learning algorithms","Article","Article in press","All Open Access; Gold Open Access","Scopus","2-s2.0-85181559106"
"Topçu B.; Öz I.","Topçu, Burak (57672081800); Öz, Işıl (37097877800)","57672081800; 37097877800","Soft error vulnerability prediction of GPGPU applications","2023","Journal of Supercomputing","79","6","","6965","6990","25","","10.1007/s11227-022-04933-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142132964&doi=10.1007%2fs11227-022-04933-2&partnerID=40&md5=ad9ea91ce3ff566190b83c818495c22c","As graphics processing units (GPUs) evolve to offer high performance for general-purpose computations in addition to inherently fault-tolerant graphics applications, soft error reliability becomes a significant concern. Fault injection provides a method of evaluating the soft error vulnerability of target programs. Since performing fault injection experiments for complex GPU hardware structures takes impractical times, the prediction-based techniques to evaluate the soft error vulnerability of general-purpose GPU (GPGPU) programs based on metrics from different domains get crucial for both HPC developers and GPU vendors. In this work, we propose machine learning (ML)-based prediction frameworks for the soft error vulnerability evaluation of GPGPU programs. We consider program characteristics, hardware usage and performance metrics collected from the simulation and the profiling tools. While we utilize regression models to predict the masked fault rates, we build classification models to specify the vulnerability level of the GPGPU programs based on their silent data corruption (SDC) and crash rates. Our prediction models achieve maximum prediction accuracy rates of 95.9, 88.46, and 85.7% for masked fault rates, SDCs, and crashes, respectively. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","","Computer graphics; Computer hardware; Error correction; Forecasting; Program processors; Radiation hardening; Regression analysis; Software testing; Fault injection; Fault rates; Fault-tolerant; General-purpose computations; General-purpose GPUs; GPU programs; Graphics applications; Hardware structures; Performance; Soft error; Graphics processing unit","Article","Final","","Scopus","2-s2.0-85142132964"
"Sultana K.Z.; Boyd C.B.; Williams B.J.","Sultana, Kazi Zakia (23494078600); Boyd, Charles B. (58540657700); Williams, Byron J. (24478168500)","23494078600; 58540657700; 24478168500","A Software Vulnerability Prediction Model Using Traceable Code Patterns and Software Metrics","2023","SN Computer Science","4","5","599","","","","","10.1007/s42979-023-02077-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168257983&doi=10.1007%2fs42979-023-02077-5&partnerID=40&md5=b1a1c569459b4f4996017080301ae4f6","The goal of this research is to build a vulnerability prediction model to assist developers in evaluating the security of software systems during the early stages of development. In this study, we used some traceable patterns which can be automatically identified or extracted from the source code of functions or methods. These patterns have been introduced in the earlier studies and termed as nanopatterns. We also used software metrics along with nanopatterns as features for training a model for vulnerability prediction. In this study, we blend two different kinds of features and propose nano-metrics consisting of a set of nanopatterns and method-level software metrics to predict vulnerability more accurately than existing models. This study investigates how the proposed features perform in vulnerability prediction compared to traditional software metrics. We designed and conducted machine learning and statistical analysis based experiments to predict vulnerabilities reported for Apache Tomcat (releases 6 and 7), Apache CXF, Android (versions 6 and 7), and two stand-alone Java web applications of Stanford Securibench. We present the performance measures using tenfold cross validation and cross-project validation of our proposed approach. We also identified significant pairs of metrics and patterns in vulnerable methods. We found that the proposed nano-metrics have a lower false negative rate and higher recall in predicting vulnerable code than software metrics (lowest recall is 67 vs. 63% in Logistic Regression). Nano-metrics show higher precision than nanopatterns which improves their overall F2 -measure compared to software metrics (highest is 90 vs. 79% in Logistic Regression). Our experiments present a new set of features in building a vulnerability prediction model with better recall and precision. © 2023, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.","Nanopatterns; Software metrics; Software quality; Software security; Software testing; Software vulnerability","","Article","Final","","Scopus","2-s2.0-85168257983"
"Silva C.; Ribeiro R.; Romão J.","Silva, C. (58342092100); Ribeiro, R. (58717287000); Romão, J. (23670511700)","58342092100; 58717287000; 23670511700","Predicting Mitigation on Catastrophic Event Such as Tsunami—Automatization of a Model Alert","2023","Lecture Notes in Networks and Systems","597 LNNS","","","614","626","12","","10.1007/978-3-031-21438-7_50","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148716393&doi=10.1007%2f978-3-031-21438-7_50&partnerID=40&md5=a2ea4aa4e70318dd379beba3c1bbbe5a","Increase of human impact on the environment entailed increase of the numbers of natural disasters and related economic and human losses in last decades. Many natural systems exhibit chaotic behavior, including the weather, hydrology, neuroscience, and population dynamics. Although many of these systems can be described by relatively simple dynamical equations, characterizing these systems can be challenging due to sensitivity of initial conditions and difficulties in differentiating chaotic behavior from noise. The prediction system is made up of several networks that learned the relationships between technical points and modelling strategies. This study aims to develop a software framework for modeling tsunami vulnerability alert developing theoretical architecture alert building from 3 types of algorithms models. We present a method combining variational technique previously used for parameter estimation in chaotic systems with hidden variables. We applied the method simulated from the interconnecting system architecture to perform communication between types of algorithms. In this paper, we explore the need for a new generation of warning products, aimed at the flooding hazard, to reduce future tsunami impacts on society. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Algorithm; Artificial intelligence; Optimization; Prediction system; Supervised and unsupervised learning; Tsunami impact","","Conference paper","Final","","Scopus","2-s2.0-85148716393"
"Zhang Q.; Fang C.; Yu B.; Sun W.; Zhang T.; Chen Z.","Zhang, Quanjun (57462106200); Fang, Chunrong (55321130800); Yu, Bowen (58561561800); Sun, Weisong (57208225070); Zhang, Tongke (58561461800); Chen, Zhenyu (55579848600)","57462106200; 55321130800; 58561561800; 57208225070; 58561461800; 55579848600","Pre-Trained Model-Based Automated Software Vulnerability Repair: How Far are We?","2023","IEEE Transactions on Dependable and Secure Computing","","","","1","18","17","","10.1109/TDSC.2023.3308897","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169697811&doi=10.1109%2fTDSC.2023.3308897&partnerID=40&md5=1dcb8d712e729f0a4d3f7f036fadf486","Various approaches are proposed to help under-resourced security researchers to detect and analyze software vulnerabilities. It is still incredibly time-consuming and labor-intensive for security researchers to fix such reported vulnerabilities due to the increasing size and complexity of modern software systems. The time lag between the reporting and fixing of a security vulnerability causes software systems to suffer from significant exposure to possible attacks. Very recently, some techniques propose to apply pretrained models to fix security vulnerabilities and have proved their success in improving repair accuracy. However, the effectiveness of existing pre-trained models has not been systematically compared and little is known about their advantages and disadvantages. To bridge this gap, we perform the first extensive study on applying various pre-trained models to automated vulnerability repair. The experimental results on two vulnerability datasets show that all studied pre-trained models consistently outperform the state-ofthe- art technique VRepair with a prediction accuracy of 32.94<inline-formula><tex-math notation=""LaTeX"">$\sim$</tex-math></inline-formula>44.96&#x0025;. We also investigate the impact of three major phases (i.e., data pre-processing, model training and repair inference) in the vulnerability repair workflow. Inspired by the findings, we construct a simplistic vulnerability repair approach that adopts the transfer learning from bug fixing. Surprisingly, such a simplistic approach can further improve the prediction accuracy of pre-trained models by 9.40&#x0025; on average. Besides, we provide additional discussion from different aspects (e.g., code representation and a preliminary study with ChatGPT) to illustrate the capacity and limitation of pre-trained model-based techniques. Finally, we further pinpoint various practical guidelines (e.g., the improvement of fine-tuning) for advanced pre-trained model-based vulnerability repair in the near future. Our study highlights the promising future of adopting pre-trained models to patch real-world security vulnerabilities and reduce the manual debugging effort of security experts in practice. IEEE","Codes; Maintenance engineering; Pre-trained model; Predictive models; Security; security vulnerability; Task analysis; Training; Transfer learning; vulnerability repair","Computer software; Data handling; Job analysis; Personnel training; Code; Model-based OPC; Pre-trained model; Predictive models; Security; Security vulnerabilities; Software vulnerabilities; Task analysis; Transfer learning; Vulnerability repair; Repair","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85169697811"
"Nguyen Hung T.; Nguyen Phuc H.; Tran Dinh K.; Le Tran Thanh N.; To Trong N.; Ngo Khanh K.; Phan The D.; Pham Van H.","Nguyen Hung, Thinh (58778485900); Nguyen Phuc, Hai (58778413300); Tran Dinh, Khoa (58778384000); Le Tran Thanh, Nhan (58778413400); To Trong, Nghia (58778413500); Ngo Khanh, Khoa (57205680810); Phan The, Duy (58000686300); Pham Van, Hau (58778486400)","58778485900; 58778413300; 58778384000; 58778413400; 58778413500; 57205680810; 58000686300; 58778486400","Binary Representation Embedding and Deep Learning For Binary Code Similarity Detection in Software Security Domain","2023","ACM International Conference Proceeding Series","","","","785","792","7","","10.1145/3628797.3628996","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180546801&doi=10.1145%2f3628797.3628996&partnerID=40&md5=700722cfae9e871642694fa5b666d8e4","Binary Code Similarity Detection (BCSD) is the process of analyzing the binary representations of two functions, programs, or related entities to generate a quantitative output that signifies the similarity score between them. This task encompasses a wide range of applications, including addressing the binary search problem, which involves searching for code segments within a binary file that are similar to a specified binary code segment. These capabilities open up numerous potential applications within the domain of binary code analysis such as software vulnerability detection, clone detection, and malware analysis. In this paper, we introduce BiSim-Inspector, a BCSD tool based on Deep Learning (DL). This tool leverages the Bytes2vec method, which we develop to transform the bytecode of binary functions into vectors, which are then fed into the Convolutional Neural Network - Gated Recurrent Unit (CNN-GRU) model. Additionally, we conducted a series of experiments to assess the effectiveness of our method by comparing it with existing state-of-the-art (SOTA) tools. We use a large-scale, well-structured, and diversified dataset, BinaryCorp, for the task of BCSD. The results show that our framework achieves a Recall rate of 89%, which is 25% higher than existing SOTA methods, without compromising the training and prediction time.  © 2023 ACM.","Binary Code Similarity Detection; Deep Learning; Software Security","Application programs; Large dataset; Learning systems; Recurrent neural networks; Binary code similarity detection; Binary representations; Code segments; Code similarities; Deep learning; Embeddings; Function project; Security domains; Similarity detection; Software security; Binary codes","Conference paper","Final","","Scopus","2-s2.0-85180546801"
"Dong Y.; Tang Y.; Cheng X.; Yang Y.; Wang S.","Dong, Yukun (55883737000); Tang, Yeer (58107288000); Cheng, Xiaotong (57371843700); Yang, Yufei (57877094600); Wang, Shuqi (57610783500)","55883737000; 58107288000; 57371843700; 57877094600; 57610783500","SedSVD: Statement-level software vulnerability detection based on Relational Graph Convolutional Network with subgraph embedding","2023","Information and Software Technology","158","","107168","","","","","10.1016/j.infsof.2023.107168","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148335038&doi=10.1016%2fj.infsof.2023.107168&partnerID=40&md5=9966eb7c81b418aca1856c43fe777623","Context: Current deep-learning based vulnerability detection methods have been proven more automatic and correct to a certain extent, nonetheless, they are limited to detect at function-level or file-level, which can hinder software developers from acquiring more detailed information and conducting more targeted repairs. Graph-based detection methods have shown dominant performance over others. Unfortunately, the information they reveal has not been fully utilized. Objective: We design SedSVD (Subgraph embedding driven Statement-level Vulnerability Detection) with two objectives: (i) to better utilize the information the code-related graphs can reflect; (ii) to detect vulnerabilities at a finer-grained level. Method: In our work, we propose a novel graph-based detection framework that embeds graphs at subgraph-level to realize statement-level detection. It first leverages Code Property Graph (CPG) to learn both semantic and syntactic information from source code, and then selects several center nodes (code elements) in CPG to build their subgraphs. After embedding each subgraph with its nodes and edges, we apply Relational Graph Convolutional Network (RGCN) to process different edges differently. A Multi-Layer Perceptron (MLP) layer is further added to ensure its prediction performance. Results: We conduct our experiments on C/C++ projects from NVD and SARD. Experimental results show that SedSVD achieves 95.15% in F1-measure which proves our work to be more effective. Conclusion: Our work detects at a finer-grained level and achieves higher F1-measure than existing state-of-art vulnerability detection techniques. Besides, we provide a more detailed detection report pointing the specific error code elements within statements. © 2023 Elsevier B.V.","Code property graph; Relational graph convolutional network; Software vulnerability detection; Statement-level detection; Subgraph embedding","C++ (programming language); Convolution; Deep learning; Graph theory; Graphic methods; Network coding; Network security; Semantics; Code property graph; Convolutional networks; Embeddings; Level detections; Property; Relational graph; Relational graph convolutional network; Software vulnerabilities; Software vulnerability detection; Statement-level detection; Subgraph embedding; Subgraphs; Vulnerability detection; Embeddings","Article","Final","","Scopus","2-s2.0-85148335038"
"Uhongora U.; Mulinde R.; Law Y.W.; Slay J.","Uhongora, Uakomba (58531200800); Mulinde, Ronald (56263134900); Law, Yee Wei (57216893610); Slay, Jill (15756115700)","58531200800; 56263134900; 57216893610; 15756115700","Deep-learning-based Intrusion Detection for Software-defined Networking Space Systems","2023","European Conference on Information Warfare and Security, ECCWS","2023-June","","","639","647","8","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167612824&partnerID=40&md5=312bd7c5c614c5e95414bffc20b023de","This paper briefly reviews the application of the Software-defined Networking (SDN) architecture to satellite networks. It highlights the prominent cyber threats that SDN-based satellite networks are vulnerable to and proposes relevant defence mechanisms. SDN transforms traditional networking architectures by separating the control plane from the forwarding (data) plane. This separation enhances scalability and centralises management. In comparison, in traditional networks, the control plane and the data plane are usually combined, resulting in complex network management and reduced scalability. Satellite networks can take advantage of these benefits offered by SDN and this supports them as key enablers of critical services, including weather prediction, global broadband Internet coverage, and Internet of Things (IoT) services. Ease of configuration and flexibility are essential for satellites providing critical services to instantly adapt to network changes. These desirable attributes can be realised by applying SDN to satellite networks. Although SDN offers significant benefits to satellite networks, it is vulnerable to cyber-Attacks and particularly due to its centralised architecture. A common attack on SDN is the Distributed Denial of Service (DDoS) attack which could render the entire SDN unavailable. To mitigate such threats, an efficient Intrusion Detection System (IDS) is required to monitor the network and detect any suspicious traffic. However, traditional IDSs produce too many false positives and often fail to detect advanced attacks. For their ability to learn feature hierarchies in network traffic data automatically, whether, for network traffic classification or anomaly detection, deep learning (DL) plays an increasingly important role in IDSs. In this paper, we present a brief review of recent developments in cyber security for SDN-based space systems, and we identify vulnerabilities and threats to an SDN-based satellite network. We further discuss the potential of a DL-based IDS for the detection of cyber threats. Finally, we identify further research gaps in the recent literature and propose future research directions. © 2023 Curran Associates Inc.. All rights reserved.","Deep Learning; Intrusion Detection System.; Satellite Networks; Software-defined Networking; Space Systems","Anomaly detection; Application programs; Complex networks; Cybersecurity; Deep learning; Denial-of-service attack; Information management; Internet of things; Intrusion detection; Network architecture; Satellites; Scalability; Software defined networking; Control planes; Cyber threats; Data planes; Deep learning; Intrusion detection system.; Intrusion Detection Systems; Networking architecture; Satellite network; Software-defined networkings; Space systems; Network security","Conference paper","Final","","Scopus","2-s2.0-85167612824"
"Lu F.-M.; Tang M.-F.; Bao Y.-X.; Zeng Q.-T.; Li Y.-C.","Lu, Fa-Ming (12240908100); Tang, Meng-Fan (57866279700); Bao, Yun-Xia (55413418600); Zeng, Qing-Tian (7401806588); Li, Yan-Cheng (58307532200)","12240908100; 57866279700; 55413418600; 7401806588; 58307532200","A Target-oriented UAF Vulnerability Prediction Method of Multi-threaded Programs; [一种目标导向的多线程程序 UAF 漏洞预测方法]","2023","Ruan Jian Xue Bao/Journal of Software","34","7","","","","","","10.13328/j.cnki.jos.006862","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161365347&doi=10.13328%2fj.cnki.jos.006862&partnerID=40&md5=34ee5ab39a9cd4d7de358fb3a565d937","Data races are common defects in multi-threaded programs. Traditional data race analysis methods are difficult to achieve both in recall and precision. Their detection reports are difficult to locate the root cause of the defect. Considering that Petri nets have the advantages of accurate behavior description and rich analysis tools in the modeling and analysis of concurrent systems, a new data race detection method based on Petri net unfolding technology is proposed. First, by analyzing a program running trace, a Petri net model of the program is mined. It implies multiple different traces of the program even though it is mined from only one trace, which can reduce the false negative rate of traditional dynamic methods while ensuring the performance. After that, a Petri net unfolding-based detection method of program potential data races is proposed, which has a significant improvement in efficiency compared with static methods. Furthermore, it can clearly show the triggering path of the data race defect. Finally, for the potential data race detected in the previous stage, a scheduling schema is designed to replay the defect based on the CalFuzzer platform, which can eliminate false positives and ensure the authenticity of detection results. The corresponding prototype system is developed, and the effectiveness of the proposed method is verified with open program instances. © 2023 Chinese Academy of Sciences. All rights reserved.","concurrency vulnerability; Petri net; reverse unfolding; software verification; use-after-free vulnerability","Petri nets; Verification; Concurrency vulnerability; Data races; Detection methods; Multi-threaded programs; Petri net unfoldings; Reverse unfolding; Software verification; Target oriented; Unfoldings; Use-after-free vulnerability; Defects","Article","Final","","Scopus","2-s2.0-85161365347"
"Abdulkadhim F.G.; Yi Z.; Tang C.; Onaizah A.N.; Ahmed B.","Abdulkadhim, Fahad Ghalib (57216300197); Yi, Zhang (56888594600); Tang, Chengkai (36643012300); Onaizah, Ameer N. (57197762972); Ahmed, Basheer (57220597497)","57216300197; 56888594600; 36643012300; 57197762972; 57220597497","Design and development of a hybrid (SDN + SOM) approach for enhancing security in VANET","2023","Applied Nanoscience (Switzerland)","13","1","","799","810","11","","10.1007/s13204-021-01908-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107873315&doi=10.1007%2fs13204-021-01908-2&partnerID=40&md5=3ade1a16efe9d159b48c1c418683399b","Vehicular ad hoc networks (VANET) are an emergent technology with a promising future. VANETs are quite different from mobile ad hoc networks (MANETs) in terms of characteristics, challenges, system architecture, and application. One of the significant challenges of the VANET is the security from two essential points of view, the prediction and prevention attackers. In this article, we proposed an operating system that combined both software-defined network (SDN) and self-organizing map (SOM) for the 5G-based VANET system. The proposed system will be a new combination of the SDN- and a self-organizing map (SOM)-based network solution to enhance security in the two dimensions, detecting and preventing attacks. This article analysis the vulnerability of the network performance with taken into account the distributed denial of service (DDoS) attacks. Then, the security of the proposed system has been analyzed and checked with existing of the DDoS. The simulation results presented in this article show that, under general conditions of networks, the proposed system can enhance the networks performance compared to the existing work. © 2021, King Abdulaziz City for Science and Technology.","5G technology; SDN; SOM; VANET; Vehicles","5G mobile communication systems; Conformal mapping; Denial-of-service attack; Network performance; Network security; Vehicular ad hoc networks; 5g technology; Design and Development; Distributed denial of service; Emergent technologies; MAP approach; Mobile ad-hoc networks; Self-organizing-maps; Software-defined networks; System applications; Vehicular Adhoc Networks (VANETs); Self organizing maps","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85107873315"
"Jia F.; Kang S.; Jiang W.; Wang G.","Jia, Fan (36634140100); Kang, Shuya (58604761200); Jiang, Weiqiang (57217307920); Wang, Guangtao (58604946300)","36634140100; 58604761200; 57217307920; 58604946300","Multi-user recommendation algorithm based on vulnerability similarity; [基于相似性的多用户漏洞推荐算法 收藏]","2023","Qinghua Daxue Xuebao/Journal of Tsinghua University","63","9","","1399","1407","8","","10.16511/j.cnki.qhdxxb.2023.21.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171612051&doi=10.16511%2fj.cnki.qhdxxb.2023.21.007&partnerID=40&md5=64c733dd99db0b1b3d5b324241a38cb0","[Objective] In recent years, the number of publicly disclosed vulnerabilities has increased, and software security personnel and vulnerability enthusiasts have experienced increasing difficulty in finding the vulnerability information they are interested in. A recommendation algorithm can provide personalized vulnerability suggestions to help users obtain valuable vulnerability information efficiently. However, recommendation systems related to vulnerabilities generally have problems such as one-sided analysis, complex implementation methods, strong professionalism, and data privacy, and research on directly recommending vulnerabilities as recommendation items is scarce.[Methods] This paper selects the vulnerability itself as the recommendation item, collects data from public datasets, and adopts a simple and efficient recommendation algorithm for personalized recommendations of vulnerabilities. As a classical recommendation model, the collaborative filtering recommendation algorithm is widely used and computationally efficient. However, the user-vulnerability interaction matrix is sparser than the interaction matrix analyzed by the classical recommendation model, which seriously affects the use effect of the collaborative filtering recommendation algorithm. To solve this problem, this paper introduces a vulnerability similarity research algorithm, comprehensively considers 13 features, such as vulnerability type, severity, and vulnerability description text, and integrates them into content-based recommendation algorithms, emphasizing the universal connection between vulnerabilities in recommendation algorithms. By calculating the similar vulnerabilities of each vulnerability the target user has interacted with, the algorithm summarizes the list of vulnerabilities with the highest recommended value and recommends it to the user. Simultaneously, the algorithm fully considers the characteristics of personal users and product users and combines the labeling mechanism to finally form a multi-user vulnerability recommendation algorithm based on similarity, effectively improving the sparsity and cold start of the recommendation algorithm. [Results] The experiments on public datasets show that 1) the content recommendation algorithm based on similarity can achieve better accuracy than the traditional collaborative filtering algorithm on all types of users. Particularly, the precision, recall, and Fl score of the recommendation algorithm results for product users increase by 58. 86%, 58. 53%, and 0. 586 1, respectively. 2) The recommendation list of the content recommendation algorithm based on similarity is more effective and more consistent with the user's vulnerability preferences. For product users, the the normalized discounted cumulative gain score of the recommendation list increases by 0. 596 5. 3) The result coverage of the content recommendation algorithm based on similarity is much higher than that of the collaborative filtering algorithm. Among human users, the result coverage of the content recommendation algorithm based on similarity is 7. 6 times that of original interest data, which shows that the recommendation algorithm successfully mobilizes more vulnerabilities to recommend that users have not previously interacted with.[Conclusions] This paper takes vulnerabilities as a recommendation item to recommend vulnerabilities for multiple types of users and proposes a multi-user vulnerability recommendation algorithm based on similarity. The algorithm mainly introduces the vulnerability similarity calculation method and integrates it into the content-based recommendation algorithm. The algorithm proposed in this paper solves the problems of the high sparsity of a user-vulnerability interaction matrix and cold-start problems of user-based collaborative filtering algorithms and effectively improves the accuracy and effectiveness of recommendations. © 2023 Press of Tsinghua University. All rights reserved.","collaborative filtering recommendation; content-based recommendation; recommendation algorithm; vulnerability similarity","Collaborative filtering; Data privacy; Matrix algebra; Signal filtering and prediction; Collaborative filtering algorithms; Collaborative filtering recommendations; Content recommendations; Content-based recommendation; Interaction matrices; Multi-user recommendations; Multiusers; Public dataset; Recommendation algorithms; Vulnerability similarity; Recommender systems","Article","Final","","Scopus","2-s2.0-85171612051"
"Zhang X.-Y.; Shen C.; Lin C.-H.; Li Q.; Wang Q.; Li Q.; Guan X.-H.","Zhang, Xiao-Yu (57214915278); Shen, Chao (36446592900); Lin, Chen-Hao (57221245073); Li, Qian (57211305736); Wang, Qian (58337803400); Li, Qi (57200602838); Guan, Xiao-Hong (7201463208)","57214915278; 36446592900; 57221245073; 57211305736; 58337803400; 57200602838; 7201463208","The Testing and Repairing Methods for Machine Learning Model Security; [面向机器学习模型安全的测试与修复]","2022","Tien Tzu Hsueh Pao/Acta Electronica Sinica","50","12","","2884","2918","34","","10.12263/DZXB.20220821","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159964328&doi=10.12263%2fDZXB.20220821&partnerID=40&md5=d2ee799dadfc9d8fbbffddd1197209b8","In recent years, artificial intelligence technology led by machine learning algorithms has been widely used in many fields, such as computer vision, natural language processing, speech recognition, etc. A variety of machine learning models have greatly facilitated people’s lives. The workflow of a machine learning model consists of three stages. First, the model receives the raw data which is collected or generated by the developers as the model input and preprocesses the data through preprocessing algorithms, such as data augmentation and feature extraction. Subsequently, the model defines the architecture of neurons or layers in the model and constructs a computational graph through operators(e.g., convolution and pooling). Finally, the model calls the machine learning framework function to implement the operators and calculates the prediction result of the input data according to the weights of model neurons. In this process, slight fluctuations in the output of individual neurons in the model may lead to an entirely different model output, which can bring huge security risks. However, due to the insufficient understanding of the inherent vulnerability of machine learning models and their black box characteristic behaviors, it is difficult for researchers to identify or locate these potential security risks in advance. This brings many risks and hidden dangers to personal property safety and even national security. There is great significance to studying the testing and repairing methods for machine learning model security, which can help deeply understand the internal risks and vulnerabilities of models, comprehensively guarantee the security of machine learning systems, and widely apply artificial intelligence technology. The existing testing research for the machine learning model security has mainly focused on the correctness, robustness, and other testing properties of the model, and this research has achieved certain results. This paper intends to start from different security testing attributes, introduces the existing machine learning model security testing and repair technology in detail, summarizes and analyzes the deficiencies in the existing research, and discusses the technical progress and challenges of machine learning model security testing and repairing, providing guidance and reference for the safe application of the model. In this paper, we first introduce the structural composition and main testing properties of the machine learning model security. Afterwards, we systematically summarize and analyze the existing work from the three components of the machine learning model—data, algorithm, and implementation, and six model security-related testing properties-correctness, robustness, fairness, efficiency, interpretability, and privacy. We also discuss the effectiveness and limitations of the existing testing and repairing methods. Finally, we discuss several technical challenges and potential development directions of the testing and repairing methods for machine learning model security in the future. © 2022 Chinese Institute of Electronics. All rights reserved.","artificial intelligence security; machine learning model repairing; machine learning model testing; machine learning security; software repairing; software testing","Learning algorithms; Machine components; Machine learning; National security; Natural language processing systems; Repair; Software testing; Speech recognition; Tensile testing; Artificial intelligence security; Machine learning model repairing; Machine learning model testing; Machine learning models; Machine learning security; Machine-learning; Model testing; Software repairing; Software testings; Neurons","Article","Final","","Scopus","2-s2.0-85159964328"
"Aladics T.; Hegedűs P.; Ferenc R.","Aladics, Tamás (57217147140); Hegedűs, Péter (25926433300); Ferenc, Rudolf (6603559878)","57217147140; 25926433300; 6603559878","An AST-Based Code Change Representation and Its Performance in Just-in-Time Vulnerability Prediction","2023","Communications in Computer and Information Science","1859 CCIS","","","169","186","17","","10.1007/978-3-031-37231-5_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172413585&doi=10.1007%2f978-3-031-37231-5_8&partnerID=40&md5=50bf6c49992485703409557bd7fa16fd","The presence of software vulnerabilities is an ever-growing issue in software development. In most cases, it is desirable to detect vulnerabilities as early as possible, preferably in a just-in-time manner, when the vulnerable piece is added to the code base. The industry has a hard time combating this problem as manual inspection is costly and traditional means, such as rule-based bug detection, are not robust enough to follow the pace of the emergence of new vulnerabilities. The actively researched field of machine learning could help in such situations as models can be trained to detect vulnerable patterns. However, machine learning models work well only if the data is appropriately represented. In our work, we propose a novel way of representing changes in source code (i.e. code commits), the Code Change Tree, a form that is designed to keep only the differences between two abstract syntax trees of Java source code. We compared its effectiveness in predicting if a code change introduces a vulnerability against multiple representation types and evaluated them by a number of machine learning models as a baseline. The evaluation is done on a novel dataset that we published as part of our contributions using a 2-phase dataset generator method. Based on our evaluation we concluded that using Code Change Tree is a valid and effective choice to represent source code changes as it improves performance. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Code change representation; Just-in-time; Vulnerability prediction","Codes (symbols); Just in time production; Machine learning; Software design; Trees (mathematics); Code change representation; Code changes; Just-in-time; Machine learning models; Machine-learning; Manual inspection; Performance; Rule-based bug detections; Software vulnerabilities; Vulnerability prediction; Forecasting","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85172413585"
"Croft R.; Babar M.A.; Kholoosi M.M.","Croft, Roland (57219534056); Babar, M. Ali (6602842620); Kholoosi, M. Mehdi (57386934400)","57219534056; 6602842620; 57386934400","Data Quality for Software Vulnerability Datasets","2023","Proceedings - International Conference on Software Engineering","","","","121","133","12","","10.1109/ICSE48619.2023.00022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165165395&doi=10.1109%2fICSE48619.2023.00022&partnerID=40&md5=b4a39c466b88148a3ca9551955237b1c","The use of learning-based techniques to achieve automated software vulnerability detection has been of longstanding interest within the software security domain. These data-driven solutions are enabled by large software vulnerability datasets used for training and benchmarking. However, we observe that the quality of the data powering these solutions is currently ill-considered, hindering the reliability and value of produced outcomes. Whilst awareness of software vulnerability data preparation challenges is growing, there has been little investigation into the potential negative impacts of software vulnerability data quality. For instance, we lack confirmation that vulnerability labels are correct or consistent. Our study seeks to address such shortcomings by inspecting five inherent data quality attributes for four state-of-the-art software vulnerability datasets and the subsequent impacts that issues can have on software vulnerability prediction models. Surprisingly, we found that all the analyzed datasets exhibit some data quality problems. In particular, we found 20-71% of vulnerability labels to be inaccurate in real-world datasets, and 17-99% of data points were duplicated. We observed that these issues could cause significant impacts on downstream models, either preventing effective model training or inflating benchmark performance. We advocate for the need to overcome such challenges. Our findings will enable better consideration and assessment of software vulnerability data quality in the future. © 2023 IEEE.","data quality; machine learning; software vulnerability","Data reduction; Large dataset; Machine learning; Data driven; Data preparation; Data quality; Is-enabled; Machine-learning; Quality attributes; Security domains; Software security; Software vulnerabilities; Vulnerability detection; Benchmarking","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85165165395"
"Meiseles A.; Motro Y.; Rokach L.; Moran-Gilad J.","Meiseles, Amiel (57214114361); Motro, Yair (14014035400); Rokach, Lior (9276243500); Moran-Gilad, Jacob (57218509412)","57214114361; 14014035400; 9276243500; 57218509412","Vulnerability of pangolin SARS-CoV-2 lineage assignment to adversarial attack","2023","Artificial Intelligence in Medicine","146","","102722","","","","","10.1016/j.artmed.2023.102722","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177979911&doi=10.1016%2fj.artmed.2023.102722&partnerID=40&md5=24b48c0ab86a6bf409e17d4df44acff2","Pangolin is the most popular tool for SARS-CoV-2 lineage assignment. During COVID-19, healthcare professionals and policymakers required accurate and timely lineage assignment of SARS-CoV-2 genomes for pandemic response. Therefore, tools such as Pangolin use a machine learning model, pangoLEARN, for fast and accurate lineage assignment. Unfortunately, machine learning models are susceptible to adversarial attacks, in which minute changes to the inputs cause substantial changes in the model prediction. We present an attack that uses the pangoLEARN architecture to find perturbations that change the lineage assignment, often with only 2–3 base pair changes. The attacks we carried out show that pangolin is vulnerable to adversarial attack, with success rates between 0.98 and 1 for sequences from non-VoC lineages when pangoLEARN is used for lineage assignment. The attacks we carried out are almost never successful against VoC lineages because pangolin uses Usher and Scorpio – the non-machine-learning alternative methods for VoC lineage assignment. A malicious agent could use the proposed attack to fake or mask outbreaks or circulating lineages. Developers of software in the field of microbial genomics should be aware of the vulnerabilities of machine learning based models and mitigate such risks. © 2023 Elsevier B.V.","Adversarial; COVID-19; Cyber security; Machine learning; Surveillance; Variants","Animals; COVID-19; Health Personnel; Humans; Machine Learning; Pangolins; SARS-CoV-2; Cybersecurity; Machine learning; Adversarial; Base pairs; Cyber security; Health care professionals; Machine learning models; Machine-learning; Model prediction; Policy makers; Surveillance; Variant; animal; coronavirus disease 2019; health care personnel; human; machine learning; Pholidota (animal); Severe acute respiratory syndrome coronavirus 2; Coronavirus; COVID-19","Article","Final","","Scopus","2-s2.0-85177979911"
"Sellitto G.; Sheykina A.; Palomba F.; De Lucia A.","Sellitto, Giulia (57841951500); Sheykina, Alexandra (58699839000); Palomba, Fabio (55321369000); De Lucia, Andrea (7003641564)","57841951500; 58699839000; 55321369000; 7003641564","An empirical study on the performance of vulnerability prediction models evaluated applying real-world labelling","2023","CEUR Workshop Proceedings","3543","","","","","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177065061&partnerID=40&md5=f395d7f57412946cfe095595ac071afb","Software vulnerabilities are infamous threats to the security of computing systems, and it is vital to detect and correct them before releasing any piece of software to the public. Many approaches for the detection of vulnerabilities have been proposed in the literature; in particular, those leveraging machine learning techniques, i.e., vulnerability prediction models, seem quite promising. However, recent work has warned that most models have only been evaluated in in-vitro settings, under certain assumptions that do not resemble the real scenarios in which such approaches are supposed to be employed. This observation ignites the risk that the encouraging results obtained in previous literature may be not as well convenient in practice. Recognizing the dangerousness of biased and unrealistic evaluations, we aim to dive deep into the problem, by investigating whether and to what extent vulnerability prediction models performance changes when measured in realistic settings. To do this, we perform an empirical study evaluating the performance of a vulnerability prediction model, configured with three data balancing techniques, executed at three different degrees of realism, leveraging two datasets. Our findings highlight that the outcome of any measurement strictly depends on the experiment setting, calling researchers to take into account the actuality and applicability in practice of the approaches they propose and evaluate. © 2021 Copyright for this paper by its authors.","Empirical Study; Realistic Evaluation; Vulnerability Prediction","Balancing; Learning systems; Machine learning; Security of data; Computing system; Empirical studies; Labelings; Machine learning techniques; Performance; Prediction modelling; Real-world; Realistic evaluations; Software vulnerabilities; Vulnerability prediction; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85177065061"
"Vo H.D.; Nguyen S.","Vo, Hieu Dinh (55446198800); Nguyen, Son (57205025014)","55446198800; 57205025014","Can an old fashioned feature extraction and a light-weight model improve vulnerability type identification performance?","2023","Information and Software Technology","164","","107304","","","","","10.1016/j.infsof.2023.107304","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167448659&doi=10.1016%2fj.infsof.2023.107304&partnerID=40&md5=1bb115a91d7d3d1bf9650ff90c42c50a","Recent advances in automated vulnerability detection have achieved potential results in helping developers determine vulnerable components. However, after detecting vulnerabilities, investigating to fix vulnerable code is a non-trivial task. In fact, the types of vulnerability, such as buffer overflow or memory corruption, could help developers quickly understand the nature of the weaknesses and localize vulnerabilities for security analysis. In this work, we investigate the problem of vulnerability type identification (VTI). The problem is modeled as the multi-label classification task, which could be effectively addressed by “pre-training, then fine-tuning” framework with deep pre-trained embedding models. We evaluate the performance of the well-known and advanced pre-trained models for VTI on a large set of vulnerabilities. Surprisingly, their performance is not much better than that of the classical baseline approach with an old-fashioned bag-of-word, TF-IDF. Meanwhile, these deep neural network approaches cost much more resources and require GPU. We also introduce a lightweight independent component to refine the predictions of the baseline approach. Our idea is that the types of vulnerabilities could strongly correlate to certain code tokens (distinguishing tokens) in several crucial parts of programs. The distinguishing tokens for each vulnerability type are statistically identified based on their prevalence in the type versus the others. Our results show that the baseline approach enhanced by our component can outperform the state-of-the-art deep pre-trained approaches while retaining very high efficiency. Furthermore, the proposed component could also improve the neural network approaches by up to 92.8% in macro-average F1. © 2023 Elsevier B.V.","Software vulnerability; Vulnerability resolution; Vulnerability type identification","Classification (of information); Codes (symbols); Buffer memory; Buffer overflows; Features extraction; Light weight; Non-trivial tasks; Performance; Software vulnerabilities; Vulnerability detection; Vulnerability resolution; Vulnerability type identification; Deep neural networks","Article","Final","","Scopus","2-s2.0-85167448659"
"Bassi D.; Singh H.","Bassi, Deepali (57764927900); Singh, Hardeep (58376239500)","57764927900; 58376239500","A Systematic Literature Review on Software Vulnerability Prediction Models","2023","IEEE Access","11","","","110289","110311","22","","10.1109/ACCESS.2023.3312613","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171550834&doi=10.1109%2fACCESS.2023.3312613&partnerID=40&md5=939445eb4f00f0d0f62ea8dcc37d4b81","The prediction of software vulnerability requires crucial awareness during the software specification, design, development, and configuration to achieve less vulnerable and secure software. Software vulnerability prediction is the process of model development that can be beneficial for the early prediction of vulnerable components at various granularity levels such as file, class, and method. Machine learning and deep learning techniques are gaining popularity in developing vulnerability prediction models. This paper performs a systematic review of primary studies from 2000 to 2022 in the literature that used machine learning and deep learning techniques for software vulnerability prediction. In addition to this, the paper understands the concept of resampling methods to handle imbalanced dataset problems; summarizes the important hyperparameter optimization methods to tune hyperparameters; explains the types of features, data pre-processing techniques, dimensionality reduction, and feature selection techniques. Furthermore, encapsulating the comparison of ML/DL techniques and highlighting the best technique is performed. The paper identifies seventy-seven research studies that use thirty-Two machine learning and five deep learning techniques. Additionally, it identifies five different feature types, data pre-processing methods, thirty-seven datasets, nine data balancing techniques, twenty-six performance measures, six hyperparameter optimization methods, and the ranges of hyperparameters. Finally, guidelines for researchers to increase the productivity of software vulnerability prediction models have been illustrated in the paper.  © 2013 IEEE.","deep learning; machine learning; software vulnerability prediction; Systematic literature review","Data handling; Deep learning; Feature Selection; Learning algorithms; Learning systems; Deep learning; Features extraction; Machine-learning; Quality assessment; Software; Software vulnerabilities; Software vulnerability prediction; Systematic; Systematic literature review; Forecasting","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85171550834"
"Watanabe R.; Matsunaka T.; Kubota A.; Urakawa J.","Watanabe, Ryu (18134656000); Matsunaka, Takashi (24332483400); Kubota, Ayumu (14622729200); Urakawa, Jumpei (57191333176)","18134656000; 24332483400; 14622729200; 57191333176","Machine Learning Based Prediction of Vulnerability Information Subject to a Security Alert","2023","International Conference on Information Systems Security and Privacy","","","","313","320","7","","10.5220/0011613700003405","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176342394&doi=10.5220%2f0011613700003405&partnerID=40&md5=07bbeb64f330b445a06c9953ac1c62f4","The security alerts announced by various organizations can be used as an indicator of the severity and danger of vulnerabilities. The alerts are public notifications issued by security-related organizations or product/software vendors. The experts from such organizations determine whether it is a necessity of a security alert based on the published vulnerability information, threats, and publicized damages caused by the attacks to warn the public of high-risk vulnerabilities or cyberattacks. However, it may take some time between the disclosure of the vulnerability and the release of a security alert. If this delay can be shortened, it will be possible to guess the severity of the vulnerability earlier. For this purpose, the authors have proposed a machine learning method to predict whether a disclosed vulnerability is severe enough to publicize a security alert. In this paper, our proposed scheme and the evaluation we conduct to verify its accuracy are denoted. © 2023 by SCITEPRESS – Science and Technology Publications, Lda.","Machine Learning; Security Alert; Security Measure; Vulnerability Management","","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85176342394"
"Hanna C.; Petke J.","Hanna, Carol (58080683000); Petke, Justyna (36615643200)","58080683000; 36615643200","Hot Patching Hot Fixes: Reflection and Perspectives","2023","Proceedings - 2023 38th IEEE/ACM International Conference on Automated Software Engineering, ASE 2023","","","","1781","1786","5","","10.1109/ASE56229.2023.00021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179001042&doi=10.1109%2fASE56229.2023.00021&partnerID=40&md5=721dc2f10d9d387822b21a6374111ab0","With our reliance on software continuously increasing, it is of utmost importance that it be reliable. However, complete prevention of bugs in live systems is unfortunately an impossible task due to time constraints, incomplete testing, and developers not having knowledge of the full stack. As a result, mitigating risks for systems in production through hot patching and hot fixing has become an integral part of software development. In this paper, we first give an overview of the terminology used in the literature for research on this topic. Subsequently, we build upon these findings and present our vision for an automated framework for predicting and mitigating critical software issues at runtime. Our framework combines hot patching and hot fixing research from multiple fields, in particular: software defect and vulnerability prediction, automated test generation and repair, as well as runtime patching. We hope that our vision inspires research collaboration between the different communities.  © 2023 IEEE.","Prediction methods; Predictive maintenance; Repair; Software Engineering; Software maintenance","Forecasting; Program debugging; Repair; Software design; Software testing; Automated test generations; Critical software; Integral part; Mitigating risk; Prediction methods; Predictive maintenance; Runtimes; Software defects; Software vulnerabilities; Time constraints; Computer software maintenance","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85179001042"
"Lipp S.; Elsner D.; Kacianka S.; Pretschner A.; Böhme M.; Banescu S.","Lipp, Stephan (57754702800); Elsner, Daniel (57224771491); Kacianka, Severin (56875422100); Pretschner, Alexander (12645083400); Böhme, Marcel (55321057200); Banescu, Sebastian (35147584400)","57754702800; 57224771491; 56875422100; 12645083400; 55321057200; 35147584400","Green Fuzzing: A Saturation-Based Stopping Criterion using Vulnerability Prediction","2023","ISSTA 2023 - Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis","","","","127","139","12","","10.1145/3597926.3598043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167700795&doi=10.1145%2f3597926.3598043&partnerID=40&md5=70b55b6a73a4064351fb9d9993e46c62","Fuzzing is a widely used automated testing technique that uses random inputs to provoke program crashes indicating security breaches. A difficult but important question is when to stop a fuzzing campaign. Usually, a campaign is terminated when the number of crashes and/or covered code elements has not increased over a certain period of time. To avoid premature termination when a ramp-up time is needed before vulnerabilities are reached, code coverage is often preferred over crash count to decide when to terminate a campaign. However, a campaign might only increase the coverage on non-security-critical code or repeatedly trigger the same crashes. For these reasons, both code coverage and crash count tend to overestimate the fuzzing effectiveness, unnecessarily increasing the duration and thus the cost of the testing process. The present paper explores the tradeoff between the amount of saved fuzzing time and number of missed bugs when stopping campaigns based on the saturation of covered, potentially vulnerable functions rather than triggered crashes or regular function coverage. In a large-scale empirical evaluation of 30 open-source C programs with a total of 240 security bugs and 1,280 fuzzing campaigns, we first show that binary classification models trained on software with known vulnerabilities (CVEs), using lightweight machine learning features derived from findings of static application security testing tools and proven software metrics, can reliably predict (potentially) vulnerable functions. Second, we show that our proposed stopping criterion terminates 24-hour fuzzing campaigns 6-12 hours earlier than the saturation of crashes and regular function coverage while missing (on average) fewer than 0.5 out of 12.5 contained bugs.  © 2023 ACM.","empirical study; fuzzing; stopping criterion","Application programs; C (programming language); Open source software; Program debugging; Automated testing; Code coverage; Empirical studies; Function coverage; Fuzzing; Random input; Regular function; Security breaches; Stopping criterion; Testing technique; Software testing","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85167700795"
"Senanayake J.; Kalutarage H.; Al-Kadri M.O.; Petrovski A.; Piras L.","Senanayake, Janaka (57225052818); Kalutarage, Harsha (55364477000); Al-Kadri, Mhd Omar (56652325000); Petrovski, Andrei (8916414800); Piras, Luca (57213311356)","57225052818; 55364477000; 56652325000; 8916414800; 57213311356","Android Code Vulnerabilities Early Detection Using AI-Powered ACVED Plugin","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13942 LNCS","","","339","357","18","","10.1007/978-3-031-37586-6_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169056799&doi=10.1007%2f978-3-031-37586-6_20&partnerID=40&md5=9cb7299334ce482f77c689aa1ea32c08","During Android application development, ensuring adequate security is a crucial and intricate aspect. However, many applications are released without adequate security measures due to the lack of vulnerability identification and code verification at the initial development stages. To address this issue, machine learning models can be employed to automate the process of detecting vulnerabilities in the code. However, such models are inadequate for real-time Android code vulnerability mitigation. In this research, an open-source AI-powered plugin named Android Code Vulnerabilities Early Detection (ACVED) was developed using the LVDAndro dataset. Utilising Android source code vulnerabilities, the dataset is categorised based on Common Weakness Enumeration (CWE). The ACVED plugin, featuring an ensemble learning model, is implemented in the backend to accurately and efficiently detect both source code vulnerabilities and their respective CWE categories, with a 95% accuracy rate. The model also leverages explainable AI techniques to provide source code vulnerability prediction probabilities for each word. When integrated with Android Studio, the ACVED plugin can provide developers with the vulnerability status of their current source code line in real-time, assisting them in mitigating vulnerabilities. The plugin, model, and scripts can be found on GitHub, and it receives regular updates with new training data from the LVDAndro dataset, enabling the detection of novel vulnerabilities recently added to CWE. © 2023, IFIP International Federation for Information Processing.","Android application security; artificial intelligence; code vulnerability; labelled dataset; plugin","Android (operating system); Artificial intelligence; Codes (symbols); Computer programming languages; Mobile security; Open source software; Open systems; Security of data; Android application security; Android applications; Application development; Application security; Code vulnerability; Labeled dataset; Plug-ins; Real- time; Security measure; Source codes; Learning systems","Conference paper","Final","","Scopus","2-s2.0-85169056799"
"Hu Y.-T.; Wang S.-Y.; Wu Y.-M.; Zou D.-Q.; Li W.-K.; Jin H.","Hu, Yu-Tao (57880867900); Wang, Su-Yuan (58308619700); Wu, Yue-Ming (57202109788); Zou, De-Qing (8935128200); Li, Wen-Ke (57205355807); Jin, Hai (56434989100)","57880867900; 58308619700; 57202109788; 8935128200; 57205355807; 56434989100","A Slice-level vulnerability detection and interpretation method based on graph neural network; [基于图神经网络的切片级漏洞检测及解释方法]","2023","Ruan Jian Xue Bao/Journal of Software","34","6","","2204","2221","17","","10.13328/j.cnki.jos.006849","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161433606&doi=10.13328%2fj.cnki.jos.006849&partnerID=40&md5=275ff9786c3350fce0cf1a9cebd0c0f6","As software becomes more complex, the need for research on vulnerability detection is increasing. The rapid discovery and patching of software vulnerabilities is able to minimize the damage caused by vulnerabilities. As an emerging detection method, deep learning-based vulnerability detection methods can learn from the vulnerability code and automatically generate its implied vulnerability pattern, saving a lot of human effort. However, deep learning-based vulnerability detection methods are not yet perfect; function-level detection methods have a coarse detection granularity with low detection accuracy; slice-level detection methods can effectively reduce sample noise, but there are still the following two aspects of the problem: On the one hand, most of the existing methods use artificial vulnerability datasets for experiments, and the ability to detect vulnerabilities in real environments is still in doubt; on the other hand, the work is only dedicated to detecting the existence of vulnerabilities in the slice samples and the lack of interpretability of the detection results. To address above issues, this paper proposes a slice-level vulnerability detection and interpretation method based on the graph neural network. The method first normalizes the C/C++ source code and extracts slices to reduce the interference of redundant information in the samples; secondly, a graph neural network model is used to embed the slices to obtain their vector representations to preserve the structural information and vulnerability features of the source code; then the vector representations of slices are fed into the vulnerability detection model for training and prediction; finally, the trained vulnerability detection model and the vulnerability slices to be explained are fed into the vulnerability interpreter to obtain the specific lines of vulnerability code. The experimental results show that in terms of vulnerability detection, the method achieves an F1 score of 75.1% for real-world vulnerability, which is 41.2%-110.4% higher than the comparative methods. In terms of vulnerability interpretation, the method can reach 73.6% accuracy when limiting the top 10% of critical nodes, which is 8.9% and 24.9% higher than the other two interpreters, and the time overhead is reduced by 42.5% and 15.4%, respectively. Finally, this method correctly detects and explains 59 real vulnerabilities in the four open-source software, proving its practicality in real-world vulnerability discovery. © 2023 Chinese Academy of Sciences. All rights reserved.","Deep Learning; Explainable AI; Graph Neural Network; Vulnerability Detection","C++ (programming language); Deep learning; Graph neural networks; Learning systems; Network security; Open systems; Deep learning; Detection methods; Detection models; Explainable AI; Graph neural networks; Interpretation methods; Level detections; Real-world; Vector representations; Vulnerability detection; Open source software","Article","Final","","Scopus","2-s2.0-85161433606"
"Mandal D.; Kosesoy I.","Mandal, Dilek (57963916300); Kosesoy, Irfan (55293615200)","57963916300; 55293615200","Prediction of Software Security Vulnerabilities from Source Code Using Machine Learning Methods","2023","2023 Innovations in Intelligent Systems and Applications Conference, ASYU 2023","","","","","","","","10.1109/ASYU58738.2023.10296747","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178286167&doi=10.1109%2fASYU58738.2023.10296747&partnerID=40&md5=b63f6cc916526133c28166295b3a6b18","One of the most significant problems in software engineering is the presence of security vulnerabilities in software. Attackers can exploit these vulnerabilities to gain unauthorized access to systems, leak information, corrupt data, and cause service interruptions. Therefore, in addition to developing secure software, the detection of existing security vulnerabilities in software is also considered as an important research topic. In this study, security vulnerabilities in the source code of software were predicted using machine learning methods. The OWASP Benchmark Test pocket was used as the dataset. This dataset consisted of Java codes and was utilized for training machine learning models Logistic Regression, Decision Tree, Support Vector Machines, K-Nearest Neighbors, and Random Forest. TF-IDF and Doc2Vec methods were employed to extract feature vectors from the source code. In the conducted experimental study, the highest prediction accuracy (0.97) was achieved using the TF-IDF feature extraction method and the Decision Tree, SVM and Logistic Regression algorithms. © 2023 IEEE.","AST Tree; Doc2Vec; Feature Extraction; Machine Learning Algorithms; Software Vulnerability; TF-IDF","Benchmarking; Extraction; Feature extraction; Learning systems; Logistic regression; Nearest neighbor search; Random forests; Support vector regression; AST tree; Doc2vec; Features extraction; Machine learning algorithms; Machine learning methods; Security vulnerabilities; Software security; Software vulnerabilities; Source codes; TF-IDF; Decision trees","Conference paper","Final","","Scopus","2-s2.0-85178286167"
"Sharma R.; Shrivastava A.K.; Pham H.","Sharma, Ruchi (56520677700); Shrivastava, Avinash K. (55789191000); Pham, Hoang (7201637869)","56520677700; 55789191000; 7201637869","Software security evaluation using multilevel vulnerability discovery modeling","2023","Quality Engineering","35","2","","341","352","11","","10.1080/08982112.2022.2132404","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141352582&doi=10.1080%2f08982112.2022.2132404&partnerID=40&md5=853314ce431b75355ac0e3ba6d72ab85","In this work, we propose a new vulnerability discovery model by predicting the number and probability of occurrence of vulnerabilities of different severity levels in software. The severity prediction assumes that the vulnerability score is a continuous variable distributed over a range of 0–10 as per the widely accepted common vulnerability scoring system. We have further developed a risk assessment model which can be used to define the security level of software and is helpful in risk assessment and patch management. A numerical illustration is done on real-life dataset to validate the proposed model. © 2022 Taylor & Francis Group, LLC.","Modeling; risk assessment; severity; software security; vulnerability","Discovery model; Modeling; Multilevels; Probability of occurrence; Risks assessments; Security evaluation; Severity; Software security; Vulnerability; Vulnerability discovery; Risk assessment","Article","Final","","Scopus","2-s2.0-85141352582"
"Tian X.; Chang J.; Zhang C.; Rong J.; Wang Z.; Zhang G.; Wang H.; Wu G.; Hu J.; Zhang Y.","Tian, Xiao (57485972100); Chang, Jiyou (58559931600); Zhang, Chi (58560405400); Rong, Jingfeng (58560249900); Wang, Ziyu (58560561800); Zhang, Guanghua (55619309101); Wang, He (55237485600); Wu, Gaofei (55286445400); Hu, Jinglu (7406417389); Zhang, Yuqing (56027290000)","57485972100; 58559931600; 58560405400; 58560249900; 58560561800; 55619309101; 55237485600; 55286445400; 7406417389; 56027290000","Survey of Open-Source Software Defect Prediction Method; [开源软件缺陷预测方法综述]","2023","Jisuanji Yanjiu yu Fazhan/Computer Research and Development","60","7","","1467","1488","21","","10.7544/issn1000-1239.202221046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169555408&doi=10.7544%2fissn1000-1239.202221046&partnerID=40&md5=2da291f20fd61db4460df3e39092ae73","Open-source software defect prediction reduces software repair costs and improves product quality by mining data from software history warehouses, using the syntactic semantic features of metrics related to software defects or the source code itself, and utilizing machine learning or deep learning methods to find software defects in advance. Vulnerability prediction extracts and tags code modules by mining software instance repositories to predict whether new code instances contain vulnerabilities in order to reduce the cost of vulnerability discovery and fixing. We investigate and analyze the relevant literatures in the field of software defect prediction from 2000 to December 2022. Taking machine learning and deep learning as the starting point, we sort out two types of prediction models which are based on software metrics and grammatical semantics. Based on the two types of models, the difference and connection between software defect prediction and vulnerability prediction are analyzed. Moreover, six frontier hot issues such as dataset source and processing, code vector representation method, pre-training model improvement, deep learning model exploration, fine-grained prediction technology, software defect prediction and vulnerability prediction model migration are analyzed in detail. Finally, the future development direction of software defect prediction is pointed out. © 2023 Science Press. All rights reserved.","deep learning; machine learning; metric; semantic and syntactic analysis; software defect prediction; vulnerability prediction","Cost reduction; Deep learning; Defects; Forecasting; Learning systems; Open systems; Semantics; Syntactics; Deep learning; Machine-learning; Metric; Open-source softwares; Prediction modelling; Semantic analysis; Software defect prediction; Software defects; Syntactic analysis; Vulnerability prediction; Open source software","Article","Final","","Scopus","2-s2.0-85169555408"
"Kudjo P.K.; Brown S.A.; Mensah S.","Kudjo, Patrick Kwaku (57195678643); Brown, Selasie Aformaley (57215127506); Mensah, Solomon (57191254462)","57195678643; 57215127506; 57191254462","Improving software vulnerability classification performance using normalized difference measures","2023","International Journal of System Assurance Engineering and Management","14","3","","1010","1027","17","","10.1007/s13198-023-01911-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152536647&doi=10.1007%2fs13198-023-01911-6&partnerID=40&md5=1a4a9ac9fcb5311c644d96eac5df9794","Vulnerability Classification Models (VCMs) play a crucial role in software reliability engineering and hence, have attracted significant studies from researchers and practitioners. Recently, machine learning and data mining techniques have emerged as important paradigms for vulnerability classification. However, there are some major drawbacks of existing vulnerability classification models, which include difficulties in curating real vulnerability reports and their associated code fixes from large software repositories. Additionally, different types of features such as the traditional software metrics and text mining features that are extracted from term vectors are used to build vulnerability classification models, which often results in the curse of dimensionality. This significantly impacts the time required for classification and the prediction accuracy of existing vulnerability classification models. To address these deficiencies, this study presents a vulnerability classification framework using the term frequency-inverse document frequency (TF-IDF), and the normalized difference measure. In the proposed framework, the TF-IDF model is first used to compute the frequency and weight of each word from the textual description of vulnerability reports. The normalized difference measure is then employed to select an optimal subset of feature words or terms for the machine learning algorithms. The proposed approach was validated using three vulnerable software applications containing a total number of 3949 real vulnerabilities and five machine learning algorithms, namely Naïve Bayes, Naïve Bayes Multinomial, Support Vector Machines, K-Nearest Neighbor, and Decision Tree. Standard classification evaluation metrics such as precision, recall, F-measure, and accuracy were applied to assess the performance of the models and the results were validated using Welch t-test, and Cliff’s delta effect size. The outcome of this study demonstrates that normalized difference measure and k-nearest neighbor significantly improves the accuracy of vulnerability report classification. © 2023, The Author(s) under exclusive licence to The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden.","Feature selection; Normalized difference measure; Severity; Software vulnerability","Application programs; Classification (of information); Data mining; Feature Selection; Learning algorithms; Learning systems; Nearest neighbor search; Software reliability; Support vector machines; Text processing; Classification models; Classification performance; Features selection; Machine learning algorithms; Normalized difference measure; Normalized differences; Severity; Software vulnerabilities; Term frequencyinverse document frequency (TF-IDF); Vulnerability classifications; Decision trees","Article","Final","","Scopus","2-s2.0-85152536647"
"Sachdeva A.; Lazarine B.; Zhu H.; Samtani S.","Sachdeva, Agrim (57863264600); Lazarine, Ben (57205675529); Zhu, Hongyi (56304013300); Samtani, Sagar (57188838077)","57863264600; 57205675529; 56304013300; 57188838077","User Profiling and Vulnerability Introduction Prediction in Social Coding Repositories: A Dynamic Graph Embedding Approach: Vulnerability Introduction Prediction in Social Coding Repositories","2023","ACM International Conference Proceeding Series","","","","19","25","6","","10.1145/3607505.3607512","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171448656&doi=10.1145%2f3607505.3607512&partnerID=40&md5=5e5eed1da91608cfff8e39fbf2bbaef2","Social Coding Repositories (SCRs) such as GitHub host open-source code that is significant to the global economy. However, open-source code is especially vulnerable, with most vulnerabilities being introduced due to human error. An important mitigation strategy is preventing the introduction of vulnerabilities. One of the ways this can be achieved is through targeted developer security training, which necessitates the identification of high-risk actors and predicting the introduction of vulnerabilities. In this study, we propose a novel framework for predicting the introduction of vulnerabilities in SCRs by users, with a novel dynamic graph representation learning model, security continuous propagation, and evolution (seCoPE). The proposed seCoPE framework addresses the limitations of existing methods by incorporating the relative influence of nodes on the propagation of information to generate high-quality nodal embeddings. We systematically evaluate seCoPE against prevailing Recurrent Neural Network (RNN) based and Attention-based models on a vulnerability introduction dataset. The proposed framework has important implications for cybersecurity providers, firms, and software developers.  © 2023 ACM.","Dynamic Graph Representation Learning; Open-source software; Scientific Cyberinfrastructure; Security Training and Awareness; Vulnerability Management","Cybersecurity; Embeddings; Forecasting; Network security; Open systems; Recurrent neural networks; User profile; Cyberinfrastructure; Dynamic graph; Dynamic graph representation learning; Graph representation; Open-source code; Open-source softwares; Scientific cyberinfrastructure; Security awareness; Security training; Vulnerability management; Open source software","Conference paper","Final","","Scopus","2-s2.0-85171448656"
"Zhang X.-Y.; Jin W.-X.; Liu J.-W.; Fan M.; Liu T.","Zhang, Xin-Yu (57489076300); Jin, Wu-Xia (57194442597); Liu, Jing-Wen (57258728800); Fan, Ming (56259274200); Liu, Ting (55835301800)","57489076300; 57194442597; 57258728800; 56259274200; 55835301800","Evolutionary Coupling Analysis Method of Software Entity Based on Episode Mining; [结合情节挖掘的软件实体演化耦合分析方法]","2023","Ruan Jian Xue Bao/Journal of Software","34","6","","","","","","10.13328/j.cnki.jos.006853","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161396843&doi=10.13328%2fj.cnki.jos.006853&partnerID=40&md5=ed1c34f6cffea40157d85a3acb01ca7f","The entity evolution coupling analysis of software systems is helpful for analysis activities such as co-change candidate prediction, risk identification of software supply chain, code vulnerability traceability, defect prediction and architecture problem localization. The evolutionary coupling between two entities indicates that these entities tend to be changed together in the software revision history. Existing methods present a low accuracy to detect the frequent ""having distance"" co-change in the revision history. To address this problem, this paper proposes an evolutionary coupling analysis method based on the combination of Association Rules mining, Episode mining and Latent Semantic Indexing(Association Rule, MINEPI and LSI based Method, AR-MIM), which mines co-change relations of ""having distance"". The experiment verified the effectiveness of AR-MIM by compared with the four baseline methods on the dataset, collecting 58 Python projects, 242074 pieces of training data, and 330660 pieces of ground truth. The results show that the precision, recall, and F1 score of AR-MIM are better than those of existing methods in co-change candidate prediction. © 2023 Chinese Academy of Sciences. All rights reserved.","Association Rule Mining; commit history; episode mining; evolutionary coupling; LSI","Data mining; Forecasting; Risk assessment; Semantics; Supply chains; Analysis method; Commit history; Coupling analysis; Episode mining; Evolutionary coupling; LSI; Prediction risks; Risk Identification; Software entities; Software-systems; Association rules","Article","Final","","Scopus","2-s2.0-85161396843"
"Zheng Z.; Liu Y.; Zhang B.; Ren J.; Zong Y.; Wang Q.; Yang X.; Liu Q.","Zheng, Zhangqi (57200407791); Liu, Yongshan (56194246500); Zhang, Bing (56587016400); Ren, Jiadong (56012213300); Zong, Yongsheng (57208750003); Wang, Qian (56237686800); Yang, Xiaolei (57746799300); Liu, Qian (57209558097)","57200407791; 56194246500; 56587016400; 56012213300; 57208750003; 56237686800; 57746799300; 57209558097","Software defect prediction method based on the heterogeneous integration algorithm","2023","Journal of Intelligent and Fuzzy Systems","45","3","","4807","4824","17","","10.3233/JIFS-224457","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169915385&doi=10.3233%2fJIFS-224457&partnerID=40&md5=02e0bd94b8978db4e6cd92b339f21860","A software defect is a common cyberspace security problem, leading to information theft, system crashes, and other network hazards. Software security is a fundamental challenge for cyberspace security defense. However, when researching software defects, the defective code in the software is small compared with the overall code, leading to data imbalance problems in predicting software vulnerabilities. This study proposes a heterogeneous integration algorithm based on imbalance rate threshold drift for the data imbalance problem and for predicting software defects. First, the Decision Tree-based integration algorithm was designed following sample perturbation. Moreover, the Support Vector Machine (SVM)-based integration algorithm was designed based on attribute perturbation. Following the heterogeneous integration algorithm, the primary classifier was trained by sample diversity and model structure diversity. Second, we combined the integration algorithms of two base classifiers to form a heterogeneous integration model. The imbalance rate was designed to achieve threshold transfer and obtain software defect prediction results. Finally, the NASA-MDP and Juliet datasets were used to verify the heterogeneous integration algorithm's validity, correctness, and generalization based on the Decision Tree and SVM.  © 2023 - IOS Press. All rights reserved.","heterogeneous; imbalance rate; integration; Software defect; threshold shift","Codes (symbols); Defects; Forecasting; Integration; NASA; Network security; Support vector machines; Cyberspaces; Data imbalance; Heterogeneous; Heterogeneous integration; Imbalance problem; Imbalance rate; Integration algorithm; Software defect prediction; Software defects; Threshold shifts; Decision trees","Article","Final","","Scopus","2-s2.0-85169915385"
"Kamal N.; Raheja S.","Kamal, Navirah (57924119400); Raheja, Supriya (55932100200)","57924119400; 55932100200","Prediction of Software Vulnerabilities Using Random Forest Regressor","2023","Lecture Notes in Electrical Engineering","968","","","411","424","13","","10.1007/978-981-19-7346-8_35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152628999&doi=10.1007%2f978-981-19-7346-8_35&partnerID=40&md5=74cdc869765c66e7c93bb54aec6502e1","The launch of new software technologies with new features helps the developer in software development but it can be prone to vulnerabilities. Software vulnerabilities are still a critical issue for software security as they can negatively impact the organization and the end user. To mitigate this problem, various techniques have been adopted, machine learning is one of them. The main objective of this paper is to predict the severity of software vulnerabilities using a random forest regressor algorithm. To evaluate the performance, two other machine learning algorithms are also implemented for the same task. A dataset of the National Vulnerabilities database is used for the present work. The efficacy of the models has been evaluated and compared using four different performance metrics namely mean absolute error, mean square error, root mean square error, and R2 score. Random forest regressor performed the best out of the applied machine learning algorithms with a root mean square error of 0.01945. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2023.","Decision tree; K-nearest; Machine learning techniques; Random forest; Software development; Software security; Vulnerabilities","Errors; Learning algorithms; Machine learning; Mean square error; Software design; Critical issues; K-near; Machine learning algorithms; Machine learning techniques; Random forests; Root mean square errors; Software security; Software technology; Software vulnerabilities; Vulnerability; Decision trees","Conference paper","Final","","Scopus","2-s2.0-85152628999"
"Chalabi R.; Yazdanpanah O.; Dolatshahi K.M.","Chalabi, Romina (58632810800); Yazdanpanah, Omid (55943079500); Dolatshahi, Kiarash M. (48761203100)","58632810800; 55943079500; 48761203100","Nonmodel rapid seismic assessment of eccentrically braced frames incorporating masonry infills using machine learning techniques","2023","Journal of Building Engineering","79","","107784","","","","","10.1016/j.jobe.2023.107784","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173237099&doi=10.1016%2fj.jobe.2023.107784&partnerID=40&md5=2d42e9b3cee4d9a7b158eb410f5647b8","This study investigates the seismic behavior of eccentrically braced frames (EBFs) taking into account the influence of masonry infill walls using a nonmodel scenario-based machine learning framework. Predicting engineering demand parameters (EDPs) by exploiting data-driven approaches and developing the associated fragility curves of infilled EBFs are the main objectives of this research. To this end, a set of 4- and 8-story archetype EBF structures considering 12 distinct infill properties, a total number of 4 bare and 48 infilled EBF models, are created in OpenSees software. A nonlinear pushover analysis is then conducted to assess the overall impact of infills on 4- and 8-story EBFs. Incremental dynamic analysis under 44 far-field ground motions is performed to collect an extensive raw database consisting of 37,164 and 461,480 data points for each variable of bare and infilled EBFs, respectively. It encompasses seismic intensity measure (Saavg), wavelet-based robust damage sensitive feature (rDSF), derived from roof absolute acceleration, frame geometric information, and infill parameter as input features and also EDPs containing peak and residual story drift ratios and peak floor accelerations (PFA) as response variables. After data preprocessing, both regression analysis and machine learning (ML) techniques are utilized to estimate EDPs under two scenarios of Input-Output and Output-Only based on the availability of Saavg. Comparing error measures, particularly in the Extreme Gradient Boosted Tree (ExGBT) algorithm, reveals strong observed-predicted EDP compatibility. Linear equations from bare frame data predict infilled frame EDPs without modification coefficients. Data preprocessing demonstrates that infills decrease drift-based EDPs due to stiffness improvement, while their impact on PFA remains inconclusive. Saavg and rDSF-based fragility curves at three damage states show accurate predictions and effectively reduce vulnerability in infilled 4-story EBFs. However, infills undesirably affect non-ductile behavior in 8-story EBFs, leading to increased global damage as supported by pushover analysis. © 2023 Elsevier Ltd","Eccentrically braced frames; Engineering demand parameters; Fragility curves; Machine learning techniques; Masonry infill walls; Nonmodel approach","Acceleration; Forecasting; Infill drilling; Learning algorithms; Machine learning; Masonry materials; Seismology; Structural frames; Trees (mathematics); Damage-sensitive features; Eccentrically braced frames; Engineering demand parameters; Fragility curves; Infill walls; Machine learning techniques; Masonry infill wall; Masonry infills; Nonmodel approach; Peak floor; Regression analysis","Article","Final","","Scopus","2-s2.0-85173237099"
"Pakshad P.; Shameli-Sendi A.; Khalaji Emamzadeh Abbasi B.","Pakshad, Puya (58107947700); Shameli-Sendi, Alireza (55308191100); Khalaji Emamzadeh Abbasi, Behzad (58107831700)","58107947700; 55308191100; 58107831700","A security vulnerability predictor based on source code metrics","2023","Journal of Computer Virology and Hacking Techniques","19","4","","615","633","18","","10.1007/s11416-023-00469-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148378492&doi=10.1007%2fs11416-023-00469-y&partnerID=40&md5=b89a02f62a03d44b332f48c520ff0c11","Detecting security vulnerabilities in the source code of software systems is one of the most important challenges in the field of software security. We need an effective solution to discover and patch vulnerabilities before our valuable information is compromised. Security testing is a type of software testing that checks whether software is vulnerable to cyber attacks. This study aimed to pursue three main objectives: (1) The first goal is to identify the vulnerable functions of a C/C++ software program based on code metrics. This can reduce the cost of software security testing and also redirect the related activities to identified vulnerable functions rather than to the entire software, (2) The second goal is to identify the type of attack related to the vulnerability function, and (3) Finally, the ultimate goal is to analyze the relationship between code metrics and the vulnerabilities. This goal can help us understand which code structure is most likely to contain vulnerable code. This paper first aimed to create a comprehensive view of the source code of the target software using graph concepts. Second, a set of source code metrics and calculated by crawling on the related graph using the static analysis approach. Finally, the vulnerability prediction model presented in this paper is based on machine learning technique applied on metrics extracted from program source code. Compared to previous work, new achievements have been made in this paper. One of the most important ones is the very high accuracy detection of the proposed model in detecting the type of vulnerability. Moreover, 15 code metrics are used to predict vulnerabilities. Our analysis on feature importance indicates that what structure the software program code has, most likely, it will be vulnerable. Experimental results in 10 real projects (OpenSSL, SQLite, FreeType, LibTiff, Libxslt, Binutils, FFmpeg, ImageMagick, OpenSC, and rdesktop) indicated that the security testing predictor proposed in this paper could predict on average 89% of the really vulnerable functions of the source code and 86% of the vulnerability type of the detected functions correctly. © 2023, The Author(s), under exclusive licence to Springer-Verlag France SAS, part of Springer Nature.","Code property graph; Program metric; Security testing; Vulnerability detection","C++ (programming language); Codes (symbols); Cybersecurity; Forecasting; Learning systems; Network security; Software testing; Code metrics; Code property graph; Program metric; Property; Security testing; Security vulnerabilities; Software project; Source code metrics; Source codes; Vulnerability detection; Static analysis","Article","Final","","Scopus","2-s2.0-85148378492"
"Siavvas M.; Tsoukalas D.; Kalouptsoglou I.; Manganopoulou E.; Manolis G.; Kehagias D.; Tzovaras D.","Siavvas, Miltiadis (57194500913); Tsoukalas, Dimitrios (57208865760); Kalouptsoglou, Ilias (57219327969); Manganopoulou, Evdoxia (58101357600); Manolis, Georgios (7006170934); Kehagias, Dionysios (7003972544); Tzovaras, Dimitrios (13105681700)","57194500913; 57208865760; 57219327969; 58101357600; 7006170934; 7003972544; 13105681700","Security Monitoring during Software Development: An Industrial Case Study","2023","Applied Sciences (Switzerland)","13","12","6872","","","","","10.3390/app13126872","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163984213&doi=10.3390%2fapp13126872&partnerID=40&md5=e48d44c7cdee85fcb67a1f70e66ef11e","The devastating consequences of successful security breaches that have been observed recently have forced more and more software development enterprises to shift their focus towards building software products that are highly secure (i.e., vulnerability-free) from the ground up. In order to produce secure software applications, appropriate mechanisms are required for enabling project managers and developers to monitor the security level of their products during their development and identify and eliminate vulnerabilities prior to their release. A large number of such mechanisms have been proposed in the literature over the years, but limited attempts with respect to their industrial applicability, relevance, and practicality can be found. To this end, in the present paper, we demonstrate an integrated security platform, the VM4SEC platform, which exhibits cutting-edge solutions for software security monitoring and optimization, based on static and textual source code analysis. The platform was built in a way to satisfy the actual security needs of a real software development company. For this purpose, an industrial case study was conducted in order to identify the current security state of the company and its security needs in order for the employed security mechanisms to be adapted to the specific needs of the company. Based on this analysis, the overall architecture of the platform and the parameters of the selected models and mechanisms were properly defined and demonstrated in the present paper. The purpose of this paper is to showcase how cutting-edge security monitoring and optimization mechanisms can be adapted to the needs of a dedicated company and to be used as a blueprint for constructing similar security monitoring platforms and pipelines. © 2023 by the authors.","experience report; security by design; security monitoring; software security; verification and validation; vulnerability prediction","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85163984213"
"Liu D.; Chen J.; Liu X.; Chang Y.; Li Z.; Wang R.; Yu H.; Zhang F.; Yao H.; Zhang H.","Liu, Donglan (56130894900); Chen, Jianfei (57212281958); Liu, Xin (57022611500); Chang, Yingxian (57218352748); Li, Zhenghao (57996518000); Wang, Rui (57092342500); Yu, Hao (57202712779); Zhang, Fangzhe (57767032400); Yao, Honglei (57258144300); Zhang, Hao (57203040832)","56130894900; 57212281958; 57022611500; 57218352748; 57996518000; 57092342500; 57202712779; 57767032400; 57258144300; 57203040832","Research on Intelligent Fuzz Testing Technology for Power Internet of Things Terminal","2023","ICEIEC 2023 - Proceedings of 2023 IEEE 13th International Conference on Electronics Information and Emergency Communication","","","","218","222","4","","10.1109/ICEIEC58029.2023.10199463","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168314440&doi=10.1109%2fICEIEC58029.2023.10199463&partnerID=40&md5=455c7be99137c64a4ba5e39930abb60c","For many power terminal devices, firmware binary code can be extracted directly from the device by different methods. And the binary firmware program security test, so as to find the Internet of Things terminal device security vulnerabilities. In this paper, the intelligent fuzz testing technology for power Internet of Things terminal is studied. Firstly, the basic framework of binary dynamic piling is studied, and the bottleneck of the existing framework is analyzed. In order to improve the performance of firmware program in dynamic pile driving environment, a dynamic simulation and dynamic pile driving framework combining user level simulation and system level simulation are proposed. Based on this framework, the pile driving method oriented to program coverage and the pile driving method oriented to program vulnerability guidance are studied respectively, which provides support for cross-platform fuzz testing. Secondly, dynamic stain analysis techniques are studied from both offline and online aspects. The advantage of offline dynamic smudge analysis is that it can fine-grained how data is propagated in memory during program execution, and can assist in the generation of initial test seeds. The advantage of online dynamic blot analysis is that it does not affect the performance of simulation test, and can dynamically infer the relationship between input bytes and constraints, so as to guide fuzz testing to carry out targeted variation. Finally, the intelligent prediction model of program vulnerability points is studied to intelligently locate potential defects in binary firmware programs. In order to solve these potential defects, the fuzz testing based on program coverage and potential vulnerability guidance is used, and the particle swarm optimization algorithm is used to optimize. © 2023 IEEE.","fuzz testing; intelligent fuzz testing technology; power Internet of things protocol; vulnerability detection","Automobile drivers; Defects; Firmware; Particle swarm optimization (PSO); Pile driving; Piles; Software testing; Driving method; Fuzz Testing; Intelligent fuzz testing technology; Performance; Power; Power internet of thing protocol; Terminal devices; Testing technology; Vulnerability detection; Internet of things","Conference paper","Final","","Scopus","2-s2.0-85168314440"
"Chetouane A.; Karoui K.","Chetouane, Ameni (57214825372); Karoui, Kamel (25960919600)","57214825372; 25960919600","Risk based intrusion detection system in software defined networking","2023","Concurrency and Computation: Practice and Experience","","","","","","","","10.1002/cpe.7988","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179694058&doi=10.1002%2fcpe.7988&partnerID=40&md5=a80df436f7004bc5199d3e4b7f61493c","Software defined networking (SDN) separates control from data operations. However, this technology adds a new security cost to the network architecture because of the ongoing and developing security vulnerabilities. An intrusion detection system must be continuously improved and integrated into the SDN architecture in order to provide a network defense against attacks. In this study, we propose a continual learning system based on risk assessment to detect intrusion in SDN. We suggest a technique for continually enhancing datasets to produce a more accurate prediction. The proposed system includes various processes, including risk assessment and the selection of the deep learning (DL) approach. We propose assessing the risks related to different intrusion types. Based on the risk value, we can identify which intrusion types are more important and have a dangerous impact. We use the risk values to choose the most appropriate DL approach and for the dataset's continual enrichment. We compare different DL methods using the standard metrics and two proposed metrics. Then, we propose to use a method based on the bit alternation approach to obtain a unique metric for decision-making. Finally, we have studied the efficacy of our system using two case studies. © 2023 John Wiley & Sons Ltd.","continual learning (CL); countermeasure; deep learning (DL); impact; likelihood; network security; risk assessment; security threats; software defined networking (SDN)","Computer crime; Decision making; Deep learning; Intrusion detection; Learning systems; Network architecture; Risk assessment; Software defined networking; Continual learning; Countermeasure; Deep learning; Impact; Likelihood; Networks security; Risks assessments; Security threats; Software defined networking; Software-defined networkings; Network security","Article","Article in press","","Scopus","2-s2.0-85179694058"
"Bassi D.; Singh H.","Bassi, Deepali (57764927900); Singh, Hardeep (58376239500)","57764927900; 58376239500","The Effect of Dual Hyperparameter Optimization on Software Vulnerability Prediction Models","2023","E-Informatica Software Engineering Journal","17","1","230102","","","","","10.37190/e-Inf230102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152945673&doi=10.37190%2fe-Inf230102&partnerID=40&md5=c91ea4574c2118889d80c53dee660182","Background: Prediction of software vulnerabilities is a major concern in the field of software security. Many researchers have worked to construct various software vulnerability prediction (SVP) models. The emerging machine learning domain aids in building effective SVP models. The employment of data balancing/resampling techniques and optimal hyperparameters can upgrade their performance. Previous research studies have shown the impact of hyperparameter optimization (HPO) on machine learning algorithms and data balancing techniques. Aim: The current study aims to analyze the impact of dual hyperparameter optimization on metrics-based SVP models. Method: This paper has proposed the methodology using the python framework Optuna that optimizes the hyperparameters for both machine learners and data balancing techniques. For the experimentation purpose, we have compared six combinations of five machine learners and five resampling techniques considering default parameters and optimized hyperparameters. Results: Additionally, the Wilcoxon signed-rank test with the Bonferroni correction method was implied, and observed that dual HPO performs better than HPO on learners and HPO on data balancers. Furthermore, the paper has assessed the impact of data complexity measures and concludes that HPO does not improve the performance of those datasets that exhibit high overlap. Conclusion: The experimental analysis unveils that dual HPO is 64% effective in enhancing the productivity of SVP models. © 2023 The Authors. Published by Wrocław University of Science and Technology Publishing House.","data balancing techniques; data complexity measures; hyperparameter optimization; machine learning algorithm; software vulnerability","Balancing; Forecasting; Learning algorithms; Machine learning; Balancing techniques; Complexity measures; Data balancing technique; Data complexity; Data complexity measure; Hyper-parameter; Hyper-parameter optimizations; Machine learning algorithms; Prediction modelling; Software vulnerabilities; Python","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85152945673"
"Simic V.; Stojkovic M.; Milivojevic N.; Bacanin N.","Simic, Visnja (57210568773); Stojkovic, Milan (56276493100); Milivojevic, Nikola (57172672700); Bacanin, Nikola (58238094000)","57210568773; 56276493100; 57172672700; 58238094000","Assessing water resources systems’ dynamic resilience under hazardous events via a genetic fuzzy rule-based system","2023","Journal of Hydroinformatics","25","2","","318","331","13","","10.2166/hydro.2023.101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158153348&doi=10.2166%2fhydro.2023.101&partnerID=40&md5=2870a5675f86135dc16cca1502d9c91d","In this paper, the use of a novel genetic fuzzy rule-based system (FRBS) is proposed for assessing the resilience of a water resources system to hazards. The proposed software framework generates a set of highly interpretable rules that transparently represent the causal relationships of hazardous events, their timings, and intensities that can lead to the system’s failure. This is achieved automatically through an evolutionary learning procedure that is applied to the data acquired from system dynamics (SD) and hazard simulations. The proposed framework for generating an explainable predictive model of water resources system resilience is applied to the Pirot water resources system in the Republic of Serbia. The results indicate that our approach extracted high-level knowledge from the large datasets derived from multi-model simulations. The rule-based knowledge structure facilitates its common-sense interpretation. The presented approach is suitable for identifying scenario components that lead to increased system vulnerability, which are very hard to detect from massive raw data. The fuzzy model also proves to be a satisfying fuzzy classifier, exhibiting precisions of 0.97 and 0.96 in the prediction of low resilience and high rapidity, respectively. © 2023 The Authors.","fuzzy rule-based system; genetic algorithm; water system resilience","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85158153348"
"Zhang D.; Xian H.; Chen J.; Xu Z.","Zhang, Dongping (58658371700); Xian, Hequn (24328872300); Chen, Jiyang (57215199529); Xu, Zhiguo (58658020200)","58658371700; 24328872300; 57215199529; 58658020200","VDCNet: A Vulnerability Detection and Classification System in Cross-Project Scenarios","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","14254 LNCS","","","305","316","11","","10.1007/978-3-031-44207-0_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174622827&doi=10.1007%2f978-3-031-44207-0_26&partnerID=40&md5=d4dfa33483859dab3a432e91433301fa","The existence of software vulnerabilities is the primary cause of most security incidents in cyberspace. Timely detection of potential vulnerabilities from source code during the software development stage is a critical issue for developers. With the increasing scale of open-source projects, traditional static analysis tools are becoming more and more unreliable and stagnant in their development. Meanwhile, approaches for vulnerability detection based on deep learning are being investigated. This paper introduces a novel deep learning-based vulnerability detection system, VDCNet, to identify and classify multiple vulnerabilities more effectively. We extract advanced semantic information from AST representations of source code and capture patterns of vulnerable functions by training neural networks. VDCNet constructs a BERT model for embedding and a Bi-LSTM network for prediction. The experimental results on a comprehensive dataset demonstrate that our method is more efficient in binary vulnerability detection than other deep learning-based methods, with outstanding multi-classification performance in cross-project scenarios. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Code representation; Deep learning; Feature extraction; Vulnerability detection","Classification (of information); Codes (symbols); Feature extraction; Learning systems; Long short-term memory; Network security; Open source software; Semantics; Software design; Classification system; Code representation; Deep learning; Detection system; Features extraction; Security incident; Software vulnerabilities; Source codes; Vulnerability classifications; Vulnerability detection; Static analysis","Conference paper","Final","","Scopus","2-s2.0-85174622827"
"Sultana K.Z.; Anu V.; Chong T.-Y.","Sultana, Kazi Zakia (23494078600); Anu, Vaibhav (57191251672); Chong, Tai-Yin (57215345965)","23494078600; 57191251672; 57215345965","Using software metrics for predicting vulnerable classes in java and python based systems","2023","Information Security Journal","","","","","","","","10.1080/19393555.2023.2240343","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175105286&doi=10.1080%2f19393555.2023.2240343&partnerID=40&md5=28ff46a60f13b6bbe0bada94db02cdec","[Context:] Failure to predict vulnerability in the earlier stage of development can cause vulnerable code being written and deployed in the final software product. Vulnerability prediction using software metrics as features can support the discovery process by localizing vulnerable code. Existing studies have successfully employed metrics for vulnerability prediction for some platforms (C/C++ or Java projects). We propose that a comparative evaluation of how these metrics perform in projects of different languages can help the developers in deciding whether metrics-based prediction approach can be effective in their own project’s context. [Objective:] The purpose of this research is to analyze/compare the performance of software metrics in vulnerability-prediction for different programming language contexts (Java vs. Python). [Method:] We conducted experiments on vulnerabilities reported for Apache Tomcat (releases 6 and 7), Apache CXF, and two Python projects (Django and Keystone). We applied machine learning for predicting a particular type of code component (Java and Python classes) as vulnerable/non-vulnerable. [Results:] We found that metrics-based prediction can predict Java vulnerable classes with higher recall and precision than the Python vulnerable classes. [Conclusion:] This study at class-level will help developers to predict vulnerabilities at the class-level and assist in secure coding in object-oriented programming. © 2023 Taylor & Francis Group, LLC.","Machine learning; software metrics; software security; vulnerability prediction","C++ (programming language); Computer software; Forecasting; Java programming language; Machine learning; Object oriented programming; Apache tomcats; Applied machine learning; Class level; Comparative evaluations; Machine-learning; Performance; Software metrics; Software products; Software security; Vulnerability prediction; Python","Article","Article in press","","Scopus","2-s2.0-85175105286"
"Miyata R.; Washizaki H.; Sumoto K.; Yoshioka N.; Fukazawa Y.; Okubo T.","Miyata, Rikuho (58554896500); Washizaki, Hironori (8905784000); Sumoto, Kensuke (57911142300); Yoshioka, Nobukazu (8105005200); Fukazawa, Yoshiaki (7101986896); Okubo, Takao (36053580600)","58554896500; 8905784000; 57911142300; 8105005200; 7101986896; 36053580600","Identifying missing relationships of CAPEC attack patterns by transformer models and graph structure","2023","Proceedings - 2023 IEEE/ACM 1st International Workshop on Software Vulnerability, SVM 2023","","","","14","17","3","","10.1109/SVM59160.2023.00008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169086765&doi=10.1109%2fSVM59160.2023.00008&partnerID=40&md5=5d7258c08978e5851bc5bf0ab7ace18f","As threats to software vulnerabilities diversify, countermeasures against various threat patterns become more critical. The Common Attack Pattern Enumeration and Classification (CAPEC) is a catalog of security attack patterns that helps understand what attacks can be launched against these vulnerabilities. CAPEC defines relationships between attack patterns, but these are manually associated so that some may be missed. This paper proposes a method to identify missed relationships using the transformer model and existing relational graph structures. Specifically, pre-trained models are fine-tuned using BERT and Longformer based on the names and descriptions of the two attack patterns and their relationships. Then missed relationships are identified by the classification task, and graph structure rules are defined for the identified relations to determine whether they are graph-structurally correct. Finally, whether the relations are semantically correct is verified. Our evaluation found that 41 likely relationships were missed. © 2023 IEEE.","BERT; CAPEC; Longformer; relation prediction; Transformer","Attack patterns; BERT; Common attack pattern enumeration and classification; Graph structures; Longformer; Relation prediction; Security attacks; Software vulnerabilities; Transformer; Transformer modeling; Graphic methods","Conference paper","Final","","Scopus","2-s2.0-85169086765"
"Liu P.; Ding Z.; Zi H.; Chen Y.; Liu Z.","Liu, Peng (57217248352); Ding, Zude (35213295800); Zi, Hao (57217312182); Chen, Yusheng (57373227900); Liu, Zhengchu (57224521462)","57217248352; 35213295800; 57217312182; 57373227900; 57224521462","Seismic vulnerability analysis of shield tunnel based on random IDA and machine learning algorithms","2023","Journal of Railway Science and Engineering","20","12","","4848","4860","12","","10.19713/j.cnki.43-1423/u.T20230157","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180880531&doi=10.19713%2fj.cnki.43-1423%2fu.T20230157&partnerID=40&md5=b343830f905e1ed0f5fa05829e7ba3be","The vulnerability analysis of tunnels involves randomness and uncertainty in seismic motion, structure, and geotechnical parameters. When considering the uncertainty of multiple parameters, establishing the seismic vulnerability curve of the structure based on numerical methods is computationally expensive and requires a large number of random seismic response calculations. To balance the objectivity of the tunnel seismic vulnerability results with computational efficiency, an automatic random IDA script program, coded in Python, was developed for automatic analysis and post-processing of tunnel random seismic response. Ten machine learning algorithms were established for predicting structural earthquake damage via feature selection, dataset partitioning, preprocessing, and parameter tuning. The predictive performance of the algorithms was compared and analyzed, including the sensitivity of soil random parameters to structural vulnerability. The tunnel vulnerability curve, established based on machine learning prediction, generally agreed with the numerical method results, demonstrating the effectiveness of machine learning algorithms in predicting tunnel seismic vulnerability, particularly the BPNN algorithm, which is highly reliable, with close proximity to the numerical method. Machine learning algorithms, notably the BPNN algorithm model, which are less susceptible to sample quantity, have potential and good applicability in vulnerability prediction. With respect to the sensitivity of soil physical and mechanical parameters to shield tunnel seismic vulnerability, there is an order of decreasing importance: elastic modulus, Poisson’ s ratio, density, damping, friction angle, and cohesion. © 2023, Central South University Press. All rights reserved.","machine learning algorithms; random IDA methods; seismic response models; seismic susceptibility curves; shield tunnels","Computational efficiency; Computer software; Forecasting; Learning algorithms; Learning systems; Machine learning; Parameter estimation; Seismic response; Shielding; Uncertainty analysis; Machine learning algorithms; Random IDA method; Random seismic response; Response model; Seismic response model; Seismic susceptibility curve; Seismic vulnerability; Shield tunnel; Uncertainty; Vulnerability analysis; Numerical methods","Article","Final","","Scopus","2-s2.0-85180880531"
"Greco C.","Greco, Claudia (57242868100)","57242868100","IoT Security: Exploring Strategies and Approaches","2023","CEUR Workshop Proceedings","3478","","","703","710","7","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173461542&partnerID=40&md5=e522e940ca60ba72d03952493427129c","That of IoT security is an extensive and tough field, which presents significant challenges that can be tackled from various perspectives, ranging from defensive to offensive approaches. The purpose of my research is to explore new attack vectors and protective techniques to address the myriad vulnerabilities plaguing IoT devices, specifically focusing on software security. This goal is pursued by exploring multiple aspects of IoT security, including Binary Analysis of firmware, IoT Penetration Testing and Vulnerability Assessment, and Attack Detection and Prediction, and Code Obfuscation. © 2023 CEUR-WS. All rights reserved.","Attack Detection and Prediction; Code Obfuscation; Firmware Re-hosting; IoT Security; Penetration Testing; Vulnerability Discovery","Data obfuscation; Internet of things; Attack detection; Attack prediction; Attack vector; Binary analysis; Code obfuscation; Firmware re-hosting; IoT security; Penetration testing; Software security; Vulnerability discovery; Firmware","Conference paper","Final","","Scopus","2-s2.0-85173461542"
"Long K.; Guo J.; Yang K.","Long, Kai (57855746100); Guo, Junjun (55978093400); Yang, Kunpeng (58242293300)","57855746100; 55978093400; 58242293300","Learning to Predict Tendency of CVE Vulnerability","2023","Proceedings - 2023 6th International Conference on Computer Network, Electronic and Automation, ICCNEA 2023","","","","188","192","4","","10.1109/ICCNEA60107.2023.00048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177555755&doi=10.1109%2fICCNEA60107.2023.00048&partnerID=40&md5=c257f321ad238cef3f4216b8a59eb0eb","Predicting the trend of software vulnerabilities enables proactive warnings, aiding in mitigating security incident losses. Capturing the long-Term dependencies among data is crucial for achieving highly accurate predictions. Leveraging the remarkable data relationship-capturing capabilities of learning-based methods demonstrated in various domains, we implemented several learning-based approaches to forecast the outbreak trend of software vulnerabilities. The results show that Informer, Autoformer, and DLinear achieved accuracy rates exceeding 90%. Informer, with its residual connection module, obtained significant prediction results with fewer data points. Autoformer, benefiting from its data decomposition structure, performed better in datasets with periodic patterns. DLinear, on the other hand, exhibited superior performance in relatively stable datasets. However, LSTM performed poorly across multiple datasets due to the cumulative error effect. © 2023 IEEE.","Prediction; Security prevention; Time series; Vulnerability","Long short-term memory; Accurate prediction; Data relationships; Highly accurate; Learning-based methods; Long-term dependencies; Security incident; Security prevention; Software vulnerabilities; Times series; Vulnerability; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85177555755"
"Ryan G.; Shah A.; She D.; Jana S.","Ryan, Gabriel (57203547108); Shah, Abhishek (57219177367); She, Dongdong (57052523200); Jana, Suman (26221197900)","57203547108; 57219177367; 57052523200; 26221197900","Precise Detection of Kernel Data Races with Probabilistic Lockset Analysis","2023","Proceedings - IEEE Symposium on Security and Privacy","2023-May","","","2086","2103","17","","10.1109/SP46215.2023.10179366","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166468055&doi=10.1109%2fSP46215.2023.10179366&partnerID=40&md5=5743f9ac786db627dfb770af9cddde39","Finding data races is critical for ensuring security in modern kernel development. However, finding data races in the kernel is challenging because it requires jointly searching over possible combinations of system calls and concurrent execution schedules. Kernel race testing systems typically perform this search by executing groups of fuzzer seeds from a corpus and applying a combination of schedule fuzzing and dynamic race prediction on traces. However, predicting which combinations of seeds can expose races in the kernel is difficult as fuzzer seeds will usually follow different execution paths when executed concurrently due to inter-thread communications and synchronization.To address this challenge, we introduce a new analysis for kernel race prediction, Probabilistic Lockset Analysis (PLA) that addresses the challenges posed by race prediction for the kernel. PLA leverages the observation that system calls almost always perform certain memory accesses to shared memory to perform their function. PLA uses randomized concurrent trace sampling to identify memory accesses that are performed consistently and estimates the probability of races between them subject to kernel lock synchronization. By prioritizing high probability races, PLA is able to make accurate predictions.We evaluate PLA against comparable kernel race testing methods and show it finds races at a 3× higher rate over 24 hours. We use PLA to find 183 races in linux kernel v5.18-rc5, including 102 harmful races. PLA is able to find races that have severe security impact in heavily tested core kernel modules, including use-after-free in memory management, OOB write in network cryptography, and leaking kernel heap memory information. Some of these vulnerabilities have been overlooking by existing systems for years: one of the races found by PLA involving an OOB write has been present in the kernel since 2013 (version v3.14-rc1) and has been designated a high severity CVE.  © 2023 IEEE.","concurrent-program-testing; kernel-security; software-testing; systems-security","Forecasting; Information management; Linux; Locks (fasteners); Concurrent execution; Concurrent program testing; Data races; Kernel development; Kernel-security; Memory access; Probabilistics; Software testings; System calls; System security; Software testing","Conference paper","Final","","Scopus","2-s2.0-85166468055"
"Yin J.; Chen G.; Hong W.; Wang H.; Cao J.; Miao Y.","Yin, Jiao (54884588500); Chen, Guihong (57202736210); Hong, Wei (58035044400); Wang, Hua (57215111932); Cao, Jinli (7403353999); Miao, Yuan (7101982230)","54884588500; 57202736210; 58035044400; 57215111932; 7403353999; 7101982230","Empowering Vulnerability Prioritization: A Heterogeneous Graph-Driven Framework for Exploitability Prediction","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","14306 LNCS","","","289","299","10","","10.1007/978-981-99-7254-8_23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175948084&doi=10.1007%2f978-981-99-7254-8_23&partnerID=40&md5=d17f9bfd8dda73e138b7ee3a3b42ed5c","With the increasing number of software vulnerabilities being disclosed each year, prioritizing them becomes essential as it is challenging to patch all of them promptly. Exploitability prediction plays a crucial role in assessing the severity of vulnerabilities and determining their prioritization. Most existing works on exploitability prediction focus on building predictive models based on features extracted from individual vulnerabilities, neglecting the relationships between vulnerabilities and their contextual information. Only a few studies have explored using homogeneous graph-based techniques to enhance performance in this domain. This paper proposes a novel heterogeneous graph-driven framework for enhancing vulnerability exploitability prediction. The framework comprises two heterogeneous graph feature extraction technique streams: topological feature concatenation and node embedding based on heterogeneous graph neural networks (HGNN). Experimental results demonstrate that both streams, leveraging heterogeneous graph-based features, significantly improve the performance of exploitability prediction compared with using features extracted from individual vulnerabilities. Specifically, the two streams achieve 5.44% and 2.06% improvement in the F1 score, respectively. The data and codes are available on GitHub (https://github.com/happyResearcher/HG-VEP ) to facilitate reproducibility and further research in this field. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Exploitability prediction; Graph neural networks; Heterogeneous graph; Software vulnerability","Graph neural networks; Graphic methods; Topology; Contextual information; Exploitability prediction; Graph neural networks; Heterogeneous graph; Model-based OPC; Performance; Predictive models; Prioritization; Software vulnerabilities; Vulnerability prioritization; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85175948084"
"Guo J.; Wang Z.; Li H.; Xue Y.","Guo, Junjun (55978093400); Wang, Zhengyuan (57225154088); Li, Haonan (57225147070); Xue, Yang (57225149369)","55978093400; 57225154088; 57225147070; 57225149369","Detecting vulnerability in source code using CNN and LSTM network","2023","Soft Computing","27","2","","1131","1141","10","","10.1007/s00500-021-05994-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109335336&doi=10.1007%2fs00500-021-05994-w&partnerID=40&md5=6984a53337a52ea54458c35f3d12bd71","Automated vulnerability detection has become a research hot spot because it is beneficial for improving software quality and security. The code metric (CM) is one class of important representations of vulnerability in source code. The implicit relationships among different metric attributes have not been sufficiently considered in traditional vulnerability detection based on CMs. In this paper, in view of the local perception capability of convolutional neural network (CNN) and the time-series prediction capability of long short-term memory (LSTM), we propose VulExplore, a compound neural network model for vulnerability detection that consists of a CNN for feature extraction and an LSTM network for deep representation. Moreover, to further indicate the vulnerability features in the source code, we reconstruct a CM dataset that includes two additional important attributes: maintainability index and average number of vulnerabilities committed per line. Our proposed numerical method can obtain both false-negative rate (FNR) and false-positive rate (FPR) under 20% and, meanwhile, achieve recall and precision over 80%, respectively. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Code metrics (CMs); Convolutional neural network (CNN); Long short-term memory (LSTM); Source code; Vulnerability detection","Brain; Computer software selection and evaluation; Convolution; Convolutional neural networks; Feature extraction; Network coding; Network security; Numerical methods; Code metric; Code metrics; Convolutional neural network; Hotspots; Long short-term memory; Memory network; Software Quality; Source codes; Vulnerability detection; Long short-term memory","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85109335336"
"Malhotra R.; Vidushi V.","Malhotra, Ruchika (15758058000); Vidushi, Vidushi (58132440000)","15758058000; 58132440000","Impact of Word Embedding Methods on Software Vulnerability Severity Prediction Models","2023","Proceedings of the 13th International Conference on Cloud Computing, Data Science and Engineering, Confluence 2023","","","","293","297","4","","10.1109/Confluence56041.2023.10048868","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149518205&doi=10.1109%2fConfluence56041.2023.10048868&partnerID=40&md5=85de7352a18cd65cab60e908c4e59584","Software vulnerability severity prediction has recently gained popularity in software engineering. To perform this task, a prediction model has to be developed. The model's performance greatly depends on the feature vectors used to train it. These feature vectors are formed using textual data present on software vulnerability and converting it to numerical form. There are various methods for feature vector representation using word embeddings. Unlike traditional methods such as TF-IDF, word embedding methods using deep learning map the words into vectors, preserving the semantic relationships between words as well as doing automatic feature selection, which holds great importance when dealing with textual data. Therefore, word embedding methods using deep learning showed promising results over traditional TF-IDF. Hence, in this paper, we conducted a controlled experiment to examine the effect of various word embedding methods for feature vector representation on the performance of the prediction model. The models developed in this study used TF-IDF, word2vec, and GloVe, coupled with SVM, LSTM, and Bi-LSTM classification algorithms. The classification performance of different word embedding methods is analyzed on five different vulnerability datasets of Mozilla products. The experiments showed that the best performance is achieved by using the GloVe method with the Bi-LSTM algorithm giving an average AUC value of 0.81, which is an improvement of 12.5% from the traditional method of feature vector representation. © 2023 IEEE.","Feature Vectors; Prediction Model; Software Vulnerability Severity; Word Embeddings","Classification (of information); Embeddings; Forecasting; Long short-term memory; Semantics; Software engineering; Support vector machines; Embedding method; Embeddings; Features vector; Performance; Prediction modelling; Software vulnerabilities; Software vulnerability severity; Textual data; Vector representations; Word embedding; Vectors","Conference paper","Final","","Scopus","2-s2.0-85149518205"
"Lucchetti F.; Graczyk R.; Völp M.","Lucchetti, Federico (57671578400); Graczyk, Rafal (24174282000); Völp, Marcus (24465745800)","57671578400; 24174282000; 24465745800","Toward resilient autonomous driving—An experience report on integrating resilience mechanisms into the Apollo autonomous driving software stack","2023","Frontiers in Computer Science","5","","1125055","","","","","10.3389/fcomp.2023.1125055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153621001&doi=10.3389%2ffcomp.2023.1125055&partnerID=40&md5=55da0b1f34407e519fee0b2644290fc9","Autonomous driver assistance systems (ADAS) have been progressively pushed to extremes. Today, increasingly sophisticated algorithms, such as deep neural networks, assume responsibility for critical driving functionality, including operating the vehicle at various levels of autonomy. Elaborate obstacle detection, classification, and prediction algorithms, mostly vision-based, trajectory planning, and smooth control algorithms, take over what humans learn until they are permitted to control vehicles and beyond. And even if humans remain in the loop (e.g., to intervene in case of error, as required by autonomy levels 3 and 4), it remains questionable whether distracted human drivers will react appropriately, given the high speed at which vehicles drive and the complex traffic situations they have to cope with. A further pitfall is trusting the whole autonomous driving stack not to fail due to accidental causes and to be robust against cyberattacks of increasing sophistication. In this experience report, we share our findings in retrofitting application-agnostic resilience mechanisms into an existing hardware-/software-stack for autonomous driving—Apollo—as well as where application knowledge helps improve existing resilience algorithms. Our goal is to ultimately decrease the vulnerability of autonomously driving vehicles to accidental faults and attacks, allowing them to absorb and tolerate both, as well as to come out of them at least as secure as before the attack has happened. We demonstrate replication and rejuvenation on the driving stack's Control module and indicate how this resilience can be extended both downwards to the hardware level, as well as upwards to the prediction and planning modules. Copyright © 2023 Lucchetti, Graczyk and Völp.","Apollo; autonomous driving; fault and intrusion tolerance; resilience; SVL simulator","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85153621001"
"Chakir O.; Rehaimi A.; Sadqi Y.; Abdellaoui Alaoui E.A.; Krichen M.; Gaba G.S.; Gurtov A.","Chakir, Oumaima (57441951400); Rehaimi, Abdeslam (58124315700); Sadqi, Yassine (56321092500); Abdellaoui Alaoui, El Arbi (57204093522); Krichen, Moez (8973115500); Gaba, Gurjot Singh (55027229000); Gurtov, Andrei (6506910785)","57441951400; 58124315700; 56321092500; 57204093522; 8973115500; 55027229000; 6506910785","An empirical assessment of ensemble methods and traditional machine learning techniques for web-based attack detection in industry 5.0","2023","Journal of King Saud University - Computer and Information Sciences","35","3","","103","119","16","","10.1016/j.jksuci.2023.02.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149223904&doi=10.1016%2fj.jksuci.2023.02.009&partnerID=40&md5=418c732c99c7615f9eaf7fa0144e8a7f","Cybersecurity attacks that target software have become profitable and popular targets for cybercriminals who consciously take advantage of web-based vulnerabilities and execute attacks that might jeopardize essential industry 5.0 features. Several machine learning-based techniques have been developed in the literature to identify these types of assaults. In contrast to single classifiers, ensemble methods have not been evaluated empirically. To the best of our knowledge, this work is the first empirical evaluation of both homogeneous and heterogeneous ensemble approaches compared to single classifiers for web-based attack detection in industry 5.0, utilizing two of the most realistic public web-based attack datasets. The authors divided the experiment into three main phases: In the first phase, they evaluated the performance of five well-established supervised machine learning (ML) classifiers. In the second phase, they constructed a heterogeneous ensemble of the three best-performing ML algorithms using max voting and stacking methods. In the third phase, they used four well-known homogeneous ensembles to evaluate the performance of the bagging and boosting method. The results based on the ECML/PKDD 2007 and CSIC HTTP 2010 datasets revealed that bagging, particularly Random Forest, outperformed single classifiers in terms of accuracy, precision, F-value, FPR, and area of the ROC curve with values of 99.597%, 98.274%, 99.129%, 0.523%, 100 and 99.867%, 99.867%, 99.867%, 0.267%, 100, respectively. In contrast, single classifiers performed better than boosting and stacking. However, in terms of FPR, the boosting exceeded single classifiers. Max voting is appropriate when accuracy, precision, and FPR are the primary concerns, whereas single classifiers can be employed when recall, FNR, training, and prediction times are critical elements. In terms of training time, ensemble approaches are more likely to be affected by data volume than single classifiers. The paper's findings will help security researchers and practitioners identify the most efficient learning techniques for securing web applications. © 2023 The Author(s)","Cybersecurity; Ensemble methods; Industry 5.0; Machine learning; Web-based attack detection","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85149223904"
"Fu M.; Nguyen V.; Tantithamthavorn C.(k).; Le T.; Phung D.","Fu, Michael (57685110800); Nguyen, Van (57202983154); Tantithamthavorn, Chakkrit (kla) (55361007600); Le, Trung (57202557822); Phung, Dinh (7003397144)","57685110800; 57202983154; 55361007600; 57202557822; 7003397144","VulExplainer: A Transformer-Based Hierarchical Distillation for Explaining Vulnerability Types","2023","IEEE Transactions on Software Engineering","49","10","","4550","4565","15","","10.1109/TSE.2023.3305244","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168263380&doi=10.1109%2fTSE.2023.3305244&partnerID=40&md5=915c3a0a9c829c9a2c0c80dd0a28f186","Deep learning-based vulnerability prediction approaches are proposed to help under-resourced security practitioners to detect vulnerable functions. However, security practitioners still do not know what type of vulnerabilities correspond to a given prediction (aka CWE-ID). Thus, a novel approach to explain the type of vulnerabilities for a given prediction is imperative. In this paper, we propose VulExplainer, an approach to explain the type of vulnerabilities. We represent VulExplainer as a vulnerability classification task. However, vulnerabilities have diverse characteristics (i.e., CWE-IDs) and the number of labeled samples in each CWE-ID is highly imbalanced (known as a highly imbalanced multi-class classification problem), which often lead to inaccurate predictions. Thus, we introduce a Transformer-based hierarchical distillation for software vulnerability classification in order to address the highly imbalanced types of software vulnerabilities. Specifically, we split a complex label distribution into sub-distributions based on CWE abstract types (i.e., categorizations that group similar CWE-IDs). Thus, similar CWE-IDs can be grouped and each group will have a more balanced label distribution. We learn TextCNN teachers on each of the simplified distributions respectively, however, they only perform well in their group. Thus, we build a transformer student model to generalize the performance of TextCNN teachers through our hierarchical knowledge distillation framework. Through an extensive evaluation using the real-world 8,636 vulnerabilities, our approach outperforms all of the baselines by 5%-29%. The results also demonstrate that our approach can be applied to Transformer-based architectures such as CodeBERT, GraphCodeBERT, and CodeGPT. Moreover, our method maintains compatibility with any Transformer-based model without requiring any architectural modifications but only adds a special distillation token to the input. These results highlight our significant contributions towards the fundamental and practical problem of explaining software vulnerability.  © 1976-2012 IEEE.","software security; Software vulnerability","Abstracting; Deep learning; Distillation; Job analysis; Network security; Value engineering; Code; Security; Security practitioners; Software; Software security; Software vulnerabilities; Static VAr compensator; Task analysis; Transformer; Vulnerability classifications; Forecasting","Article","Final","","Scopus","2-s2.0-85168263380"
"Fryer A.; Dean T.; Lachine B.","Fryer, Andrew (57970070000); Dean, Thomas (7102447732); Lachine, Brian (57226400950)","57970070000; 7102447732; 57226400950","Input Output Grammar Coverage in Fuzzing","2023","MILCOM 2023 - 2023 IEEE Military Communications Conference: Communications Supporting Military Operations in a Contested Environment","","","","937","943","6","","10.1109/MILCOM58377.2023.10356308","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182405459&doi=10.1109%2fMILCOM58377.2023.10356308&partnerID=40&md5=3de2826a160a24cdf4a1874bbd7de312","Fuzz testing enables the discovery of vulnerabilities, ideally providing details to help mitigate such issues and measure the extent to which code branches were tested. The inputs used during fuzz testing can usually be specified using a grammar. The complexity of the grammar of the input can be reflected in the different code branches covered during testing. The hypothesis of this research is that coverage of the input grammar can provide some prediction of the code coverage achieved during fuzz testing. In this work, grammar coverage is compared to code coverage using the LibAFL framework and the Knot DNS server as the target, comparing the well known AFL feedback against nine other feedback algorithms.A key to this approach is using the input language to build feature vectors which represent the structure and abstract the values of the input. This research demonstrated that grammar-based coverage is useful in the absence of execution data, both for fuzzing feedback and for identifying potentially erroneous output data.  © 2023 IEEE.","Fuzzing; Grammar; Software Testing","Code coverage; DNS server; Features vector; Feedback algorithms; Fuzz Testing; Fuzzing; Grammar; Grammar coverage; Input-output; Software testings; Software testing","Conference paper","Final","","Scopus","2-s2.0-85182405459"
"Croft R.; Xie Y.; Babar M.A.","Croft, Roland (57219534056); Xie, Yongzheng (57226598098); Babar, Muhammad Ali (6602842620)","57219534056; 57226598098; 6602842620","Data Preparation for Software Vulnerability Prediction: A Systematic Literature Review","2023","IEEE Transactions on Software Engineering","49","3","","1044","1063","19","","10.1109/TSE.2022.3171202","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129673347&doi=10.1109%2fTSE.2022.3171202&partnerID=40&md5=a056903028bcb120050cd1f37b644494","Software Vulnerability Prediction (SVP) is a data-driven technique for software quality assurance that has recently gained considerable attention in the Software Engineering research community. However, the difficulties of preparing Software Vulnerability (SV) related data is considered as the main barrier to industrial adoption of SVP approaches. Given the increasing, but dispersed, literature on this topic, it is needed and timely to systematically select, review, and synthesize the relevant peer-reviewed papers reporting the existing SV data preparation techniques and challenges. We have carried out a Systematic Literature Review (SLR) of SVP research in order to develop a systematized body of knowledge of the data preparation challenges, solutions, and the needed research. Our review of the 61 relevant papers has enabled us to develop a taxonomy of data preparation for SVP related challenges. We have analyzed the identified challenges and available solutions using the proposed taxonomy. Our analysis of the state of the art has enabled us identify the opportunities for future research. This review also provides a set of recommendations for researchers and practitioners of SVP approaches.  © 2022 IEEE.","Data preparation; data quality; software vulnerability prediction; systematic literature review","Computer software selection and evaluation; Data structures; Quality assurance; Taxonomies; Code; Data driven technique; Data integrity; Data preparation; Data quality; Software; Software vulnerabilities; Software vulnerability prediction; Systematic; Systematic literature review; Forecasting","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85129673347"
"Yang X.; Wang S.; Li Y.; Wang S.","Yang, Xu (57564451200); Wang, Shaowei (54685504200); Li, Yi (57214684298); Wang, Shaohua (55935247000)","57564451200; 54685504200; 57214684298; 55935247000","Does data sampling improve deep learning-based vulnerability detection? Yeas! and Nays!","2023","Proceedings - International Conference on Software Engineering","","","","2287","2298","11","","10.1109/ICSE48619.2023.00192","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171735470&doi=10.1109%2fICSE48619.2023.00192&partnerID=40&md5=5cb7c6114b2ccce9a31d7962fbe7ab8d","Recent progress in Deep Learning (DL) has sparked interest in using DL to detect software vulnerabilities automatically and it has been demonstrated promising results at detecting vulnerabilities. However, one prominent and practical issue for vulnerability detection is data imbalance. Prior study observed that the performance of state-of-the-art (SOTA) DL-based vulnerability detection (DLVD) approaches drops precipitously in real world imbalanced data and a 73% drop of F1-score on average across studied approaches. Such a significant performance drop can disable the practical usage of any DLVD approaches. Data sampling is effective in alleviating data imbalance for machine learning models and has been demonstrated in various software engineering tasks. Therefore, in this study, we conducted a systematical and extensive study to assess the impact of data sampling for data imbalance problem in DLVD from two aspects: i) the effectiveness of DLVD, and ii) the ability of DLVD to reason correctly (making a decision based on real vulnerable statements). We found that in general, oversampling outperforms undersampling, and sampling on raw data outperforms sampling on latent space, typically random oversampling on raw data performs the best among all studied ones (including advanced one SMOTE and OSS). Surprisingly, OSS does not help alleviate the data imbalance issue in DLVD. If the recall is pursued, random undersampling is the best choice. Random oversampling on raw data also improves the ability of DLVD approaches for learning real vulnerable patterns. However, for a significant portion of cases (at least 33% in our datasets), DVLD approach cannot reason their prediction based on real vulnerable statements. We provide actionable suggestions and a roadmap to practitioners and researchers. © 2023 IEEE.","data sampling; deep learning; interpretable AI; Vulnerability detection","Deep learning; Software engineering; Data imbalance; Data sampling; Deep learning; Detection approach; Interpretable AI; Over sampling; Performance; Recent progress; Software vulnerabilities; Vulnerability detection; Drops","Conference paper","Final","","Scopus","2-s2.0-85171735470"
"Vasireddy D.T.; Dale D.S.; Li Q.","Vasireddy, Dinesh T. (58686301300); Dale, Dakota S. (57982122800); Li, Qinghua (56275683000)","58686301300; 57982122800; 56275683000","CVSS Base Score Prediction Using an Optimized Machine Learning Scheme","2023","2023 Resilience Week, RWS 2023","","","","","","","","10.1109/RWS58133.2023.10284627","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176130718&doi=10.1109%2fRWS58133.2023.10284627&partnerID=40&md5=55d7ee3ea5838da99207e36ac6b3de62","The Common Vulnerability Scoring System (CVSS) is commonly used to measure the severity of software vulner-abilities. It consists of a CVSS Score Vector (i.e., a vector of metrics) and a CVSS Base Score calculated based on the vector. The Base Score is widely used by electric utilities to measure the risk levels of vulnerabilities and prioritize remediation actions. However, the process of determining the CVSS metric values is currently very time-consuming since it is manually done by human experts based on text descriptions of vulnerabilities, which increases the delays of remediating vulnerabilities and hence increases security risks at electric utilities. In this paper, we develop an efficient and effective solution to automatically predict the CVSS Base Score of vulnerabilities primarily based on their text descriptions, leveraging Natural Language Processing and machine learning techniques. Text descriptions for tens of thousands of vulnerabilities are comprehensively interpreted and vectorized using Doc2Vec, fed to a neural network with a condensed regression structure, which is then fine-tuned using Bayesian Optimization. By exploring and selecting the most efficient option at each stage of development, we create an optimized scheme that predicts CVSS Base Scores with very low error. Our work shows that it is possible to effectively predict CVSS Base Scores using simple but optimized neural networks. It makes crucial progress toward addressing the inefficiencies of the current CVSS severity assessment process through automation. © 2023 IEEE.","Automation; Cybersecurity; Machine Learning; Neural Networks; Vulnerability","Cybersecurity; Electric utilities; Forecasting; Learning algorithms; Natural language processing systems; Neural networks; Risk assessment; Common vulnerability scoring systems; Cyber security; Human expert; Learning schemes; Machine-learning; Metric values; Neural-networks; Risk levels; System metrics; Vulnerability; Machine learning","Conference paper","Final","","Scopus","2-s2.0-85176130718"
"Napier K.; Bhowmik T.; Wang S.","Napier, Kollin (57911144400); Bhowmik, Tanmay (57679627000); Wang, Shaowei (54685504200)","57911144400; 57679627000; 54685504200","An empirical study of text-based machine learning models for vulnerability detection","2023","Empirical Software Engineering","28","2","38","","","","","10.1007/s10664-022-10276-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147533050&doi=10.1007%2fs10664-022-10276-6&partnerID=40&md5=f04de5ba9d5776779f7ec25d24393a23","With an increase in complexity and severity, it is becoming harder to identify and mitigate vulnerabilities. Although traditional tools remain useful, machine learning models are being adopted to expand efforts. To help explore methods of vulnerability detection, we present an empirical study on the effectiveness of text-based machine learning models by utilizing 344 open-source projects, 2,182 vulnerabilities and 38 vulnerability types. With the availability of vulnerabilities being presented in forms such as code snippets, we construct a methodology based on extracted source code functions and create equal pairings. We conduct experiments using seven machine learning models, five natural language processing techniques and three data processing methods. First, we present results based on full context function pairings. Next, we introduce condensed functions and conduct a statistical analysis to determine if there is a significant difference between the models, techniques, or methods. Based on these results, we answer research questions regarding model prediction for testing within and across projects and vulnerability types. Our results show that condensed functions with fewer features may achieve greater prediction results when testing within rather than across. Overall, we conclude that text-based machine learning models are not effective in detecting vulnerabilities within or across projects and vulnerability types. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Machine learning; Text-based analysis; Vulnerability detection","Codes (symbols); Data handling; Learning algorithms; Natural language processing systems; Open source software; Data processing methods; Empirical studies; Language processing techniques; Machine learning models; Machine-learning; Natural languages; Open source projects; Source codes; Text-based analyse; Vulnerability detection; Machine learning","Article","Final","","Scopus","2-s2.0-85147533050"
"Qian J.; Li A.; Chen Y.; Zhou X.","Qian, Jin (58042776600); Li, Ang (58347028600); Chen, Yuanzhong (58350194300); Zhou, Xinyue (58350194400)","58042776600; 58347028600; 58350194300; 58350194400","Prediction search on the threat of darknet based on machine learning","2023","2023 IEEE International Conference on Control, Electronics and Computer Technology, ICCECT 2023","","","","876","880","4","","10.1109/ICCECT57938.2023.10140830","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162890734&doi=10.1109%2fICCECT57938.2023.10140830&partnerID=40&md5=039fab2de5707c5982d1a4d1f5ee137e","The anonymity and untraceability of the darknet provide fertile ground for the development of illegal trade and terrorism. In recent years, the number of darknet websites dominated by Tor and I2P has been increasing sharply. There have been many vulnerabilities that have happened on the darknet causing a generalized impact on the real world, which is attracting increasing concern from all sides. In order to monitor the traffic of darknet and find out whether the vulnerabilities discussed on the darknet may lead to serious consequences in the real world, we propose a method to predict the threat of the darknet based on machine learning. Firstly, the project builds a crawler system, using the privoxy service to set up agents and using Tor service to connect to the darknet. Then, with firefox's headless browser, we recursively access the contents of the darknet webpage. The project crawls the HTML of webpage and searches for important contents in HTML through python's selenium library. The crawled data and returned header information are stored in the database. As a pack of software, kibana can perform full-text searches of data stored in the elasticsearch database. In addition, based on react framework, web visualization of database data is realized by using echarts, grommet, grid-layout and other control groups. Line charts, pie charts and lists are used to display the total amount of data, extra daily data and other information. Finally, the project washes HTML data to obtain corpus. The word vector and Lda model are combined to realize web page classification. The labeled data set, containing vulnerability exploitation, is modeled by various methods, including Bayesian, random forest, SVM and other common methods. With the performance of the model evaluated, SVM can achieve 93% accuracy rate on 80% of the test sets. On this basis, the time node when the vulnerability may be used in the open network is predicted.  © 2023 IEEE.","darknet crawler; forecasting bug use; full-text search; web visualization","Data visualization; Database systems; HTML; Learning systems; Network security; Support vector machines; Visualization; Websites; Darknet crawler; Darknets; Forecasting bug use; Full-text search; Machine-learning; On-machines; Real-world; Untraceability; Web visualization; Web-page; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85162890734"
"Li X.; Zhu Z.","Li, Xuejian (55870800200); Zhu, Zhengguang (58744594400)","55870800200; 58744594400","Software Defect Detection Based on Feature Fusion and Alias Analysis","2023","Proceedings - 7th IEEE International Test Conference in Asia, ITC-Asia 2023","","","","","","","","10.1109/ITC-Asia58802.2023.10301168","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178632383&doi=10.1109%2fITC-Asia58802.2023.10301168&partnerID=40&md5=95e5ec3bfde61988224aa8f5dcf26e2e","As the scale of software systems continues to increase, predicting program defects in a quick and efficient manner has become a significant research area. Recent studies have introduced deep learning models that use neural networks to extract code features and build classifiers for defect prediction. However, most existing research focuses on extracting code features at a single granularity and single level, resulting in a lack of rich code features and low prediction accuracy. To address this issue, this paper proposes a defect prediction framework based on feature fusion and alias analysis for predicting the presence of non-inferable aliases and vulnerabilities in programs. The proposed approach parses the program into two different program representations, namely Abstract Syntax Tree (AST) and Program dependency Graph (PDG), and extracts code features for feature fusion using Long Short-Term Memory (LSTM) networks and Graph Convolutional Networks (GCN), respectively. To evaluate the effectiveness of the proposed approach, the Software Assurance Reference Dataset (SARD) from the National Institute of Standards and Technology (NIST) is chosen as the experimental dataset. The classifiers constructed using the proposed approach are used to predict the presence of non-inferable aliases and vulnerabilities in programs. The experimental results demonstrate the effectiveness of the proposed approach in predicting program defects.  © 2023 IEEE.","alias analysis; defect prediction; feature fusion; neural networks","Classification (of information); Codes (symbols); Feature extraction; Forecasting; Long short-term memory; Trees (mathematics); Alias analysis; Defect detection; Defect prediction; Features fusions; Learning models; Neural-networks; Research areas; Research focus; Software defects; Software-systems; Defects","Conference paper","Final","","Scopus","2-s2.0-85178632383"
"Kazemi F.; Asgarkhani N.; Jankowski R.","Kazemi, F. (57217047347); Asgarkhani, N. (57215197930); Jankowski, R. (7102473802)","57217047347; 57215197930; 7102473802","Predicting seismic response of SMRFs founded on different soil types using machine learning techniques","2023","Engineering Structures","274","","114953","","","","","10.1016/j.engstruct.2022.114953","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140488917&doi=10.1016%2fj.engstruct.2022.114953&partnerID=40&md5=97dea2fddb40caa02f7109772e3942f8","Predicting the Maximum Interstory Drift Ratio (M-IDR) of Steel Moment-Resisting Frames (SMRFs) is a useful tool for designers to approximately evaluate the vulnerability of SMRFs. This study aims to explore supervised Machine Learning (ML) algorithms to build a surrogate prediction model for SMRFs to reduce the need for complex modeling. For this purpose, twenty well-known ML algorithms implemented in Python software are trained based on the dataset achieved from Incremental Dynamic Analyses (IDAs) performed on the 2-Story, 3-Story, 4-Story, 5-Story, 6-Story, 7-Story, 8-Story, and 9-Story SMRFs modeled in Opensees. Then, important features of weight, fundamental period of structure (T1), the RSN number of record subsets, the direction of the record subsets, soil types, and Sa(T1) of analysis, which affect the results of predictions, were selected by trial and error. Having these important features, data-driven techniques developed in Python software were compared for predicting the M-IDR of SMRFs as target. Results showed that ML algorithms of GPReg, PLSReg, SReg, LReg, GReg, MLPReg, SVM, and LSVR had lower values of coefficient of determination (R2 lower than 0.5) in both train and test datasets. In addition, XGBoost, BReg, HistGBR, and ERTReg algorithms achieved higher values of R2 (i.e. upper than 0.95 in the 5-Story SMRF) with low Mean Squared Error (MSE) for prediction of M-IDR. Therefore, using these algorithms mitigates the need for computationally expensive, time-consuming, and complex analysis, while preliminary prediction of M-IDR can be considered a low computational and efficient tool for seismic vulnerability assessment. © 2022 Elsevier Ltd","Data-driven techniques; Machine learning algorithm; Maximum interstory drift ratio; Seismic vulnerability assessment; Supervised learning","Computational efficiency; Computer software; Forecasting; High level languages; Learning algorithms; Mean square error; Support vector machines; Data driven technique; Inter-story drift ratios; Machine learning algorithms; Maximum interstory drift ratio; Ratio of steel; Seismic vulnerability; Seismic vulnerability assessment; Soil types; Steel moment resisting frame; Vulnerability assessments; machine learning; prediction; seismic response; structural response; supervised learning; vulnerability; Python","Article","Final","","Scopus","2-s2.0-85140488917"
"Fu M.; Tantithamthavorn C.; Le T.; Kume Y.; Nguyen V.; Phung D.; Grundy J.","Fu, Michael (57685110800); Tantithamthavorn, Chakkrit (55361007600); Le, Trung (57202557822); Kume, Yuki (58335922900); Nguyen, Van (57202983154); Phung, Dinh (7003397144); Grundy, John (7102156137)","57685110800; 55361007600; 57202557822; 58335922900; 57202983154; 7003397144; 7102156137","AIBugHunter: A Practical tool for predicting, classifying and repairing software vulnerabilities","2024","Empirical Software Engineering","29","1","4","","","","","10.1007/s10664-023-10346-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177178535&doi=10.1007%2fs10664-023-10346-3&partnerID=40&md5=a2dc8557c9f85d763a69284ed60d482a","Many Machine Learning(ML)-based approaches have been proposed to automatically detect, localize, and repair software vulnerabilities. While ML-based methods are more effective than program analysis-based vulnerability analysis tools, few have been integrated into modern Integrated Development Environments (IDEs), hindering practical adoption. To bridge this critical gap, we propose in this article AIBugHunter, a novel Machine Learning-based software vulnerability analysis tool for C/C++ languages that is integrated into the Visual Studio Code (VS Code) IDE. AIBugHunter helps software developers to achieve real-time vulnerability detection, explanation, and repairs during programming. In particular, AIBugHunter scans through developers’ source code to (1) locate vulnerabilities, (2) identify vulnerability types, (3) estimate vulnerability severity, and (4) suggest vulnerability repairs. We integrate our previous works (i.e., LineVul and VulRepair) to achieve vulnerability localization and repairs. In this article, we propose a novel multi-objective optimization (MOO)-based vulnerability classification approach and a transformer-based estimation approach to help AIBugHunter accurately identify vulnerability types and estimate severity. Our empirical experiments on a large dataset consisting of 188K+ C/C++ functions confirm that our proposed approaches are more accurate than other state-of-the-art baseline methods for vulnerability classification and estimation. Furthermore, we conduct qualitative evaluations including a survey study and a user study to obtain software practitioners’ perceptions of our AIBugHunter tool and assess the impact that AIBugHunter may have on developers’ productivity in security aspects. Our survey study shows that our AIBugHunter is perceived as useful where 90% of the participants consider adopting our AIBugHunter during their software development. Last but not least, our user study shows that our AIBugHunter can enhance developers’ productivity in combating cybersecurity issues during software development. AIBugHunter is now publicly available in the Visual Studio Code marketplace. © 2023, The Author(s).","Vulnerability classification; Vulnerability localization; Vulnerability prediction; Vulnerability repair","C++ (programming language); Computer software; Large dataset; Machine learning; Multiobjective optimization; Object oriented programming; Repair; Software design; Studios; Visual languages; Analysis tools; Integrated development environment; Localisation; Machine-learning; Software vulnerabilities; Vulnerability analysis; Vulnerability classifications; Vulnerability localization; Vulnerability prediction; Vulnerability repair; Classification (of information)","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85177178535"
"Khan T.; Faisal M.","Khan, Tabrez (58191990700); Faisal, Mohd. (57210120371)","58191990700; 57210120371","An efficient Bayesian network model (BNM) for software risk prediction in design phase development","2023","International Journal of Information Technology (Singapore)","15","4","","2147","2160","13","","10.1007/s41870-023-01244-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153193087&doi=10.1007%2fs41870-023-01244-4&partnerID=40&md5=0cc58be73a1fd726e8a504036c386ed4","The primary purpose of a software risk assessment is to predict risks and vulnerabilities that may exist in each phase of the software development life cycle (SDLC). Risk factors have a significant impact on the timeline, budget, and quality of software development. It's very important to know and understand the risks before they can be effectively managed. Researchers have developed several tools to manage the risk that help reduce the number of failed software projects and increase the number of successful software projects. This study aims to ascertain which risks are important and how often they happen, and to explore and reveal the situations where the risks could lead to software failure in the design phase. We are developing a model that can predict risks during the design process so that we can find the risk factors that lead to risks in software development. These risks have been analyzed, classified, and incorporated into Risk Prediction Trees (RPTs). Bayesian network (BN) techniques have been used to propose a model for estimating the probability of risk during the software design phase. The Bayesian network approach is used because the data can be obtained from software that has already been used. It has the flexibility to predict risk in real-time. And it has the best risk prediction rates when it comes to potential risk factors. The outcome of this study shows that, compared to other standard machine-learning approaches, BN can be used to predict possible risks in the early software design phase. © 2023, The Author(s), under exclusive licence to Bharati Vidyapeeth's Institute of Computer Applications and Management.","Bayesian network; Design phase; Machine-learning; Risk assessment; Risk prediction tree; SDLC","","Article","Final","","Scopus","2-s2.0-85153193087"
"Nguyen T.G.; Le-Cong T.; Kang H.J.; Widyasari R.; Yang C.; Zhao Z.; Xu B.; Zhou J.; Xia X.; Hassan A.E.; Le X.-B.D.; Lo D.","Nguyen, Truong Giang (57863721400); Le-Cong, Thanh (57219762867); Kang, Hong Jin (57215115758); Widyasari, Ratnadira (57216963049); Yang, Chengran (57226270615); Zhao, Zhipeng (58072710200); Xu, Bowen (57189036787); Zhou, Jiayuan (57216100717); Xia, Xin (58275220100); Hassan, Ahmed E. (7402686972); Le, Xuan-Bach D. (57187852200); Lo, David (35269388000)","57863721400; 57219762867; 57215115758; 57216963049; 57226270615; 58072710200; 57189036787; 57216100717; 58275220100; 7402686972; 57187852200; 35269388000","Multi-Granularity Detector for Vulnerability Fixes","2023","IEEE Transactions on Software Engineering","49","8","","4035","4057","22","","10.1109/TSE.2023.3281275","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161062826&doi=10.1109%2fTSE.2023.3281275&partnerID=40&md5=a46ac465a45a0289e0090356b5690497","With the increasing reliance on Open Source Software, users are exposed to third-party library vulnerabilities. Software Composition Analysis (SCA) tools have been created to alert users of such vulnerabilities. SCA requires the identification of vulnerability-fixing commits. Prior works have proposed methods that can automatically identify such vulnerability-fixing commits. However, identifying such commits is highly challenging, as only a very small minority of commits are vulnerability fixing. Moreover, code changes can be noisy and difficult to analyze. We observe that noise can occur at different levels of detail, making it challenging to detect vulnerability fixes accurately. To address these challenges and boost the effectiveness of prior works, we propose MiDas (Multi-Granularity Detector for Vulnerability Fixes). Unique from prior works, MiDas constructs different neural networks for each level of code change granularity, corresponding to commit-level, file-level, hunk-level, and line-level, following their natural organization and then use an ensemble model combining all base models to output the final prediction. This design allows MiDas to better cope with the noisy and highly-imbalanced nature of vulnerability-fixing commit data. In addition, to reduce the human effort required to inspect code changes, we have designed an effort-aware adjustment for MiDas's outputs based on commit length. The evaluation result demonstrates that MiDas outperforms the current state-of-the-art baseline on both Java and Python-based datasets in terms of AUC by 4.9% and 13.7%, respectively. Furthermore, in terms of two effort-aware metrics, i.e., EffortCost@L and Popt@L, MiDas also performs better than the state-of-the-art baseline up to 28.2% and 15.9% on Java, 60% and 51.4% on Python, respectively.  © 1976-2012 IEEE.","deep learning; machine learning; software security; Vulnerability-fixing commit classification","Codes (symbols); Deep learning; Java programming language; Job analysis; Open source software; Open systems; Software testing; Code; Component analysis; Deep learning; Ensemble learning; Java; Predictive models; Security; Software component analyse; Software security; Software-component; Task analysis; Vulnerability-fixing commit identification; Python","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85161062826"
"Franca H.L.; Teixeira C.; Laranjeiro N.","Franca, Horacio L. (37013121600); Teixeira, Cesar (55826531700); Laranjeiro, Nuno (22980190100)","37013121600; 55826531700; 22980190100","Automating Vulnerability Management in the Software Development Lifecycle","2023","Proceedings - 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume, DSN-S 2023","","","","188","190","2","","10.1109/DSN-S58398.2023.00051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169291228&doi=10.1109%2fDSN-S58398.2023.00051&partnerID=40&md5=105dd32bf742f253a42b7cf80031ab07","Managing the presence of vulnerabilities in software can be a time and resource consuming process. The advancements in machine learning (ML) over the past few years have allowed us to automate parts of the software development lifecycle, including the identification of vulnerabilities starting from bug reports. However, such approaches have known gaps generally related with subpar effectiveness. In this PhD, we intend to propose a vulnerability management framework aiming at four main objectives: i) highly effective vulnerability identification starting from bug reports; ii) detailed vulnerability classification; iii) prediction of main aspects related with the correction (e.g., defect triage); and iv) recommending corrections based on the detailed knowledge obtained in the previous phases. © 2023 IEEE.","issue report; machine learning; security; software development; vulnerabilities","Life cycle; Software design; Bug reports; Issue report; Machine-learning; Management frameworks; Security; Software development life-cycle; Vulnerability; Vulnerability classifications; Vulnerability management; Machine learning","Conference paper","Final","","Scopus","2-s2.0-85169291228"
"Zhu X.; Liu S.; Jolfaei A.","Zhu, Xiaogang (57215327205); Liu, Shigang (56195519800); Jolfaei, Alireza (36680369700)","57215327205; 56195519800; 36680369700","A Fuzzing Method for Security Testing of Sensors","2023","IEEE Sensors Journal","","","","1","1","0","","10.1109/JSEN.2023.3301517","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167785225&doi=10.1109%2fJSEN.2023.3301517&partnerID=40&md5=2a071b517c72fa2aa3b9d864be07a255","The security issues in sensors impede the experience of using sensors to improve the quality of our life. By analysing security issues in open-source programs for sensors, we can understand the security problems in sensors. Fuzzing is one of the most effective techniques to identify potential software vulnerabilities. Most fuzzers aim to improve code coverage. However, a tester may want to focus on examining some specific code regions. In this paper, we proposed a deep learning (DL) guided fuzzing for software vulnerability detection, named DeFuzz. DeFuzz includes two main schemes: (1) We employ a pre-trained DL prediction model to identify the potentially vulnerable functions and the locations (<italic>i.e</italic>., vulnerable addresses). Precisely, we employ Bidirectional-LSTM (BiLSTM) to identify attention words, and the vulnerabilities are associated with these attention words in functions. (2) Then, we employ directed fuzzing to examine the potential vulnerabilities by generating inputs that tend to arrive at the predicted locations. To evaluate the effectiveness of the proposed DeFuzz technique, we have conducted experiments on real-world data sets. Experimental results show that our DeFuzz can discover more coverage and run faster than AFL. Moreover, DeFuzz exposes 43 more bugs than AFL on real-world applications. IEEE","Computer bugs; Deep learning; Deep Learning; Fuzz Testing; Fuzzing; Microprogramming; Security; Sensor Security; Sensors; Software","Long short-term memory; Open source software; Open systems; Program debugging; Software testing; Computer bugs; Deep learning; Fuzz Testing; Fuzzing; Security; Security issues; Sensor securities; Software; Software vulnerabilities; Microprogramming","Article","Article in press","","Scopus","2-s2.0-85167785225"
"Kirschner Y.R.; Walter M.; Bossert F.; Heinrich R.; Koziolek A.","Kirschner, Yves R. (57223124259); Walter, Maximilian (57204473756); Bossert, Florian (58246487100); Heinrich, Robert (56352523400); Koziolek, Anne (55094731500)","57223124259; 57204473756; 58246487100; 56352523400; 55094731500","Automatic Derivation of Vulnerability Models for Software Architectures","2023","Proceedings - IEEE 20th International Conference on Software Architecture Companion, ICSA-C 2023","","","","276","283","7","","10.1109/ICSA-C57050.2023.00065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159106301&doi=10.1109%2fICSA-C57050.2023.00065&partnerID=40&md5=f3b0b20ae13e30aba61e4a84f9035008","Software architectures consist of more and more connections to different components or elements. With the increased connection and exchange between different elements also the attack surface increases, since each element might contain vulnerabilities. The vulnerabilities may be harmless on their own, but attackers could develop attack paths from the combination of different vulnerabilities. For a model-based attack propagation analysis, it is useful to have an annotated components model with vulnerabilities. However, depending on the size of the system, the manual annotation of these models is very time-consuming and error-prone. In this context, we present in this paper an approach that automatically annotates vulnerability information to the components of an architectural model. The goal here is to extract security information of source artifacts and transform them into an existing architecture-based security model to enable model-based security risk assessment. We evaluate our approach using three open-source case studies to demonstrate feasibility and accuracy. The results indicate high recall reading vulnerabilities.  © 2023 IEEE.","Component-based; Context-Aware QoS Model; Modeling and prediction; Security; Software architecture","Open source software; Risk assessment; Attack path; Automatic derivation; Component based; Context-Aware; Context-aware QoS model; Model-based OPC; Modelling and predictions; QoS models; Security; Vulnerability models; Software architecture","Conference paper","Final","","Scopus","2-s2.0-85159106301"
"Du Y.; Huang C.; Liang G.; Fu Z.; Li D.; Ding Y.","Du, Yutong (57559259500); Huang, Cheng (55511401300); Liang, Genpei (57558728100); Fu, Zhihao (57985628700); Li, Dunhan (57453969500); Ding, Yong (58735208800)","57559259500; 55511401300; 57558728100; 57985628700; 57453969500; 58735208800","ExpSeeker: extract public exploit code information from social media","2023","Applied Intelligence","53","12","","15772","15786","14","","10.1007/s10489-022-04178-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142904388&doi=10.1007%2fs10489-022-04178-9&partnerID=40&md5=ed2639d04227ad43bbd4d34f9c0f97fd","Malicious actors often utilize publicly available software vulnerabilities and exploit codes to attack vulnerable targets. Exploit codes are shared across several platforms, including exploit databases, hacker communities, and social media platforms. Public exploit code information is a type of cyber threat intelligence. It can help security experts to analyze which vulnerabilities are available for malicious actors and need to be prioritized for patching. In this paper, We propose a intelligent framework to automatically extract public exploit code information from social media. Social media sites are capable of aggregating numerous cybersecurity-related information due to their timeliness and volume. Firstly, we present a convolutional neural network classifier to identity disclose exploit codes in their content or corresponding web pages linked in tweets, which achieved 0.989 AUC and 0.939 F1-score. The model shows better prediction accuracy than the baseline approaches. Secondly, we present a Bert-BiLSTM-CRF entity recognition method to figure out the target entity which may be influenced by the exploit code. As a result, the Bert-BiLSTM-CRF model reached an F1-score of 0.959, which performed better than the 0.927 and 0.922 obtained by the same neural network using Word2vec and GloVe word embeddings respectively. Finally, the experiment results show the proposed method provide enriched supplementary information and earlier intelligence for the appearances of open exploit codes on the Internet by contrasting to the exploit database. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Exploit; Named entity recognition; Social media vulnerability mentions; Text classification","Convolutional neural networks; Cybersecurity; Personal computing; Social networking (online); Text processing; Community medias; Exploit; F1 scores; Hacker communities; Named entity recognition; Social media; Social media platforms; Social medium vulnerability mention; Software vulnerabilities; Text classification; Classification (of information)","Article","Final","","Scopus","2-s2.0-85142904388"
"Zeng T.; Yin K.; Gui L.; Jin B.; Liu X.; Liu Z.; Guo Z.; Jiang H.; Wu L.","Zeng, Taorui (57776381300); Yin, Kunlong (8905848100); Gui, Lei (55343785100); Jin, Bijing (57794321600); Liu, Xiepan (57777056200); Liu, Zhenyi (58241964900); Guo, Zizheng (57207260674); Jiang, Hongwei (57219836146); Wu, Liyang (57880793400)","57776381300; 8905848100; 55343785100; 57794321600; 57777056200; 58241964900; 57207260674; 57219836146; 57880793400","Quantitative Vulnerability Analysis of Buildings Based on Landslide Intensity Prediction; [基 于 滑 坡 致 灾 强 度 预 测 的 建 筑 物 易 损 性 定 量 评 价]","2023","Diqiu Kexue - Zhongguo Dizhi Daxue Xuebao/Earth Science - Journal of China University of Geosciences","48","5","","1807","1824","17","","10.3799/dqkx.2022.429","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164292260&doi=10.3799%2fdqkx.2022.429&partnerID=40&md5=0c4c7cb98079bdd156c85ace935a780a","In view of the lack of research about landslide intensity prediction in the current quantitative vulnerability analysis of buildings, in this paper it innovatively proposes a quantitative analysis method of the combination of intensity empirical curve based on InSAR technology and spatial displacement prediction of secondary development of ABAQUS. Taking the Shilongmen landslide in the Three Gorges Reservoir area as an example, the PS-InSAR method was adopted to calculate the cumulative displacement of the landslide in 2017‒2020 and obtained the empirical curve of landslide intensity. The ABAQUS software was used to compile the subroutine of load and pore water pressure to calculate the cumulative displacement under extreme scenarios (reservoir water level drop with heavy rainfall) and predicted the vulnerability of buildings. The evaluation system of resistance was constructed by weighting eight indicators of PSO-Fuzzy AHP model, which can be combined with the landslide intensity to quantitatively evaluate the vulnerability of buildings. The results indicate follows: (1) The evaluation system of resistance proposed in this paper can well present the structural characteristics of rural buildings in the Three Gorges Reservoir area, and has high evaluation accuracy. (2) The retrieved upper-intensity curve obtained by PS-InSAR is Ipu=0.065×Dtot0.236 which has higher prediction accuracy and effectively reduces false-negative errors. (3) The landslide intensity of extreme conditions simulated by ABAQUS increases with the increase of rainfall, the predicted vulnerability level of buildings increases, and the buildings with obvious deformation in the previous investigation are successfully warned. It is concluded that the landslide intensity prediction method and vulnerability analysis method proposed in this paper has high spatial identification and early warning accuracy, and real-time vulnerability mapping of buildings can be obtained through landslide intensity information. © 2023 China University of Geosciences. All rights reserved.","ABAQUS; building; hazard geology; landslide intensify; PS-InSAR; vulnerability","ABAQUS; Buildings; Forecasting; Geology; Petroleum reservoir evaluation; Reservoirs (water); Subroutines; Water levels; 'current; Analysis method; Hazard geology; Intensity prediction; Landslide intensify; PS-InSAR; Reservoir area; Three Gorge reservoir; Vulnerability; Vulnerability analysis; Landslides","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85164292260"
"Roy D.K.; Patgiri R.","Roy, Dharani Kanta (58204708600); Patgiri, Ripon (56568309900)","58204708600; 56568309900","CESSO-HCRNN: A Hybrid CRNN With Chaotic Enriched SSO-based Improved Information Gain to Detect Zero-Day Attacks","2023","International Journal of Advanced Computer Science and Applications","14","11","","1271","1282","11","","10.14569/IJACSA.2023.01411129","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181986095&doi=10.14569%2fIJACSA.2023.01411129&partnerID=40&md5=b65981d5f7869efe12d4df0c674d67d4","Hackers use the vulnerability before programmers have a chance to fix it, which is known as a zero-day attack. Zero-day attackers have a variety of abilities, including the ability to alter files, control machines, steal data, and install malware or adware. When a series of complex assaults uses one or more zero-day exploits, the result is a zero-day attack path. Timely assessment of zero-day threats might be enabled by early detection of zero-day attack pathways. To detect this zero-day attack, this paper introduced a Chaotic Enriched Salp Swarm Optimization (CESSO) with the help of a hybrid Convolutional Recursive Neural Network (HCRNN) is implemented. The input data is retrieved from two datasets called IDS 2018 Intrusion CSVs (CSE-CIC-IDS2018) and NSL-KDD. The data is pre-processed with the help of data cleaning and normalization. A unique hybrid feature selection method that is based on the CESSO and Information Gain(IG) is introduced. The CESSO is also used to improve the Recursive Neural Network (RNN) performance to produce an optimized RNN. The selected features are classified, and prediction is performed using the hybrid Convolutional Neural Network (CNN) with RNN called HCRNN. The implementation of the zero-day attack is performed using MATLAB software. The accuracy achieved for dataset 1 is 98.36%, and for dataset 2 is 97.14%. © (2023), (Science and Information Organization). All Rights Reserved.","and MATLAB software; chaotic enriched salp swarm optimization; data cleaning; Hackers; normalization; vulnerability; zero-day attack","Convolution; Malware; Neural networks; Personal computing; Zero-day attack; And MATLAB software; Chaotic enriched salp swarm optimization; Chaotics; Data cleaning; Hacker; Normalisation; Salp swarms; Swarm optimization; Vulnerability; Zero day attack; MATLAB","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85181986095"
"Ni C.; Tian C.; Yang K.; Lo D.; Chen J.; Yang X.","Ni, Chao (57189892547); Tian, Cong (58294490200); Yang, Kaiwen (57658615700); Lo, David (35269388000); Chen, Jiachi (57184505400); Yang, Xiaohu (8258116000)","57189892547; 58294490200; 57658615700; 35269388000; 57184505400; 8258116000","Automatic Identification of Crash-inducing Smart Contracts","2023","Proceedings - 2023 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2023","","","","108","119","11","","10.1109/SANER56733.2023.00020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160559216&doi=10.1109%2fSANER56733.2023.00020&partnerID=40&md5=b58b073282bd05726c5b5c1b6d24901c","Smart contract, a special software code running on and resided in the blockchain, enlarges the general application of blockchain and exchanges assets without dependence of external parties. With blockchain's characteristic of immutability, they cannot be modified once deployed. Thus, the contract and the records are persisted on the blockchain forever, including failed transactions that are caused by runtime errors and result in the waste of computation, storage, and fees. In this paper, we refer to smart contracts which will cause runtime errors as crash-inducing smart contracts. However, automatic identification of crash-inducing smart contracts is limited investigated in the literature. The existing approaches to identify crash-inducing smart contracts are either limited in finding vulnerability (e.g., pattern-based static analysis) or very expensive (e.g., program analysis), which is insufficient for Ethereum.To reduce runtime errors on Ethereum, we propose an efficient, generalizable, and machine learning-based crash-inducing smart contract detector, CRASHSCDET, to automatically identify crash-inducing smart contracts. To investigate the effectiveness of CRASHSCDET, we firstly propose 34 static source code metrics from four dimensions (i.e., complexity metrics, count metrics, object-oriented metrics, and Solidity-specific metrics) to characterize smart contracts. Then, we collect a large-scale dataset of verified smart contracts (i.e., 54,739) and label these smart contracts based on their execution traces on Etherscan. We make a comprehensive comparison with three state-of-the-art approaches and the results show that CRASHSCDET can achieve good performance (i.e., 0.937 of F1-measure and 0.980 of AUC on average) and statistically significantly improve the baselines by 0.5%-60.4% in terms of F1-measure and by 41.2%-44.3% in terms of AUC, which indicates the effectiveness of static source code metrics in identifying crash-inducing smart contracts. We further investigate the importance of different types of metrics and find that metrics in different dimensions have varying abilities to depict the characteristic of smart contracts. Especially, metrics belonging to the ""Count""dimension are the most discriminative ones but combining all metrics can achieve better prediction performance. © 2023 IEEE.","Crash-inducing Smart Contract; Ethereum; Machine Learning; Quality Assurance; Static Source Code Metric","Application programs; Blockchain; Computer software selection and evaluation; Computer systems programming; Errors; Ethereum; Large dataset; Machine learning; Object oriented programming; Quality assurance; Static analysis; Automatic identification; Block-chain; Code running; Crash-inducing smart contract; Machine-learning; Run-time errors; Software codes; Source code metrics; Static source code metric; Static sources; Smart contract","Conference paper","Final","","Scopus","2-s2.0-85160559216"
"Halepmollası R.; Hanifi K.; Fouladi R.F.; Tosun A.","Halepmollası, Rusen (56246366600); Hanifi, Khadija (57195217883); Fouladi, Ramin F. (55807299700); Tosun, Ayse (26649652100)","56246366600; 57195217883; 55807299700; 26649652100","A Comparison of Source Code Representation Methods to Predict Vulnerability Inducing Code Changes","2023","International Conference on Evaluation of Novel Approaches to Software Engineering, ENASE - Proceedings","2023-April","","","469","478","9","","10.5220/0011859300003464","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150987535&doi=10.5220%2f0011859300003464&partnerID=40&md5=b965b88102e7aedf09ae3f4c5c41b955","Vulnerability prediction is a data-driven process that utilizes previous vulnerability records and their associated fixes in software development projects. Vulnerability records are rarely observed compared to other defects, even in large projects, and are usually not directly linked to the related code changes in the bug tracking system. Thus, preparing a vulnerability dataset and building a predicting model is quite challenging. There exist many studies proposing software metrics-based or embedding/token-based approaches to predict software vulnerabilities over code changes. In this study, we aim to compare the performance of two different approaches in predicting code changes that induce vulnerabilities. While the first approach is based on an aggregation of software metrics, the second approach is based on embedding representation of the source code using an Abstract Syntax Tree and skip-gram techniques. We employed Deep Learning and popular Machine Learning algorithms to predict vulnerability-inducing code changes. We report our empirical analysis over code changes on the publicly available SmartSHARK dataset that we extended by adding real vulnerability data. Software metrics-based code representation method shows a better classification performance than embedding-based code representation method in terms of recall, precision and F1-Score. Copyright © 2023 by SCITEPRESS - Science and Technology Publications, Lda. Under CC license (CC BY-NC-ND 4.0)","Abstract Syntax Tree; Embeddings; Software Metrics; Software Vulnerabilities","Codes (symbols); Deep learning; Forecasting; Software design; Syntactics; Trees (mathematics); Abstract Syntax Trees; Code changes; Code representation; Data driven; Embeddings; Representation method; Software development projects; Software metrics; Software vulnerabilities; Source code representations; Embeddings","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85150987535"
"Zheng Z.; Liu Y.; Zhang B.; Liu X.; He H.; Gong X.","Zheng, Zhangqi (57200407791); Liu, Yongshan (56194246500); Zhang, Bing (56587016400); Liu, Xinqian (57208900397); He, Hongyan (57682798300); Gong, Xiang (58331991300)","57200407791; 56194246500; 56587016400; 57208900397; 57682798300; 58331991300","A multitype software buffer overflow vulnerability prediction method based on a software graph structure and a self-attentive graph neural network","2023","Information and Software Technology","160","","107246","","","","","10.1016/j.infsof.2023.107246","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158061817&doi=10.1016%2fj.infsof.2023.107246&partnerID=40&md5=d8c400769ec5c094d46a25ebb18c87f5","Context: Buffer overflow vulnerabilities are one of the most common and dangerous software vulnerabilities; however, the complexity of software code makes predicting buffer overflow vulnerabilities in software challenging. Objective: To accurately predict multiple types of software buffer overflow vulnerabilities, this paper proposes a multitype software buffer overflow vulnerability prediction method called MSVAGraph that is based on the graph structure of software and a self-attentive graph neural network. Method: First, by analyzing software buffer overflow type vulnerabilities, a vulnerability feature set GSVFset extraction method based on graph structure is proposed to act as the software's basic unit. Second, a self-attentive pooling mechanism is used to design a vulnerability feature update mechanism based on a self-attentive graph neural network to transform the graph structure of the vulnerability feature set GSVFset into a feature vector representation. Finally, based on the updated GSVFset feature vector, a time-recursive-based neural network is designed to construct a prediction method for multitype software buffer overflow vulnerabilities. Results: The method proposed in this paper validates executable programs of four types of buffer overflow vulnerabilities in the Juliet dataset using precision, accuracy, recall and F1 value as evaluation metrics. The prediction results have higher values after introducing the self-attentive pooling mechanism. Conclusion: The proposed MSVAGraph achieves high precision, accuracy, recall and F1 value, and can better preserve the network topology and node content information of graphs in the software's graph structure. © 2023 Elsevier B.V.","Graph neural networks; Multitype buffer overflow vulnerability; Self-attentive; Software graph structure","Buffer storage; Forecasting; Graph neural networks; Network security; Buffer overflows; Features sets; Features vector; Graph neural networks; Graph structures; Multitype; Multitype buffer overflow vulnerability; Prediction methods; Self-attentive; Software graph structure; Graphic methods","Article","Final","","Scopus","2-s2.0-85158061817"
"Zou P.; Yang S.; Zhang T.; Wei S.","Zou, Ping (57807214800); Yang, Shujie (57190405213); Zhang, Tao (57198706164); Wei, Shitong (58160872800)","57807214800; 57190405213; 57198706164; 58160872800","Risk-Aware SFC Placement Method in Edge Cloud Environment","2023","Communications in Computer and Information Science","1696 CCIS","","","105","116","11","","10.1007/978-981-19-9697-9_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151055144&doi=10.1007%2f978-981-19-9697-9_9&partnerID=40&md5=c1e8814cd9efa14dec6cfa15758e35fd","Network Function Virtualization (NFV) is a technology that separates software from hardware, NFV increases the manageability, scalability, and flexibility of network function configuration. However, with these conveniences, it is the scheduling and management of complex virtual network resources created by NFV. The key to these problems is the deployment of the Service Function Chain (SFC). There are two major problems in SFC deployment: 1) Complexity of resource scheduling. 2) Vulnerability of SFC. To solve the above problems, we propose a risk-aware SFC deployment method. LSTM (Long Short-Term Memory) is used to predict possible attacks, and DQN (Deep Q Network) uses its results to complete SFC deployments. Our model is validated in a simulation network. The results show that proposed risk-aware SFC deployment is significantly better than traditional resource-oriented deployment in terms of network elasticity, and is not inferior to it in terms of latency. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Attack prediction; Deep reinforcement learning; SFC","Complex networks; Long short-term memory; Network function virtualization; Transfer functions; Attack prediction; Cloud environments; Deep reinforcement learning; Edge clouds; Network functions; Placement methods; Reinforcement learnings; Risk aware; Service function chain; Service functions; Reinforcement learning","Conference paper","Final","","Scopus","2-s2.0-85151055144"
"Gu Y.; Hu F.; Guang Y.","Gu, Yeming (57423401600); Hu, Fan (58660250400); Guang, Yan (55574847200)","57423401600; 58660250400; 55574847200","Semantic Contribution-based Binary Code Scoring","2023","2023 IEEE International Conference on Sensors, Electronics and Computer Engineering, ICSECE 2023","","","","153","158","5","","10.1109/ICSECE58870.2023.10263351","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175009684&doi=10.1109%2fICSECE58870.2023.10263351&partnerID=40&md5=0734462556ead6e03a522ba6f15d2770","Binary code scoring analyzes a specified binary code region and provides an assessment of its weakness. Code weakness is not the same as software vulnerability; it represents the probability of future software vulnerabilities in the target code. Current vulnerability prediction research based on neural network models mostly focuses on directly locating vulnerable code positions. However, related studies have shown limitations in the neural network models' ability to learn binary code features, making it difficult to effectively distinguish subtle differences in code. Direct vulnerability detection using neural network models fails to identify vulnerabilities caused by processes such as code porting, functional adjustments, and patches. Additionally, due to the model's inherent limitations, it results in significant missed detections and false positives. To address these issues, this paper proposes a code scoring method based on semantic contribution. By utilizing the semantic function call graph (SemFCG) to represent code features and function call relationships, a matrix representation is achieved. The semantic attention model SemFCGAT is designed to learn semantic relationships between functions within SemFCG. Finally, code scoring is implemented based on the semantic contribution of neighboring functions to the central function. Experimental results demonstrate that our method can effectively score specified code regions, providing support for various software analysis tasks.  © 2023 IEEE.","Binary code; Deep learning; Graph embedding; Scoring; Semantic","Binary codes; Deep learning; Network security; Neural network models; Current vulnerabilities; Deep learning; Function-call graphs; Graph embeddings; Learn+; Neural network model; Scoring; Semantic functions; Software vulnerabilities; Target codes; Semantics","Conference paper","Final","","Scopus","2-s2.0-85175009684"
"Groppe J.; Groppe S.; Möller R.","Groppe, Jinghua (22979685000); Groppe, Sven (22234205300); Möller, Ralf (57205092803)","22979685000; 22234205300; 57205092803","Variables are a Curse in Software Vulnerability Prediction","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","14146 LNCS","","","516","521","5","","10.1007/978-3-031-39847-6_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174731733&doi=10.1007%2f978-3-031-39847-6_41&partnerID=40&md5=f0b613ff2541113683a48e118a8bb148","Deep learning-based approaches for software vulnerability prediction currently mainly rely on the original text of software code as the feature of nodes in the graph of code and thus could learn a representation that is only specific to the code text, rather than the representation that depicts the ‘intrinsic’ functionality of a program hidden in the text representation. One curse that causes this problem is an infinite number of possibilities to name a variable. In order to lift the curse, in this work we introduce a new type of edge called name dependence, a type of abstract syntax graph based on the name dependence, and an efficient node representation method named 3-property encoding scheme. These techniques will allow us to remove the concrete variable names from code, and facilitate deep learning models to learn the functionality of software hidden in diverse code expressions. The experimental results show that the deep learning models built on these techniques outperform the ones based on existing approaches not only in the prediction of vulnerabilities but also in the memory need. The factor of memory usage reductions of our techniques can be up to the order of 30,000 in comparison to existing approaches. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.","3-property encoding; abstract syntax graph; deep learning; name dependence; software security; software vulnerability","Abstracting; Deep learning; Encoding (symbols); Graphic methods; Learning systems; Signal encoding; Syntactics; 3-property encoding; Abstract syntax; Abstract syntax graph; Deep learning; Encodings; Name dependence; Property; Software security; Software vulnerabilities; Syntax graphs; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85174731733"
"Hanifi K.; Fouladi R.F.; Unsalver B.G.; Karadag G.","Hanifi, Khadija (57195217883); Fouladi, Ramin F. (55807299700); Unsalver, Basak Gencer (57218542961); Karadag, Goksu (57208707247)","57195217883; 55807299700; 57218542961; 57208707247","Software Vulnerability Prediction Knowledge Transferring Between Programming Languages","2023","International Conference on Evaluation of Novel Approaches to Software Engineering, ENASE - Proceedings","2023-April","","","479","486","7","","10.5220/0011859800003464","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160541772&doi=10.5220%2f0011859800003464&partnerID=40&md5=a06890f1dc7f268ca463ca74e3b93986","Developing automated and smart software vulnerability detection models has been receiving great attention from both research and development communities. One of the biggest challenges in this area is the lack of code samples for all different programming languages. In this study, we address this issue by proposing a transfer learning technique to leverage available datasets and generate a model to detect common vulnerabilities in different programming languages. We use C source code samples to train a Convolutional Neural Network (CNN) model, then, we use Java source code samples to adopt and evaluate the learned model. We use code samples from two benchmark datasets: NIST Software Assurance Reference Dataset (SARD) and Draper VDISC dataset. The results show that proposed model detects vulnerabilities in both C and Java codes with average recall of 72%. Additionally, we employ explainable AI to investigate how much each feature contributes to the knowledge transfer mechanisms between C and Java in the proposed model. Copyright © 2023 by SCITEPRESS - Science and Technology Publications, Lda. Under CC license (CC BY-NC-ND 4.0)","Machine Learning; Software Security; Source Code; Transfer Learning; Vulnerability Prediction","C (programming language); Convolutional neural networks; Knowledge management; Learning systems; Transfer learning; Detection models; Machine-learning; Research communities; Smart softwares; Software security; Software vulnerabilities; Source codes; Transfer learning; Vulnerability detection; Vulnerability prediction; Java programming language","Conference paper","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85160541772"
"Koufopoulou A.-A.; Papadimitriou A.; Pikrakis A.; Psarakis M.; Hely D.","Koufopoulou, Amalia-Artemis (57956278300); Papadimitriou, Athanasios (35300244100); Pikrakis, Aggelos (6507232714); Psarakis, Mihalis (6603176127); Hely, David (56024948300)","57956278300; 35300244100; 6507232714; 6603176127; 56024948300","On the Prediction of Hardware Security Properties of HLS Designs Using Graph Neural Networks","2023","Proceedings - IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems, DFT","","","","","","","","10.1109/DFT59622.2023.10313544","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179004344&doi=10.1109%2fDFT59622.2023.10313544&partnerID=40&md5=58626451c3023ace32aa70b18fbd506b","High-level synthesis (HLS) tools have provided significant productivity enhancements to the design flow of digital systems in recent years, resulting in highly-optimized circuits, in terms of area and latency. Given the evolution of hardware attacks, which can render them vulnerable, it is essential to consider security as a significant aspect of the HLS design flow. Yet the need to evaluate a huge number of functionally equivalent designs of the HLS design space challenges hardware security evaluation methods (e.g., fault injection - FI campaigns). In this work, we propose an evaluation methodology of hardware security properties of HLS-produced designs using state-of-the-art Graph Neural Network (GNN) approaches that achieves significant speedup and better scalability than typical evaluation methods (such as FI). We demonstrate the proposed methodology on a Double Modular Redundancy (DMR) countermeasure applied on an AES SBox implementation, enhanced by diversifying the redundant modules through HLS directives. The experimental results show that GNNs can be efficiently trained to predict important hardware security metrics concerning fault attacks (e.g., critical and detection error rates), by using regression. The proposed method predicts the fault vulnerability metrics of the HLS-based designs with high R-squared scores and achieves huge speedup compared to fault injection once the training of the GNN is completed. © 2023 IEEE.","Countermeasures; Fault Injection (FI) Attacks; Graph Neural Networks (GNN); Hardware Security; High-level Synthesis (HLS); Regression","Fault detection; Graph neural networks; Hardware security; Network security; Software testing; Countermeasure; Fault injection; Fault injection attacks; Graph neural network; Graph neural networks; High-level synthesis; Regression; Security properties; Synthesis design; High level synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85179004344"
"Mehta P.; Aggarwal S.; Tandon A.","Mehta, Prarna (57303596200); Aggarwal, Shubhangi (58499639400); Tandon, Abhishek (36025885500)","57303596200; 58499639400; 36025885500","The Effect of Topic Modelling on Prediction of Criticality Levels of Software Vulnerabilities","2023","Informatica (Slovenia)","47","6","","145","158","13","","10.31449/inf.v47i6.3712","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165458286&doi=10.31449%2finf.v47i6.3712&partnerID=40&md5=63d598ca2768e3fd45d8bdd46edb6340","In this day and age, software is an indispensable part of our per diem endeavours, thereby keeping a check on exploitable vulnerabilities has become a vital function of a software firm. The motivation of this paper is to have better understanding of vulnerabilities, creating a tool for the industry practitioners to identify a critical vulnerability that could be detrimental for the firm’s assets. In this article, 1999 vulnerabilities related to Google Chrome was analysed to understand the behaviour of vulnerabilities. The identification of trends and patterns using topic modelling technique led to extraction of topics. The extricated topics were then implemented in 10 classifiers to foresee the criticality of the vulnerability. The resulting performances were also assessed with the classifiers without implementing topic modelling techniques. A 10-fold validation was conducted on the suggested prediction model. © 2023 Slovene Society Informatika. All rights reserved.","CVSS; machine learning; software vulnerabilities; supervised learning; text mining; topic modelling","Data mining; Supervised learning; CVSS; Google+; Machine-learning; Modelling techniques; Performance; Prediction modelling; Software firms; Software vulnerabilities; Text-mining; Topic Modeling; Criticality (nuclear fission)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85165458286"
"Yang Y.-J.; Mao R.-F.; Tan R.; Shen H.-F.; Rong G.-P.","Yang, Yan-Jing (58577121100); Mao, Run-Feng (57221492890); Tan, Rui (58577594200); Shen, Hai-Feng (9336109400); Rong, Guo-Ping (36987162200)","58577121100; 57221492890; 58577594200; 9336109400; 36987162200","Robustness Verification Method for Artificial Intelligence Systems Based on Source Code Processing; [源码处理场景下人工智能系统鲁棒性验证方法]","2023","Ruan Jian Xue Bao/Journal of Software","34","9","","4018","4036","18","","10.13328/j.cnki.jos.006879","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171254326&doi=10.13328%2fj.cnki.jos.006879&partnerID=40&md5=04cd9bba14edeaf29e713e01ea97e794","The development of artificial intelligence (AI) technology provides strong support for AI systems based on source code processing. Compared with natural language processing, source code is special in semantic space. Machine learning tasks related to source code processing usually employ abstract syntax trees, data dependency graphs, and control flow graphs to obtain the structured information of codes and extract features. Existing studies can obtain excellent results in experimental scenarios through in-depth analysis of source code structures and flexible application of classifiers. However, for real application scenarios where the source code structures are more complex, most of the AI systems related to source code processing have poor performance and are difficult to implement in the industry, which triggers practitioners to consider the robustness of AI systems. As AI-based systems are generally data-driven black box systems, it is difficult to directly measure the robustness of these software systems. With the emerging adversarial attack techniques, some scholars in natural language processing have designed adversarial attacks for different tasks to verify the robustness of models and conducted large-scale empirical studies. To solve the instability of AI systems based on source code processing in complex code scenarios, this study proposes robustness verification by Metropolis-Hastings attack method (RVMHM). Firstly, the code preprocessing tool based on abstract syntax trees is adopted to extract the variable pool of the model, and then the MHM source code attack algorithm is employed to replace the prediction effect of the variable perturbation model. The robustness of AI systems is measured by observing the changes in the robustness verification index before and after the attack by interfering with the data and model interaction process. With vulnerability prediction as a typical binary classification scenario of source code processing, this study verifies the robustness of 12 groups of AI vulnerability prediction models on three datasets of open source projects to illustrate the RVMHM effectiveness for robustness verification of source code processing based on AI systems. © 2023 Chinese Academy of Sciences. All rights reserved.","AI system quality evaluation; code adversarial attack; code structure analysis","Classification (of information); Flow graphs; Forecasting; Natural language processing systems; Open source software; Open systems; Quality control; Syntactics; Trees (mathematics); Artificial intelligence system quality evaluation; Artificial intelligence systems; Code adversarial attack; Code structure; Code structure analyse; Natural languages; Quality evaluation; Source codes; Structure analysis; System quality; Semantics","Article","Final","","Scopus","2-s2.0-85171254326"
"Aghao K.R.; Tribhuvan V.","Aghao, Keyur R. (58489569400); Tribhuvan, Vinit (58490329200)","58489569400; 58490329200","Hardware Vulnerability: Meltdown","2023","Lecture Notes in Networks and Systems","676 LNNS","","","217","228","11","","10.1007/978-981-99-1699-3_14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164931069&doi=10.1007%2f978-981-99-1699-3_14&partnerID=40&md5=35f2aa221de9f5b819261799c1b9c48c","Since the dawn of the digital era, the security of the network system and its components has been solely dependent on the security of the software. The hardware was trusted blindly. Initially, the software vulnerabilities were discovered. These vulnerabilities were easy to detect and could be mitigated by modifying the code. The emerging hardware attacks have shattered this trust. Fundamentally, the security of the computer is dependent on memory isolation. This means that the kernel address ranges are not accessible to the user. Hardware attack is not just a code that runs but the hardware itself is modified to perform a specific task. This modification can be carried out at any stage of the hardware manufacturers, such as during design or fabrication. This paper contains a prediction of meltdown. Meltdown reads arbitrary kernel memory regions, including private information such as cryptographic keys and passwords, by taking advantage of the consequences of out-of-order execution on contemporary processors. A revolutionary performance feature found in a wide variety of contemporary CPUs is out-of-order execution. By processing the instruction set simultaneously, out-of-order execution speeds up calculation. This paper introduces and describes the working countermeasures to be implemented for meltdown. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Address spaces; Branch prediction; Meltdown; Out-of-order execution; Return-oriented programming; Speculative execution","Codes (symbols); Computer hardware; Cryptography; Hardware security; Network security; Address space; Branch prediction; Digital era; Hardware attack; Meltdown; Network systems; Out-of-order execution; Return-oriented programming; Software vulnerabilities; Speculative execution; Program processors","Conference paper","Final","","Scopus","2-s2.0-85164931069"
"Hoque M.S.; Jamil N.; Amin N.; Rahim A.A.A.; Jidin R.B.","Hoque, Mohammad Shamsul (57220806665); Jamil, Norziana (36682671900); Amin, Nowshad (7102424614); Rahim, Azril Azam Abdul (57224225526); Jidin, Razali B. (6508169028)","57220806665; 36682671900; 7102424614; 57224225526; 6508169028","Forecasting number of vulnerabilities using long short-term neural memory network","2021","International Journal of Electrical and Computer Engineering","11","5","","4381","4391","10","","10.11591/ijece.v11i5.pp4381-4391","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107269642&doi=10.11591%2fijece.v11i5.pp4381-4391&partnerID=40&md5=723ac43cd37c4b8c277a0e118de6c6a1","Cyber-attacks are launched through the exploitation of some existing vulnerabilities in the software, hardware, system and/or network. Machine learning algorithms can be used to forecast the number of post release vulnerabilities. Traditional neural networks work like a black box approach; hence it is unclear how reasoning is used in utilizing past data points in inferring the subsequent data points. However, the long short-term memory network (LSTM), a variant of the recurrent neural network, is able to address this limitation by introducing a lot of loops in its network to retain and utilize past data points for future calculations. Moving on from the previous finding, we further enhance the results to predict the number of vulnerabilities by developing a time series-based sequential model using a long short-term memory neural network. Specifically, this study developed a supervised machine learning based on the non-linear sequential time series forecasting model with a long short-term memory neural network to predict the number of vulnerabilities for three vendors having the highest number of vulnerabilities published in the national vulnerability database (NVD), namely microsoft, IBM and oracle. Our proposed model outperforms the existing models with a prediction result root mean squared error (RMSE) of as low as 0.072. © 2021 Institute of Advanced Engineering and Science. All rights reserved.","Information security; Long short-term memory network; Recurrent neural network; Supervised machine learning; Threat intelligence; Time series; Vulnerability prediction model","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85107269642"
"Jabeen G.; Rahim S.; Afzal W.; Khan D.; Khan A.A.; Hussain Z.; Bibi T.","Jabeen, Gul (54791058900); Rahim, Sabit (53878322000); Afzal, Wasif (24832739300); Khan, Dawar (56494381400); Khan, Aftab Ahmed (57198816614); Hussain, Zahid (58252812800); Bibi, Tehmina (57189687659)","54791058900; 53878322000; 24832739300; 56494381400; 57198816614; 58252812800; 57189687659","Machine learning techniques for software vulnerability prediction: a comparative study","2022","Applied Intelligence","52","15","","17614","17635","21","","10.1007/s10489-022-03350-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127586282&doi=10.1007%2fs10489-022-03350-5&partnerID=40&md5=05592ce2e5c7332cdbeacd2e92bb94b8","Software vulnerabilities represent a major cause of security problems. Various vulnerability discovery models (VDMs) attempt to model the rate at which the vulnerabilities are discovered in a software. Although several VDMs have been proposed, not all of them are universally applicable. Also most of them seldom give accurate predictive results for every type of vulnerability dataset. The use of machine learning (ML) techniques has generally found success in a wide range of predictive tasks. Thus, in this paper, we conducted an empirical study on applying some well-known machine learning (ML) techniques as well as statistical techniques to predict the software vulnerabilities on a variety of datasets. The following ML techniques have been evaluated: cascade-forward back propagation neural network, feed-forward back propagation neural network, adaptive-neuro fuzzy inference system, multi-layer perceptron, support vector machine, bagging, M5Rrule, M5P and reduced error pruning tree. The following statistical techniques have been evaluated: Alhazmi-Malaiya model, linear regression and logistic regression model. The applicability of the techniques is examined using two separate approaches: goodness-of-fit to see how well the model tracks the data, and prediction capability using different criteria. It is observed that ML techniques show remarkable improvement in predicting the software vulnerabilities than the statistical vulnerability prediction models. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Machine learning; Prediction models; Software vulnerability","Backpropagation; Forecasting; Fuzzy inference; Fuzzy neural networks; Fuzzy systems; Network security; Support vector machines; Comparatives studies; Discovery model; Empirical studies; Machine learning techniques; Machine-learning; Prediction modelling; Security problems; Software vulnerabilities; Statistical techniques; Vulnerability discovery; Multilayer neural networks","Article","Final","","Scopus","2-s2.0-85127586282"
"Yin J.; Li M.; Huo W.","Yin, Jiawei (57202856531); Li, Menghao (57189330422); Huo, Wei (57202860764)","57202856531; 57189330422; 57202860764","Survey on Security Researches of Processor’s Microarchitecture; [处理器微体系结构安全研究综述]","2022","Journal of Cyber Security","7","4","","17","31","14","","10.19363/J.cnki.cn10-1380/tn.2022.07.02","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138047373&doi=10.19363%2fJ.cnki.cn10-1380%2ftn.2022.07.02&partnerID=40&md5=be98d8be4a91c5d984e6b0dc463e655d","In the instruction pipeline, cache structures such as Cache and TLB, which are added to improve the execution efficiency of computer systems, are shared by different processes. The sharing of these cache structures and related execution units between different processes breaks the security boundary implemented in computer systems based on memory isolation, which in turn breaks the confidentiality and integrity of entire computer systems. The disclosure of attacks on processor’s micro-architecture such as Spectre and Meltdown indicates that the performance optimization techniques, such as out-of-order execution, branch prediction and speculative execution, that are used in current processors have some serious security flaws. They are capable to threat the entire computer ecosystem. Although there are many methods and tools for vulnerability detection and security protection of operating system kernel and user space applications, these methods and tools are not capable to be directly applied to detect the micro-architecture vulnerabilities which are hidden in the micro-architecture. Once a vulnerability occurs in a micro-architecture, it will be more dangerous and difficult to fix. In addition, because the implementation details of micro-architecture are not published by the processor vendors (e.g., Intel, AMD, and ARM), micro-architecture remains in a black-box state for micro-architecture security researchers. Moreover, there is a lack of tools and methods to assist in the analysis of micro-architecture. This also makes the security analysis of micro-architecture very difficult. Therefore, In this paper, we begin with the security threats in the current design of processor micro-architecture to analyze the roots of the micro-architecture vulnerabilities, and summarize seven attack methods on the existing processor micro-architecture. We systematically illustrate 10 kinds of software and hardware defense mechanisms and summarize the effects of them. Besides, we further discuss the research and development trend of micro-architecture security from the vulnerability examination approaches, vulnerability protection methods and security designs. © 2022 Chinese Academy of Sciences. All right reserved.","defense methods; information leakage; micro-instruction set vulnerability; processor’s micro-architecture security; side channel attack","","Article","Final","","Scopus","2-s2.0-85138047373"
"Hin D.; Kan A.; Chen H.; Babar M.A.","Hin, David (57213001510); Kan, Andrey (57217611527); Chen, Huaming (56021883400); Babar, M. Ali (6602842620)","57213001510; 57217611527; 56021883400; 6602842620","LineVD: Statement-level Vulnerability Detection using Graph Neural Networks","2022","Proceedings - 2022 Mining Software Repositories Conference, MSR 2022","","","","596","607","11","","10.1145/3524842.3527949","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134019413&doi=10.1145%2f3524842.3527949&partnerID=40&md5=5148d83b0082ec460c9f6f5ef06cd8e1","Current machine-learning based software vulnerability detection methods are primarily conducted at the function-level. However, a key limitation of these methods is that they do not indicate the specific lines of code contributing to vulnerabilities. This limits the ability of developers to efficiently inspect and interpret the predictions from a learnt model, which is crucial for integrating machine-learning based tools into the software development work-flow. Graph-based models have shown promising performance in function-level vulnerability detection, but their capability for statement-level vulnerability detection has not been extensively explored. While interpreting function-level predictions through explainable AI is one promising direction, we herein consider the statement-level software vulnerability detection task from a fully supervised learning perspective. We propose a novel deep learning framework, LineVD, which formulates statement-level vulnerability detection as a node classification task. LineVD leverages control and data dependencies between statements using graph neural networks, and a transformer-based model to encode the raw source code tokens. In particular, by addressing the conflicting outputs between function-level and statement-level information, LineVD significantly improve the prediction performance without vulnerability status for function code. We have conducted extensive experi-ments against a large-scale collection of real-world C/C++ vulnerabilities obtained from multiple real-world projects, and demonstrate an increase of 105% in F1-score over the current state-of-the-art. © 2022 ACM.","Deep Learning; Program Representation; Software Vulnerability Detection","C++ (programming language); Deep learning; Flow graphs; Forecasting; Graph neural networks; Graphic methods; Learning systems; 'current; Deep learning; Detection methods; Functions level; Graph neural networks; Machine-learning; Program representations; Software vulnerabilities; Software vulnerability detection; Vulnerability detection; Software design","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85134019413"
"Shi F.; Kai S.; Zheng J.; Zhong Y.","Shi, Fan (57216460227); Kai, Shaofeng (57607897200); Zheng, Jinghua (57194869257); Zhong, Yao (57903871100)","57216460227; 57607897200; 57194869257; 57903871100","XLNet-Based Prediction Model for CVSS Metric Values","2022","Applied Sciences (Switzerland)","12","18","8983","","","","","10.3390/app12188983","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138679171&doi=10.3390%2fapp12188983&partnerID=40&md5=f6383b919d5954a3c7ac4d695411e0cf","A plethora of software vulnerabilities are exposed daily, posing a severe threat to the Internet. It is almost impossible for security experts or software developers to deal with all vulnerabilities. Therefore, it is imperative to rapidly assess the severity of the vulnerability to be able to select which one should be given preferential attention. CVSS is now the industry’s de facto evaluation standard, which is calculated with a quantitative formula to measure the severity of a vulnerability. The CVSS formula consists of several metrics related to the vulnerability’s features. Security experts need to determine the values of each metric, which is tedious and time-consuming, therefore hindering the efficiency of severity assessment. To address this problem, in this paper, we propose a method based on a pre-trained model for the prediction of CVSS metric values. More specifically, this method utilizes the XLNet model that is fine-tuned with a self-built corpus to predict the metric values from the vulnerability description text, thus reducing the burden of the assessment procedure. To verify the performance of our method, we compare the XLNet model with other pre-trained models and conventional machine learning techniques. The experimental results show that the method outperforms these models on evaluation metrics, reaching state-of-the-art performance levels. © 2022 by the authors.","CVSS; cybersecurity; fine-tuning; machine learning; pre-trained model; predictability; vulnerability assessment","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85138679171"
"Sorrentino J.; Silva P.; Baye G.; Kul G.; Fiondella L.","Sorrentino, Julia (58046189200); Silva, Priscila (57847576900); Baye, Gaspard (57425027800); Kul, Gokhan (56344509500); Fiondella, Lance (24766252300)","58046189200; 57847576900; 57425027800; 56344509500; 24766252300","Covariate Software Vulnerability Discovery Model to Support Cybersecurity Test & Evaluation (Practical Experience Report)","2022","Proceedings - International Symposium on Software Reliability Engineering, ISSRE","2022-October","","","157","168","11","","10.1109/ISSRE55969.2022.00025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145873367&doi=10.1109%2fISSRE55969.2022.00025&partnerID=40&md5=f58d599e45f0e3ac7b3d54948297fd43","Vulnerability discovery models (VDM) have been proposed as an application of software reliability growth models (SRGM) to software security related defects. VDM model the number of vulnerabilities discovered as a function of testing time, enabling quantitative measures of security. Despite their obvious utility, past VDM have been limited to parametric forms that do not consider the multiple activities software testers undertake in order to identify vulnerabilities. In contrast, covariate SRGM characterize the software defect discovery process in terms of one or more test activities. However, data sets documenting multiple security testing activities suitable for application of covariate models are not readily available in the open literature. To demonstrate the applicability of covariate SRGM to vul-nerability discovery, this research identified a web application to target as well as multiple tools and techniques to test for vulnerabilities. The time dedicated to each test activity and the corresponding number of unique vulnerabilities discovered were documented and prepared in a format suitable for application of covariate SRGM. Analysis and prediction were then performed and compared with a flexible VDM without covariates, namely the Alhazmi-Malaiya Logistic Model (AML). Our results indicate that covariate VDM significantly outperformed the AML model on predictive and information theoretic measures of goodness of fit, suggesting that covariate VDM are a suitable and effective method to predict the impact of applying specific vulnerability discovery tools and techniques. © 2022 IEEE.","covariate model; cybersecurity; penetration testing; Software reliability; vulnerability discovery","Application programs; Cybersecurity; Defects; Information theory; Software testing; Covariate model; Covariates; Cyber security; Discovery model; Logistics model; Penetration testing; Software reliability growth models; Software-Reliability; Tools and techniques; Vulnerability discovery; Software reliability","Conference paper","Final","","Scopus","2-s2.0-85145873367"
"Leverett É.; Rhode M.; Wedgbury A.","Leverett, Éireann (57207245252); Rhode, Matilda (57194123626); Wedgbury, Adam (57218310562)","57207245252; 57194123626; 57218310562","Vulnerability Forecasting: Theory and Practice","2022","Digital Threats: Research and Practice","3","4","42","","","","","10.1145/3492328","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138711662&doi=10.1145%2f3492328&partnerID=40&md5=af00b92e008b99de6568920e2880f52c","It is possible to forecast the volume of CVEs released within a time frame with a given prediction interval. For example, the number of CVEs published between now and a year from now can be forecast within 8% of the actual value. Different predictive algorithms perform well at different lookahead values other than 365 days, such as monthly, quarterly, and half year. It is also possible to estimate the proportions of that total volume belonging to specific vendors, software, CVSS scores, or vulnerability types. Some vendors and products can be predicted with accuracy, others with too much uncertainty to be practically useful. This article documents which vendors are amenable to being forecasted. Strategic patch management should become much easier with these tools, and further uncertainty reductions can be built from the methodologies in this article.  © 2022 Association for Computing Machinery.","CVE; Cyberrisk; forecasting; prediction; vulnerabilities; vulnerability management","CVE; Cyberrisk; Patch management; Prediction interval; Predictive algorithms; Theory and practice; Time frame; Uncertainty; Vulnerability; Vulnerability management; Forecasting","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85138711662"
"Costa J.C.; Roxo T.; Sequeiros J.B.F.; Proenca H.; Inacio P.R.M.","Costa, Joana Cabral (57735726400); Roxo, Tiago (57224776216); Sequeiros, Joao B. F. (56272323400); Proenca, Hugo (14016540600); Inacio, Pedro R. M. (27168727400)","57735726400; 57224776216; 56272323400; 14016540600; 27168727400","Predicting CVSS Metric via Description Interpretation","2022","IEEE Access","10","","","59125","59134","9","","10.1109/ACCESS.2022.3179692","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131760240&doi=10.1109%2fACCESS.2022.3179692&partnerID=40&md5=b1464ad3d6a77f3c3b0b96f8c4571478","Cybercrime affects companies worldwide, costing millions of dollars annually. The constant increase of threats and vulnerabilities raises the need to handle vulnerabilities in a prioritized manner. This prioritization can be achieved through Common Vulnerability Scoring System (CVSS), typically used to assign a score to a vulnerability. However, there is a temporal mismatch between the vulnerability finding and score assignment, which motivates the development of approaches to aid in this aspect. We explore the use of Natural Language Processing (NLP) models in CVSS score prediction given vulnerability descriptions. We start by creating a vulnerability dataset from the National Vulnerability Database (NVD). Then, we combine text pre-processing and vocabulary addition to improve the model accuracy and interpret its prediction reasoning by assessing word importance, via Shapley values. Experiments show that the combination of Lemmatization and 5,000-word addition is optimal for DistilBERT, the outperforming model in our experiments of the NLP methods, achieving state-of-the-art results. Furthermore, specific events (such as an attack on a known software) tend to influence model prediction, which may hinder CVSS prediction. Combining Lemmatization with vocabulary addition mitigates this effect, contributing to increased accuracy. Finally, binary classes benefit the most from pre-processing techniques, particularly when one class is much more prominent than the other. Our work demonstrates that DistilBERT is a state-of-the-art model for CVSS prediction, demonstrating the applicability of deep learning approaches to aid in vulnerability handling. The code and data are available at https://github.com/Joana-Cabral/CVSS-Prediction.  © 2013 IEEE.","Common vulnerability scoring system; deep learning; interpretability; natural language processing; security","Deep learning; Forecasting; Natural language processing systems; Bit-error rate; Common vulnerability scoring systems; Correlation; Deep learning; Interpretability; Predictive models; Security; Vocabulary; Bit error rate","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85131760240"
"Lee S.J.; Kim S.J.; Kim T.W.; Oh Y.Y.; Kim T.W.","Lee, Seong Ju (58166461900); Kim, Sung Joo (58166365900); Kim, Tae Won (58165988200); Oh, Yang Yeol (58166413200); Kim, Tae Wan (57199878995)","58166461900; 58166365900; 58165988200; 58166413200; 57199878995","Prediction of Drought Damage using Meteorological Factor Analysis in Corn","2022","Proceedings of the World Congress on New Technologies","","","","","","1","","10.11159/icepr22.128","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151336612&doi=10.11159%2ficepr22.128&partnerID=40&md5=b4d89d3f42b6619aaa7a57b853bf64f6","In Korea, cultivation of corn and soybeans has been attempted to improve food self-sufficiency by using reclaimed land created in the 1980s.[1] It was well known that crops suffered from early growth and development inhibition, photosynthesis and yield reduction due to drought stress.[2] In this study, the crop evapotranspiration (ETc) was calculated through the analysis of climatic factors in the reclaimed land, and the vulnerability to drought stress was evaluated for each growing season of corn. Corn was cultivated (April 28 to August 31, 2021) in the Saemangeum reclaimed land (Gimje-si, Jeollabuk-do). During the corn cultivation period, the weather station measuring device (Watch model 2900ET) was installed to collect soil moisture content and meteorological data at 1-hour intervals. Standard evapotranspiration (ETo) was calculated using Specware Pro 9 software (FAO-Penman Montrith equation) [3], and crop evapotranspiration (ETc=ETo x Kc) was applied by crop coefficients (Rural Development Administration) for each growing season of corn. In this study, the mean value of crop evapotranspiration (ETc) during the corn cultivation period of the Saemangeum reclaimed land was 6.3mm/day, and the precipitation was 9.2mm/day. The precipitation was higher than the crop evapotranspiration during the corn cultivation period, but in May and June, the crop evapotranspiration (284mm) was higher than the precipitation (190mm). It meaned that irrigation was required during this period. In addition, the soil moisture content in June was also low at 12.6%, which was confirmed to be less than the effective moisture content (0.2 bar to 0.5 bar). It was determined that water shortage would occur in the early stages of growth (G1~G2) during corn cultivation (sowing on April 28) depending on natural rainfall in reclaimed land. Therefore, it was considered to be effective to delay the sowing time to prevent damage to corn grown on reclaimed land. © 2022, Avestia Publishing. All rights reserved.","","","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85151336612"
"Wåreus E.; Duppils A.; Tullberg M.; Hell M.","Wåreus, Emil (57218262501); Duppils, Anton (58688745100); Tullberg, Magnus (58688962600); Hell, Martin (8833678500)","57218262501; 58688745100; 58688962600; 8833678500","Security Issue Classification for Vulnerability Management with Semi-supervised Learning","2022","International Conference on Information Systems Security and Privacy","","","","84","95","11","","10.5220/0010813000003120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176328863&doi=10.5220%2f0010813000003120&partnerID=40&md5=1f1183783120f72dd01716bffdd16ff7","Open-Source Software (OSS) is increasingly common in industry software and enables developers to build better applications, at a higher pace, and with better security. These advantages also come with the cost of including vulnerabilities through these third-party libraries. The largest publicly available database of easily machine-readable vulnerabilities is the National Vulnerability Database (NVD). However, reporting to this database is a human-dependent process, and it fails to provide an acceptable coverage of all open source vulnerabilities. We propose the use of semi-supervised machine learning to classify issues as security-related to provide additional vulnerabilities in an automated pipeline. Our models, based on a Hierarchical Attention Network (HAN), outperform previously proposed models on our manually labelled test dataset, with an F1 score of 71%. Based on the results and the vast number of GitHub issues, our model potentially identifies about 191 036 security-related issues with prediction power over 80%. © 2022 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.","Classification; Machine Learning; Open-Source Software; Semi-supervised Learning; Vulnerabilities","","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85176328863"
"Allodi L.; Massacci F.; Williams J.","Allodi, Luca (42460911100); Massacci, Fabio (55167501300); Williams, Julian (56067715400)","42460911100; 55167501300; 56067715400","The Work-Averse Cyberattacker Model: Theory and Evidence from Two Million Attack Signatures","2022","Risk Analysis","42","8","","1623","1642","19","","10.1111/risa.13732","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105202091&doi=10.1111%2frisa.13732&partnerID=40&md5=e0c926bae6d751f13c12d93051ff66e9","The assumption that a cyberattacker will potentially exploit all present vulnerabilities drives most modern cyber risk management practices and the corresponding security investments. We propose a new attacker model, based on dynamic optimization, where we demonstrate that large, initial, fixed costs of exploit development induce attackers to delay implementation and deployment of exploits of vulnerabilities. The theoretical model predicts that mass attackers will preferably (i) exploit only one vulnerability per software version, (ii) largely include only vulnerabilities requiring low attack complexity, and (iii) be slow at trying to weaponize new vulnerabilities. These predictions are empirically validated on a large data set of observed massed attacks launched against a large collection of information systems. Findings in this article allow cyber risk managers to better concentrate their efforts for vulnerability management, and set a new theoretical and empirical basis for further research defining attacker (offensive) processes. © 2021 The Authors. Risk Analysis published by Wiley Periodicals LLC on behalf of Society for Risk Analysis.","Cyber security; hackers model; risk management; update costs","Computer Security; Information Systems; Models, Theoretical; Risk Management; Digital storage; Network security; Risk management; Attack complexity; Attack signature; Attacker models; Risk management practices; Security investments; Software versions; Theoretical modeling; Vulnerability management; cost analysis; data set; management practice; model; security; software; vulnerability; computer security; information system; risk management; theoretical model; Risk perception","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85105202091"
"Okutan A.; Mirakhorli M.","Okutan, Ahmet (55322858400); Mirakhorli, Mehdi (23390242500)","55322858400; 23390242500","Predicting the Severity and Exploitability of Vulnerability Reports using Convolutional Neural Nets","2022","Proceedings - 3rd International Workshop on Engineering and Cybersecurity of Critical Systems, EnCyCriS 2022","","","","1","8","7","","10.1145/3510003.3510210","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134875980&doi=10.1145%2f3510003.3510210&partnerID=40&md5=09412697e7e3b513463cbe76747e9706","Common Vulnerability and Exposure (CVE) reports published by Vulnerability Management Systems (VMSs) are used to evaluate the severity and exploitability of software vulnerabilities. Public vulnerability databases such as NVD uses the Common Vulnerability Scoring System (CVSS) to assign various scores to CVEs to evaluate their base severity, impact, and exploitability. Previous studies have shown that vulnerability databases rely on a manual, laborintensive and error-prone process which may lead to inconsistencies in the CVE data and delays in the releasing of new CVEs. Furthermore, it was shown that CVSS scoring is based on complex calculations and may not be accurate enough in assessing the potential severity and exploitability of vulnerabilities in real life. This work uses Convolutional Neural Networks (CNN) to train text classification models to automate the prediction of the severity and exploitability of CVEs, and proposes a new exploitability scoring method by creating a Product Hygiene Index based on the Common Product Enumeration (CPE) catalog. Using CVE descriptions published by the NVD and the exploits identified by exploit databases, it trains CNN models to predict the base severity and exploitability of CVEs. Preliminary experiment results and the conducted case study indicate that the severity of CVEs can be predicted automatically with high confidences, and the proposed exploitability scoring method achieves better results compared to the exploitability scoring provided by the NVD.  © 2022 ACM.","CVE; CVSS Scoring; Exploitability; Software Vulnerability","Classification (of information); Convolution; Convolutional neural networks; Forecasting; Neural network models; Text processing; Common vulnerabilities and exposures; Common vulnerability scoring system scoring; Common vulnerability scoring systems; Convolutional neural network; Exploitability; Management systems; Scoring methods; Software vulnerabilities; Vulnerability database; Vulnerability management; Database systems","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85134875980"
"Chowdhuryy M.H.I.; Yao F.","Chowdhuryy, Md Hafizul Islam (57204513772); Yao, Fan (56005521000)","57204513772; 56005521000","Leaking Secrets Through Modern Branch Predictors in the Speculative World","2022","IEEE Transactions on Computers","71","9","","2059","2072","13","","10.1109/TC.2021.3122830","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118579276&doi=10.1109%2fTC.2021.3122830&partnerID=40&md5=4b05554644de313f0af876a01397cced","Transient execution attacks that exploit speculation have raised significant concerns in computer systems. Typically, branch predictors are leveraged to trigger mis-speculation in transient execution attacks. In this work, we demonstrate a new class of speculation-based attacks that targets the branch prediction unit (BPU). We find that speculative resolution of conditional branches (i.e., in nested speculation) alter the states of pattern history table (PHT) in modern processors, which are not restored after the corresponding branches are later squashed. Such characteristic allows attackers to exploit the BPU as the secret transmitting medium in transient execution attacks. To evaluate the discovered vulnerability, we build a novel attack framework, BranchSpectre, that enables exfiltration of unintended secrets through observing speculative PHT updates (in the form of covert and side channels). We further investigate the PHT collision mechanism in the history-based predictor and the branch prediction mode transitions in Intel processors. Built upon such knowledge, we implement an ultra-high speed covert channel (BranchSpectre-cc) as well as two side channels (i.e., BranchSpectre-v1 and BranchSpectre-v2) that merely rely on BPU for mis-speculation trigger and secret inference in the speculative domain. Notably, BranchSpectre side channels can take advantage of much simpler code patterns than those used in Spectre attacks. We present an extensive BranchSpectre code gadget analysis on a set of popular real-world application code bases followed by a demonstration of side channel attack on OpenSSL. The evaluation results show substantially wider existence and higher exploitability of BranchSpectre code patterns in real-world software. Finally, we discuss several secure branch prediction mechanisms that can mitigate transient execution attacks exploiting modern branch predictors.  © 1968-2012 IEEE.","Branch predictor; nested speculation; pattern history; side channels; transient execution attacks","Codes (symbols); Forecasting; Program processors; Side channel attack; Transient analysis; Branch prediction; Branch predictors; Code; Conditional branch; Hardware; Micro architectures; Real-world; Side-channel; Speculative execution; Transient execution attack; Timing circuits","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85118579276"
"Alenezi F.N.; Mehmood T.","Alenezi, Freeh N. (57210197026); Mehmood, Tahir (57214700905)","57210197026; 57214700905","Data-driven Predictive Model of Windows 10's Vulnerabilities","2022","International Conference on Electrical, Computer, Communications and Mechatronics Engineering, ICECCME 2022","","","","","","","","10.1109/ICECCME55909.2022.9988548","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146425865&doi=10.1109%2fICECCME55909.2022.9988548&partnerID=40&md5=b00faf476ef44afede7d2190b48922af","A threat source that might exploit or create a hole in an information system, system security procedures, internal controls, or implementation is a computer operating system vulnerability. Since information security is a problem for everyone, predicting it is crucial. The typical method of vulnerability prediction involves manually identifying traits that might be related to unsafe code. An open framework for defining the characteristics and seriousness of software problems is called the Common Vulnerability Scoring System (CVSS). Base, Temporal, and Environmental are the three metric categories in CVSS. In this research, neural networks are utilized to build a predictive model of Windows 10 vulnerabilities using the published vulnerability data in the National Vulnerability Database. Different variants of neural networks are used which implements the back propagation for training the operating system vulnerabilities scores ranging from 0 to 10. Additionally, the research identifies the influential factors using Loess variable importance in neural networks, which shows that access complexity and polarity are only marginally important for predicting operating system vulnerabilities, while confidentiality impact, integrity impact, and availability impact are highly important. © 2022 IEEE.","Common Vulnerability Scoring System; Neural Network; Variable Selection; Vulnerability","Computer operating systems; Forecasting; Network security; Common vulnerability scoring systems; Data driven; Internal controls; Neural-networks; Operating system vulnerability; Predictive models; Security procedures; System security; Variables selections; Vulnerability; Backpropagation","Conference paper","Final","","Scopus","2-s2.0-85146425865"
"Barisal S.K.; Kishore P.; Nayak G.; Hira R.P.; Kumar R.; Kumar R.","Barisal, Swadhin Kumar (57209334434); Kishore, Pushkar (57213193273); Nayak, Gayatri (57191842936); Hira, Ridhy Pratim (57763342800); Kumar, Rohit (58263759200); Kumar, Ritesh (57198684551)","57209334434; 57213193273; 57191842936; 57763342800; 58263759200; 57198684551","Concolic-Based Software Vulnerability Prediction Using Ensemble Learning","2022","Smart Innovation, Systems and Technologies","271","","","231","241","10","","10.1007/978-981-16-8739-6_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132743549&doi=10.1007%2f978-981-16-8739-6_21&partnerID=40&md5=be42b510d0119b21700effc559bb1c65","Detecting software vulnerabilities are critical for limiting the damage caused by hostile exploits and program failures. This frequently necessitates the accurate identification of susceptible execution routes. However, as software complexity has increased, it has become notoriously difficult to find such susceptible paths by exploring the entire program execution space. Therefore, concolic testing is used in this paper as one of the ways to deal with this problem. Here, the observations and discoveries are collected from experimenting and implementing concolic testing. First, several trained classifier models like random forest, support vector machine, stochastic gradient descent, and AdaBoost are tested against a test dataset created by randomly selecting 30% of the data from the original dataset. Then, multiple classifier models help predict whether a program is faulty or benign. After testing out several classifier models, an ensemble is done on the top 3 highest accuracy classifiers. Overall, 87% accuracy is achieved with an F1-score of 85.1%. This result indicates that 87% of the program’s labels are accurately detected by our proposed model while higher F1-score represents the proposed model’s balanced detection. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Classifier model; Concolic testing; Ensemble technique; Software vulnerabilities","Adaptive boosting; Classification (of information); Damage detection; Decision trees; Gradient methods; Learning systems; Software testing; Statistical tests; Stochastic models; Stochastic systems; Classifier models; Concolic testing; Ensemble learning; Ensemble techniques; F1 scores; Program execution; Programme failures; Random forests; Software complexity; Software vulnerabilities; Support vector machines","Conference paper","Final","","Scopus","2-s2.0-85132743549"
"Haryono S.A.; Kang H.J.; Sharma A.; Sharma A.; Santosa A.; Yi A.M.; Lo D.","Haryono, Stefanus A. (57216455730); Kang, Hong Jin (57215115758); Sharma, Abhishek (57202966237); Sharma, Asankhaya (56298660300); Santosa, Andrew (6506343800); Yi, Ang Ming (57219532844); Lo, David (35269388000)","57216455730; 57215115758; 57202966237; 56298660300; 6506343800; 57219532844; 35269388000","Automated Identification of Libraries from Vulnerability Data: Can We Do Better?","2022","IEEE International Conference on Program Comprehension","2022-March","","","178","189","11","","10.1145/3524610.3527893","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133207922&doi=10.1145%2f3524610.3527893&partnerID=40&md5=bf961148a9858ac4a9ae611bbf3a59e0","Software engineers depend heavily on software libraries and have to update their dependencies once vulnerabilities are found in them. Software Composition Analysis (SCA) helps developers identify vulnerable libraries used by an application. A key challenge is the identification of libraries related to a given reported vulnerability in the National Vulnerability Database (NVD), which may not ex-plicitly indicate the affected libraries. Recently, researchers have tried to address the problem of identifying the libraries from an NVD report by treating it as an extreme multi-label learning (XML) problem, characterized by its large number of possible labels and severe data sparsity. As input, the NVD report is provided, and as output, a set of relevant libraries is returned. In this work, we evaluated multiple XML techniques. While pre-vious work only evaluated a traditional XML technique, FastXML, we trained four other traditional XML models (DiSMEC, Parabel, Bonsai, ExtremeText) as well as two deep learning-based models (XML-CNN and LightXML). We compared both their effectiveness and the time cost of training and using the models for predictions. We find that other than DiSMEC and XML-CNN, recent XML mod-els outperform the FastXML model by 3%-10% in terms of F1-scores on Top-k (k=1,2,3) predictions. Furthermore, we observe significant improvements in both the training and prediction time of these XML models, with Bonsai and Parabel model achieving 627x and 589x faster training time and 12x faster prediction time from the FastXML baseline. We discuss the implications of our experimental results and highlight limitations for future work to address.  © 2022 ACM.","machine learning; multi-label classification; vulnerability report","Application programs; Classification (of information); Deep learning; Forecasting; Learning systems; Libraries; Automated identification; Machine-learning; Multi-label classifications; National vulnerability database; Prediction time; Software composition; Software libraries; Training time; Vulnerability report; XML models; XML","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85133207922"
"Ganesh S.; Palma F.; Olsson T.","Ganesh, Sundarakrishnan (57535400300); Palma, Francis (55329588400); Olsson, Tobias (36504623000)","57535400300; 55329588400; 36504623000","Are Source Code Metrics “Good Enough” in Predicting Security Vulnerabilities? †","2022","Data","7","9","127","","","","","10.3390/data7090127","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138644516&doi=10.3390%2fdata7090127&partnerID=40&md5=090fb3c394f750e1459023a09f34cc55","Modern systems produce and handle a large volume of sensitive enterprise data. Therefore, security vulnerabilities in the software systems must be identified and resolved early to prevent security breaches and failures. Predicting security vulnerabilities is an alternative to identifying them as developers write code. In this study, we studied the ability of several machine learning algorithms to predict security vulnerabilities. We created two datasets containing security vulnerability information from two open-source systems: (1) Apache Tomcat (versions 4.x and five 2.5.x minor versions). We also computed source code metrics for these versions of both systems. We examined four classifiers, including Naive Bayes, Decision Tree, XGBoost Classifier, and Logistic Regression, to show their ability to predict security vulnerabilities. Moreover, an ensemble learner was introduced using a stacking classifier to see whether the prediction performance could be improved. We performed cross-version and cross-project predictions to assess the effectiveness of the best-performing model. Our results showed that the XGBoost classifier performed best compared to other learners, i.e., with an average accuracy of 97% in both datasets. The stacking classifier performed with an average accuracy of 92% in Struts and 71% in Tomcat. Our best-performing model—XGBoost—could predict with an average accuracy of 87% in Tomcat and 99% in Struts in a cross-version setup. © 2022 by the authors.","machine learning; prediction; security vulnerabilities; software metrics; source code","Classification (of information); Decision trees; Learning algorithms; Machine learning; Open source software; Open systems; Struts; Enterprise data; Large volumes; Machine-learning; Security failure; Security vulnerabilities; Software metrics; Software-systems; Source code metrics; Source codes; Stackings; Forecasting","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85138644516"
"Fu M.; Tantithamthavorn C.","Fu, Michael (57685110800); Tantithamthavorn, Chakkrit (55361007600)","57685110800; 55361007600","LineVul: A Transformer-based Line-Level Vulnerability Prediction","2022","Proceedings - 2022 Mining Software Repositories Conference, MSR 2022","","","","608","620","12","","10.1145/3524842.3528452","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134064233&doi=10.1145%2f3524842.3528452&partnerID=40&md5=4542cca8c3b018d30db00071e4f01abc","Software vulnerabilities are prevalent in software systems, causing a variety of problems including deadlock, information loss, or system failures. Thus, early predictions of software vulnerabilities are critically important in safety-critical software systems. Various ML/DL-based approaches have been proposed to predict vulnerabilities at the file/function/method level. Recently, IVDetect (a graph-based neural network) is proposed to predict vulnerabilities at the function level. Yet, the IVDetect approach is still inaccurate and coarse-grained. In this paper, we propose LINEVUL, a Transformer-based line-level vulnerability prediction approach in order to address several limitations of the state-of-the-art IVDetect approach. Through an empirical evaluation of a large-scale real-world dataset with 188k+ C/C++ functions, we show that LINEVUL achieves (1) 160%-379% higher F1-measure for function-level predictions; (2) 12%-25% higher Top-10 Accuracy for line-level predictions; and (3) 29%-53% less Effort@20%Recall than the baseline approaches, highlighting the significant advancement of LINEVUL towards more accurate and more cost-effective line-level vulnerability predictions. Our additional analysis also shows that our LINEVUL is also very accurate (75%-100%) for predicting vulnerable functions affected by the Top-25 most dangerous CWEs, highlighting the potential impact of our LINEVUL in real-world usage scenarios. © 2022 ACM.","AI for Software Engineering; Software Security; Vulnerability Prediction","C++ (programming language); Computer software; Cost effectiveness; Graphic methods; Large dataset; Safety engineering; Systems engineering; AI for software engineering; Early prediction; Functions level; Information loss; Safety-critical software systems; Software security; Software vulnerabilities; Software-systems; System failures; Vulnerability prediction; Forecasting","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85134064233"
"Gangolli A.; Mahmoud Q.H.; Azim A.","Gangolli, Aakash (57765433400); Mahmoud, Qusay H. (6701548037); Azim, Akramul (36023296200)","57765433400; 6701548037; 36023296200","A Machine Learning Based Approach to Detect Fault Injection Attacks in IoT Software Systems","2022","Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics","2022-October","","","2900","2905","5","","10.1109/SMC53654.2022.9945117","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142727991&doi=10.1109%2fSMC53654.2022.9945117&partnerID=40&md5=362ac18cc5534df65b28df2b16aa8a0b","With the rapid growth of Internet of Things (IoT) applications, the security of these systems has become critical. Fault injection attacks are a type of physical attack on the hardware components of an IoT system. These attacks cause the IoT system software to behave abnormally, which the adversaries exploit. Typically, these attacks have been detected through the use of a separate hardware detection mechanism, which is expensive and itself vulnerable to attack. The purpose of this paper is to propose a machine learning based approach to detect the attacks by monitoring specific run-time software parameters in the live environment of an IoT system. The proposed approach generates a labelled dataset by injecting instruction-level faults into the software executable, which is then used to train a machine learning model that can predict whether the IoT software system is currently being affected by a fault injection attack. Using a software fault injection tool to create a labelled dataset enables the use of supervised machine learning techniques, which produce more accurate prediction results than unsupervised techniques. The machine learning model can be used in the live environment of an IoT software system to monitor specific run-time software properties in order to detect the effects of a fault injection attack on the software. Additionally, the model classifies the type of fault introduced into the software as a result of the attack, which can be used to determine the necessary corrective action.  © 2022 IEEE.","attack detection; Fault injection attack; fault model; machine learning; software fault injection; software fault injection dataset; software vulnerability detection","Computer hardware; Fault detection; Learning systems; Network security; Software testing; Supervised learning; Attack detection; Fault injection; Fault injection attacks; Fault model; Machine-learning; Software fault; Software fault injection; Software fault injection dataset; Software vulnerabilities; Software vulnerability detection; Vulnerability detection; Internet of things","Conference paper","Final","","Scopus","2-s2.0-85142727991"
"Batur Şahin C.; Abualigah L.","Batur Şahin, Canan (57207688292); Abualigah, Laith (57190984712)","57207688292; 57190984712","A novel deep learning-based feature selection model for improving the static analysis of vulnerability detection","2021","Neural Computing and Applications","33","20","","14049","14067","18","","10.1007/s00521-021-06047-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105376457&doi=10.1007%2fs00521-021-06047-x&partnerID=40&md5=c52f39cc25cbe3b99160684d2d1baecd","The automatic detection of software vulnerabilities is considered a complex and common research problem. It is possible to detect several security vulnerabilities using static analysis (SA) tools, but comparatively high false-positive rates are observed in this case. Existing solutions to this problem depend on human experts to identify functionality, and as a result, several vulnerabilities are often overlooked. This paper introduces a novel approach for effectively and reliably finding vulnerabilities in open-source software programs. In this paper, we are motivated to examine the potential of the clonal selection theory. A novel deep learning-based vulnerability detection model is proposed to define features using the clustering theory of the clonal selection algorithm. To our knowledge, this is the first time we have used deep-learned long-lived team-hacker features to process memories of sequential features and mapping from the entire history of previous inputs to target vectors in theory. With an immune-based feature selection model, the proposed approach aimed to improve static analyses' detection abilities. A real-world SA dataset is used based on three open-source PHP applications. Comparisons are conducted based on using a classification model for all features to measure the proposed feature selection methods' classification improvement. The results demonstrated that the proposed method got significant enhancements, which occurred in the classification accuracy also in the true positive rate. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Deep learning; Feature selection; Immune systems; Software vulnerability prediction","Classification (of information); Feature extraction; Genetic algorithms; Learning systems; Open source software; Open systems; Personal computing; Static analysis; Classification accuracy; Classification models; Clonal selection algorithms; Feature selection methods; Open source software projects; Security vulnerabilities; Software vulnerabilities; Vulnerability detection; Deep learning","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85105376457"
"Alharbi A.; Hamid M.A.; Lahza H.","Alharbi, Abdulmohsen (57872033500); Hamid, Md. Abdul (14037299100); Lahza, Husam (57191161213)","57872033500; 14037299100; 57191161213","Predicting Malicious Software in IoT Environment Based on Machine Learning and Data Mining Techniques","2022","International Journal of Advanced Computer Science and Applications","13","8","","497","506","9","","10.14569/IJACSA.2022.0130857","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137168412&doi=10.14569%2fIJACSA.2022.0130857&partnerID=40&md5=e6a5b9a19793cb126d6aa9ed161b58a1","The Internet of Things (IoT) enable the IoT to sense and respond using the power of computing to autonomously come up with the best solutions for any industry today. However, Internet of Things have vulnerabilities since it can be hacked by cybercriminals. The cybercriminals know where the IoT vulnerabilities are, such as unsecured update mechanisms and malware (Malicious Software) to attack the IoT devices. The recently posted IoT-23 dataset based on several IoT devices such as Philips Hue, Amazon Echo devices and Somfy door lock were used for machine learning classification algorithms and data mining techniques with training and testing for predictive modelling of a variety of malware attacks like Distributed Denial of Service (DDoS), Command and Control (C&C) and various IoT botnets like Mirai and Okiru. This paper aims to develop predictive modeling that will predict malicious software to protect IoT and reduce vulnerabilities by using machine learning and data mining techniques. We collected, analyzed and processed benign and several of malicious software in IoT network traffic. Malware prediction is crucial in maintaining IoT devices’ safety and security from cybercriminals’ activities. Furthermore, the Principal Component Analysis (PCA) method was applied to determine the important features of IoT-23. In addition, this study compared with previous studies that used the IoT-23 dataset in terms of accuracy rate and other metrics. Experiments show that Random Forest (RF) classifier achieved the predictive model produced classification accuracy 0.9714% as well as predict 8754 samples with various types of malware and obtained 0.9644% of Area Under Curve (AUC) which outperforms several bassline machine learning classification models. © 2022,International Journal of Advanced Computer Science and Applications.All Rights Reserved","Cyber threats; Internet of things; Machine learning; Malware; Predictive modeling","Classification (of information); Cybersecurity; Data mining; Decision trees; Denial-of-service attack; Forecasting; Internet of things; Malware; Network security; Principal component analysis; Statistical tests; Cyber threats; Cybercriminals; Data-mining techniques; Machine data; Machine learning classification; Machine-learning; Malwares; On-machines; Power; Predictive models; Machine learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137168412"
"Croft R.; Babar M.A.; Chen H.","Croft, Roland (57219534056); Babar, M. Ali (6602842620); Chen, Huaming (56021883400)","57219534056; 6602842620; 56021883400","Noisy Label Learning for Security Defects","2022","Proceedings - 2022 Mining Software Repositories Conference, MSR 2022","","","","435","447","12","","10.1145/3524842.3528446","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134072137&doi=10.1145%2f3524842.3528446&partnerID=40&md5=83907a3fdb548fdb3f9bf40c6111809f","Data-driven software engineering processes, such as vulnerability prediction heavily rely on the quality of the data used. In this paper, we observe that it is infeasible to obtain a noise-free security defect dataset in practice. Despite the vulnerable class, the non-vulnerable modules are difficult to be verified and determined as truly exploit free given the limited manual efforts available. It results in uncertainty, introduces labeling noise in the datasets and affects conclusion validity. To address this issue, we propose novel learning methods that are robust to label impurities and can leverage the most from limited label data; noisy label learning. We investigate various noisy label learning methods applied to soft-ware vulnerability prediction. Specifically, we propose a two-stage learning method based on noise cleaning to identify and remediate the noisy samples, which improves AUC and recall of baselines by up to 8.9% and 23.4%, respectively. Moreover, we discuss several hurdles in terms of achieving a performance upper bound with semi-omniscient knowledge of the label noise. Overall, the experimental results show that learning from noisy labels can be effective for data-driven software and security analytics. © 2022 ACM.","Machine learning; noisy label learning; software vulnerabilities","Learning systems; Machine learning; Software engineering; Data driven; Labelings; Learning methods; Machine-learning; Noisy label learning; Noisy labels; Performance; Software engineering process; Software vulnerabilities; Uncertainty; Defects","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85134072137"
"Koutsoupakis I.; Tsompanakis Y.; Soupios P.; Kirmizakis P.; Kaka S.; Providakis C.","Koutsoupakis, Ioannis (57354324500); Tsompanakis, Yiannis (6602648918); Soupios, Pantelis (15835749500); Kirmizakis, Panagiotis (56335704100); Kaka, Sanlinn (6602167255); Providakis, Costas (35614148100)","57354324500; 6602648918; 15835749500; 56335704100; 6602167255; 35614148100","Seismic risk assessment of chania, greece, using an integrated computational approach","2021","Applied Sciences (Switzerland)","11","23","11249","","","","","10.3390/app112311249","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119963807&doi=10.3390%2fapp112311249&partnerID=40&md5=322440f6849efc69798ed48839340372","This study develops a comprehensive seismic risk model for the city of Chania, in Greece, which is located ina highly seismic-prone region due to the occurrenceof moderate to large earthquakes because of the nearby major subduction zone between African and Eurasian tectonic plates. The main aim is to reduce the seismic risk for the study area by incorporating the spatial distribution of the near-surface shear wave velocity model and the soil classification, along with all possible seismic sources, taking into account historical events. The study incorporates and correlates various ground motion scenarios and geological fault zones as well as information on existing buildings to develop a seismic risk model using QuakeIST software, and then the seismic hazard and a realistic prediction of resulting future adverse effects are assessed. The developed model can assist the munic-ipal authorities of Chania to be prepared for potential seismic events, as well as city planners and decisionmakers, who can use the model as an effective decision-making tool to identify the seismic vulnerability of the city buildings and infrastructure. Thus, this study enables the implementation of an appropriate and viable earthquake-related hazards strategy to mitigate damage and losses in future earthquakes. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Assessment; GIS-based tools; Interdependencies; Seismic hazard; Seismic risk; Vulnerability","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85119963807"
"Das S.S.; Halappanavar M.; Tumeo A.; Serra E.; Pothen A.; Al-Shaer E.","Das, Siddhartha Shankar (57222359971); Halappanavar, Mahantesh (49661237100); Tumeo, Antonino (22036780900); Serra, Edoardo (35812641000); Pothen, Alex (6602851606); Al-Shaer, Ehab (6602187244)","57222359971; 49661237100; 22036780900; 35812641000; 6602851606; 6602187244","VWC-BERT: Scaling Vulnerability-Weakness-Exploit Mapping on Modern AI Accelerators","2022","Proceedings - 2022 IEEE International Conference on Big Data, Big Data 2022","","","","1224","1229","5","","10.1109/BigData55660.2022.10020622","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147926221&doi=10.1109%2fBigData55660.2022.10020622&partnerID=40&md5=13cd063d0c9fea82ad0670382503ee40","Defending cybersystems needs accurate mapping of software and hardware vulnerabilities to generalized descriptions of weaknesses, and weaknesses to exploits. These mappings enable cyber defenders to build plans for effective defense and assessment of potential risks to a cybersystem. With close to 200k vulnerabilities, manual mapping is not a feasible option. However, automated mapping is challenging due to limited training data, computational intractability, and limitations in computational natural language processing. Tools based on breakthroughs in Transformer-based language models have been demonstrated to classify vulnerabilities with high accuracy. We make three key contributions in this paper: (1) We present a new framework, VWC-BERT, that augments the Transformer-based hierarchical multi-class classification framework of Das et al. (V2W-BERT) with the ability to map weaknesses to exploits. (2) We implement VWC-BERT on modern AI accelerator platforms using two data parallel techniques for the pre-training phase and demonstrate nearly linear speedups across NVIDIA accelerator platforms. We observe nearly linear speedups for up to 16 V100 and 8 A100 GPUs, and about 3.4× speedup for A100 relative to V100 GPUs. Enabled by scaling, we also demonstrate higher accuracy using a larger language model, RoBERTa-Large. We show up to 87% accuracy for strict and up to 98% accuracy for relaxed classification. (3) We develop a novel parallel link manager for the link prediction phase and demonstrate up to 21× speedup with 16 V100 GPUs relative to one V100 GPU, and thus reducing the runtime from 2.5 hours to 10 minutes. We believe that generalizability and scalability of VWC-BERT will benefit both the theoretical development and practical deployment of novel cyberdefense solutions and vulnerability classification. © 2022 IEEE.","AI Accelerators; Cybersecurity; Deep Learning; Language Models; Transformers","Classification (of information); Computational linguistics; Cybersecurity; Deep learning; Mapping; Natural language processing systems; Personnel training; Program processors; Accurate mapping; AI accelerator; Cyber security; Deep learning; High-accuracy; Language model; Linear speed-up; Scalings; Software and hardwares; Transformer; Risk assessment","Conference paper","Final","","Scopus","2-s2.0-85147926221"
"Shu R.; Xia T.; Williams L.; Menzies T.","Shu, Rui (57191822995); Xia, Tianpei (57219481614); Williams, Laurie (35565101900); Menzies, Tim (7003835495)","57191822995; 57219481614; 35565101900; 7003835495","Dazzle: Using Optimized Generative Adversarial Networks to Address Security Data Class Imbalance Issue","2022","Proceedings - 2022 Mining Software Repositories Conference, MSR 2022","","","","144","155","11","","10.1145/3524842.3528437","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134026754&doi=10.1145%2f3524842.3528437&partnerID=40&md5=884d64c9e1c22ade9b978f61c078d8eb","Background: Machine learning techniques have been widely used and demonstrate promising performance in many software security tasks such as software vulnerability prediction. However, the class ratio within software vulnerability datasets is often highly imbalanced (since the percentage of observed vulnerability is usually very low). Goal: To help security practitioners address software security data class imbalanced issues and further help build better prediction models with resampled datasets. Method: We introduce an approach called Dazzle which is an optimized version of conditional Wasserstein Generative Adversarial Networks with gradient penalty (cWGAN-GP). Dazzle explores the architecture hyperparameters of cWGAN-GP with a novel optimizer called Bayesian Optimization. We use Dazzle to generate minority class samples to resample the original imbalanced training dataset. Results: We evaluate Dazzle with three software security datasets, i.e., Moodle vulnerable files, Ambari bug reports, and JavaScript function code. We show that Dazzle is practical to use and demonstrates promising improvement over existing state-of-the-art oversampling techniques such as SMOTE (e.g., with an average of about 60% improvement rate over SMOTE in recall among all datasets). Conclusion: Based on this study, we would suggest the use of optimized GANs as an alternative method for security vulnerability data class imbalanced issues. © 2022 ACM.","Class Imbalance; Generative Adversarial Networks; Hyperparameter Optimization; Security Vulnerability Prediction","Forecasting; Learning systems; Network security; Class imbalance; Data class; Hyper-parameter optimizations; Machine learning techniques; Performance; Security datum; Security vulnerabilities; Security vulnerability prediction; Software security; Software vulnerabilities; Generative adversarial networks","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85134026754"
"Wartschinski L.; Noller Y.; Vogel T.; Kehrer T.; Grunske L.","Wartschinski, Laura (57196223346); Noller, Yannic (57202002588); Vogel, Thomas (55746572900); Kehrer, Timo (25960845900); Grunske, Lars (8966479000)","57196223346; 57202002588; 55746572900; 25960845900; 8966479000","VUDENC: Vulnerability Detection with Deep Learning on a Natural Codebase for Python","2022","Information and Software Technology","144","","106809","","","","","10.1016/j.infsof.2021.106809","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122640532&doi=10.1016%2fj.infsof.2021.106809&partnerID=40&md5=c9479aca7a958217f564d0f05bfdbfec","Context: Identifying potential vulnerable code is important to improve the security of our software systems. However, the manual detection of software vulnerabilities requires expert knowledge and is time-consuming, and must be supported by automated techniques. Objective: Such automated vulnerability detection techniques should achieve a high accuracy, point developers directly to the vulnerable code fragments, scale to real-world software, generalize across the boundaries of a specific software project, and require no or only moderate setup or configuration effort. Method: In this article, we present VUDENC (Vulnerability Detection with Deep Learning on a Natural Codebase), a deep learning-based vulnerability detection tool that automatically learns features of vulnerable code from a large and real-world Python codebase. VUDENC applies a word2vec model to identify semantically similar code tokens and to provide a vector representation. A network of long-short-term memory cells (LSTM) is then used to classify vulnerable code token sequences at a fine-grained level, highlight the specific areas in the source code that are likely to contain vulnerabilities, and provide confidence levels for its predictions. Results: To evaluate VUDENC, we used 1,009 vulnerability-fixing commits from different GitHub repositories that contain seven different types of vulnerabilities (SQL injection, XSS, Command injection, XSRF, Remote code execution, Path disclosure, Open redirect) for training. In the experimental evaluation, VUDENC achieves a recall of 78%–87%, a precision of 82%–96%, and an F1 score of 80%–90%. VUDENC's code, the datasets for the vulnerabilities, and the Python corpus for the word2vec model are available for reproduction. Conclusions: Our experimental results suggest that VUDENC is capable of outperforming most of its competitors in terms of vulnerably detection capabilities on real-world software. Comparable accuracy was only achieved on synthetic benchmarks, within single projects, or on a much coarser level of granularity such as entire source code files. © 2021","Deep learning; Long-short-term memory network; Natural codebase; Software repository mining; Static analysis; Vulnerability detection","Brain; Cell proliferation; Codes (symbols); High level languages; Long short-term memory; Network coding; Network security; Static analysis; Deep learning; Long-short-term memory network; Memory network; Natural codebase; Real-world; Software repository mining; Software vulnerabilities; Software-systems; Source codes; Vulnerability detection; Python","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85122640532"
"Arya Babu P.; Chandrakaran S.","Arya Babu, P. (57264696500); Chandrakaran, S. (6602427315)","57264696500; 6602427315","Numerical Methods in the Stability Analysis of Slopes","2022","Lecture Notes in Civil Engineering","171","","","997","1004","7","","10.1007/978-3-030-80312-4_86","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115243854&doi=10.1007%2f978-3-030-80312-4_86&partnerID=40&md5=f1b5d9061903f50173851991e33424d8","Slope failure is a complex problem that causes serious hazards throughout the world. Due to this there is large loss of life and property. So, it became very much essential to understand these slope failures, analyse and predict its vulnerability for proper mitigation of hazards. A number of methods are available for slope stability analysis and prediction. Most common among these is the convectional limit equilibrium and numerical methods. Now-a-days Artificial intelligence (AI) techniques are used widely for this purpose. This paper aims to provide a comparison of traditional methods of slope stability analysis i.e. limit equilibrium and finite element method with the recently developed artificial neural network (ANN). From the data available in the literature, a comparative study of factor of safety is carried out between the various methods through commercially available software Geostudio. The input parameters used in the study include- unit weight of soil (ϒ), cohesion (C), angle of internal friction (Ø), height of slope (H), angle of slope (α) and pore water ratio (ϒu), where as factor of safety (FOS) is the only output parameter. Also the relationship between various soil parameters and stability is established using regression analysis. The methods used in the LEM analysis are Ordinary method of slices, Bishop’s method, Morgenstein Price method, Janbu’s method, Spencer’s method and for FEM analysis strength reduction technique is used. It is found that there is good agreement between the conventional LEM and FEM. The factor of safety value obtained with FEM is a bit higher than that obtained with LEM. Also when comparing the results with ANN, it is found to give more accurate results. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","ANN; Factor of safety; FEM; LEM; Stability","Hazards; Neural networks; Numerical methods; Project management; Safety factor; Slope protection; Slope stability; Structural design; Angle of internal friction; Comparative studies; Limit equilibrium; Number of methods; Output parameters; Slope stability analysis; Stability analysis of slope; Strength reduction technique; Finite element method","Conference paper","Final","","Scopus","2-s2.0-85115243854"
"Garg A.; Degiovanni R.; Jimenez M.; Cordy M.; Papadakis M.; Le Traon Y.","Garg, Aayush (57225668650); Degiovanni, Renzo (42861129500); Jimenez, Matthieu (57191959513); Cordy, Maxime (55035833600); Papadakis, Mike (57197295611); Le Traon, Yves (55884641800)","57225668650; 42861129500; 57191959513; 55035833600; 57197295611; 55884641800","Learning from what we know: How to perform vulnerability prediction using noisy historical data","2022","Empirical Software Engineering","27","7","169","","","","","10.1007/s10664-022-10197-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138794753&doi=10.1007%2fs10664-022-10197-4&partnerID=40&md5=7436f35c0a2a295e6da9ad9f793c321e","Vulnerability prediction refers to the problem of identifying system components that are most likely to be vulnerable. Typically, this problem is tackled by training binary classifiers on historical data. Unfortunately, recent research has shown that such approaches underperform due to the following two reasons: a) the imbalanced nature of the problem, and b) the inherently noisy historical data, i.e., most vulnerabilities are discovered much later than they are introduced. This misleads classifiers as they learn to recognize actual vulnerable components as non-vulnerable. To tackle these issues, we propose TROVON, a technique that learns from known vulnerable components rather than from vulnerable and non-vulnerable components, as typically performed. We perform this by contrasting the known vulnerable, and their respective fixed components. This way, TROVON manages to learn from the things we know, i.e., vulnerabilities, hence reducing the effects of noisy and unbalanced data. We evaluate TROVON by comparing it with existing techniques on three security-critical open source systems, i.e., Linux Kernel, OpenSSL, and Wireshark, with historical vulnerabilities that have been reported in the National Vulnerability Database (NVD). Our evaluation demonstrates that the prediction capability of TROVON significantly outperforms existing vulnerability prediction techniques such as Software Metrics, Imports, Function Calls, Text Mining, Devign, LSTM, and LSTM-RF with an improvement of 40.84% in Matthews Correlation Coefficient (MCC) score under Clean Training Data Settings, and an improvement of 35.52% under Realistic Training Data Settings. © 2022, The Author(s).","Encoder-decoder; Machine translation; tf-seq2seq; Training on vulnerabilities only; Trovon; Vulnerability prediction","Classification (of information); Computer operating systems; Long short-term memory; Open source software; Open systems; Signal encoding; Data settings; Encoder-decoder; Historical data; Learn+; Machine translations; Tf-seq2seq; Training data; Training on vulnerability only; Trovon; Vulnerability prediction; Forecasting","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85138794753"
"Kumar R.; Geetha S.","Kumar, Rajesh (57221077675); Geetha, S. (24472945600)","57221077675; 24472945600","Effective Malware Detection using Shapely Boosting Algorithm","2022","International Journal of Advanced Computer Science and Applications","13","1","","101","111","10","","10.14569/IJACSA.2022.0130113","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124080618&doi=10.14569%2fIJACSA.2022.0130113&partnerID=40&md5=a35dca6537d8b58b92e5fecd39d332d3","Malware constitutes a prime exploitation tool to attack the vulnerabilities in software that lead to a threat to security. The number of malware gets generated as exploitation tools need effective methods to detect them. Machine learning methods are effective in detecting malware. The effectiveness of machine learning models can be increased by analyzing how the features that build the model contribute to the detection of malware. The model can be made robust by getting insight into how features contribute to each sample that is fed to a trained model. In this paper, the boosting machine learning model based on LightGBM is enhanced with Shapley value to detect the contribution of the top nine features for classification such as true positive or true negative and for misclassification such as false positive or false negative. This insight in the model can be used for effective and robust malware detection and to avoid wrong detections such as false positive and false negative. The comparison of the top features and their contribution in shapely value for each category of the sample gives insight and inductive learning into the model to know the reasons for misclassification. Inductive learning can be transformed into rules. The prediction by the trained model can be re-evaluated with such inductive learning and rules to ensure effective and robust prediction and avoid misclassification. The performance of models gives 98.48 at maximum and 97.45 at a minimum by 10 fold cross-validation © 2022,International Journal of Advanced Computer Science and Applications.All Rights Reserved","Artificial intelligence; Decision plot; Machine learning; Malware detection; Shapely value; Waterfall plot","Adaptive boosting; Feature extraction; Malware; Boosting algorithm; Decision plot; Exploitation tool; Machine learning methods; Machine learning models; Malware detection; Misclassifications; Model-based OPC; Shapely value; Waterfall plots; Machine learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85124080618"
"Oishwee S.J.; Codabux Z.; Stakhanova N.","Oishwee, Sahrima Jannat (57203066966); Codabux, Zadia (55893535500); Stakhanova, Natalia (21743507000)","57203066966; 55893535500; 21743507000","An exploratory study on the relationship of smells and design issues with software vulnerabilities","2022","MSR4P and S 2022 - Proceedings of the 1st International Workshop on Mining Software Repositories Applications for Privacy and Security, co-located with ESEC/FSE 2022","","","","16","20","4","","10.1145/3549035.3561182","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143129520&doi=10.1145%2f3549035.3561182&partnerID=40&md5=1788a77586625ba6676d4d45a694b27d","Software vulnerabilities are one of the leading causes of the loss of confidential data resulting in financial damages in the industry. As a result, software companies strive to discover potential vulnerabilities before the software is deployed. While traditionally, software metrics have been widely used to uncover vulnerabilities, more recent studies have been looking at code smells to detect vulnerabilities. This preliminary study explores the relationship between smells, design issues, and software vulnerabilities. As smells and design issues are indicators of potential problems in the software, establishing a relationship with vulnerabilities can be helpful for vulnerability prediction. In this study, we analyzed 561 versions of nine open-source software by exploring the smells and design issues in the vulnerable and non-vulnerable classes. We found that some smells and design issues have a statistically significant relationship with the vulnerable classes. However, after a manual analysis of the code segments containing the vulnerabilities, we found no indication that smells or design issues induce the vulnerabilities. In fact, they were still present in those code segments even after the vulnerabilities were resolved.  © 2022 ACM.","Code Smells; Design Issues; Mining Software Repositories; Software Security; Software Vulnerabilities","Odors; Open systems; Security of data; Code segments; Code smell; Confidential data; Design issues; Exploratory studies; Mining software; Mining software repository; Software repositories; Software security; Software vulnerabilities; Open source software","Conference paper","Final","","Scopus","2-s2.0-85143129520"
"Bin Nazim M.T.; Faruk M.J.H.; Shahriar H.; Khan M.A.; Masum M.; Sakib N.; Wu F.","Bin Nazim, Mohammad Taneem (57866395500); Faruk, Md Jobair Hossain (57274971500); Shahriar, Hossain (23096391900); Khan, Md Abdullah (56877965500); Masum, Mohammad (57215599253); Sakib, Nazmus (58650270000); Wu, Fan (57210292455)","57866395500; 57274971500; 23096391900; 56877965500; 57215599253; 58650270000; 57210292455","Systematic Analysis of Deep Learning Model for Vulnerable Code Detection","2022","Proceedings - 2022 IEEE 46th Annual Computers, Software, and Applications Conference, COMPSAC 2022","","","","1768","1773","5","","10.1109/COMPSAC54236.2022.00281","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134970902&doi=10.1109%2fCOMPSAC54236.2022.00281&partnerID=40&md5=8ff671ba606d6acfc5b5145f9eb992e7","Software vulnerabilities have become a serious problem with the emergence of new applications that contain potentially vulnerable or malicious code that can compromise the system. The growing volume and complexity of software source codes have opened a need for vulnerability detection methods to successfully predict malicious codes before being the prey of cyberattacks. As leveraging humans to check sources codes requires extensive time and resources and preexisting static code analyzers are unable to properly detect vulnerable codes. Thus, artificial intelligence techniques, mainly deep learning models, have gained traction to detect source code vulnerability. A systematic review is carried out to explore and understand the various deep learning methods employed for the task and their efficacy as a prediction model. Additionally, a summary of each process and its characteristics are examined and its implementation on specific data sets and their evaluation will be discussed. © 2022 IEEE.","Deep Learning; Software Security; Source code Vulnerability","Codes (symbols); Computer programming languages; Deep learning; Learning systems; Network security; Code detection; Deep learning; Learning models; Malicious codes; New applications; Software security; Software vulnerabilities; Source code vulnerability; Source codes; Systematic analysis; Application programs","Conference paper","Final","","Scopus","2-s2.0-85134970902"
"Awotunde J.B.; Misra S.; Adeniyi A.E.; Abiodun M.K.; Kaushik M.; Lawrence M.O.","Awotunde, Joseph Bamidele (57211158827); Misra, Sanjay (56962766700); Adeniyi, Abidemi Emmanuel (57205631460); Abiodun, Moses Kazeem (57220742693); Kaushik, Manju (56740421100); Lawrence, Morolake Oladayo (57844802300)","57211158827; 56962766700; 57205631460; 57220742693; 56740421100; 57844802300","A Feature Selection-Based K-NN Model for Fast Software Defect Prediction","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13380 LNCS","","","49","61","12","","10.1007/978-3-031-10542-5_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135932492&doi=10.1007%2f978-3-031-10542-5_4&partnerID=40&md5=4b0ba34d106709ce0a48a8f122d94d24","Software Defect Prediction (SDP) is an advanced technological method of predicting software defects in the software development life cycle. Various research works have been previously being done on SDP but the performance of these methods varied from several datasets, hence, making them inconsistent for SDP in the unknown software project. But the hybrid technique using feature selection enabled with machine learning for SDP can be very efficient as it takes the advantage of various methods to come up with better prediction accuracy for a given dataset when compared with an individual classifier. The major issues with individual ML-based models for SDP are the long detection time, vulnerability of the software project, and high dimensionality of the feature parameters. Therefore, this study proposes a hybrid model using a feature selection enabled Extreme Gradient Boost (XGB) classifier to address these mentioned challenges. The cleaned NASA MDP datasets were used for the implementation of the proposed model, and various performance metrics like F-score, accuracy, and MCC were used to reveal the performance of the model. The results of the proposed model when compared with state-of-the-art methods without feature selection perform better in terms of the metrics used. The results reveal that the proposed model outperformed all other prediction techniques. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Extreme gradient boost; Feature selection; Machine learning; Prediction; Software defect prediction; Software development life cycle","Classification (of information); Defects; Forecasting; Life cycle; NASA; Nearest neighbor search; Software design; Extreme gradient boost; Features selection; Machine-learning; Performance; Selection based; Software defect prediction; Software defects; Software development life-cycle; Software project; Technological methods; Feature Selection","Conference paper","Final","","Scopus","2-s2.0-85135932492"
"Yang K.; Miller P.; Martinez-Del-Rincon J.","Yang, Kaixi (58112437100); Miller, Paul (7404427902); Martinez-Del-Rincon, Jesus (8963448600)","58112437100; 7404427902; 8963448600","Convolutional Neural Network for Software Vulnerability Detection","2022","2022 Cyber Research Conference - Ireland, Cyber-RCI 2022","","","","","","","","10.1109/Cyber-RCI55324.2022.10032684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148631245&doi=10.1109%2fCyber-RCI55324.2022.10032684&partnerID=40&md5=4eb1ab7b7e2097674a3d0f384a333339","Exploitable vulnerabilities in software are one of the root causes of cybercrime, leading to financial losses, reputational damage, and wider security breaches for both enterprise and consumers. Furthermore, checking for vulnerabilities in software is no longer a human-scale problem due to code volume and complexity. To help address this problem, our work presents a deep learning model able to identify risk signals in Java source code and output a classification for a program as either vulnerable or safe. Sequences of raw Java opcodes are used to train a convolutional neural network that automatically encapsulates discriminative characteristics of a program that are then used for the prediction. Compared to traditional machine learning methods, this approach requires no prior knowledge of the software vulnerability domain, nor any hand-crafted input features. When evaluated on the publicly available benchmark dataset Juliet Test Suite containing 38520 vulnerable and 38806 safe programs, our method achieves an F1 score of 0.92. © 2022 IEEE.","Deep Learning; Software Vulnerability","Convolution; Convolutional neural networks; Cybersecurity; Deep learning; Java programming language; Learning systems; Network security; Software testing; Statistical tests; Convolutional neural network; Cyber-crimes; Deep learning; Financial loss; Learning models; Reputational damage; Root cause; Security breaches; Software vulnerabilities; Vulnerability detection; Losses","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85148631245"
"Saletta M.; Ferretti C.","Saletta, Martina (57211179736); Ferretti, Claudio (57191135612)","57211179736; 57191135612","Towards the evolutionary assessment of neural transformers trained on source code","2022","GECCO 2022 Companion - Proceedings of the 2022 Genetic and Evolutionary Computation Conference","","","","1770","1778","8","","10.1145/3520304.3534044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136324287&doi=10.1145%2f3520304.3534044&partnerID=40&md5=365ed4aff8b065e98b3d58cd24d6928e","In recent years, deep learning have become popular for solving tasks in a wide range of domains. With this growing diffusion, combined with architectures becoming increasingly complex and sophisticated, understanding how deep models make their predictions is now a crucial and challenging research issue. In this work, we are interested in analysing the behaviour of networks that deal with source code, and we address the problem along two experimental directions: in the first one, we study the activations of the neurons of a transformer trained in the detection of software vulnerabilities so as to identify if (and, eventually, where) some human understandable concepts emerge in the network. In the second one, we generate programs by applying a grammar-based evolutionary algorithm with a fitness function that favours individuals which stimulate (or weaken) the activations in neurons where given concepts majorly emerge. We then study the output of the network on sets of such evolved programs, i.e. how the evolutionary pressure along the direction of a concept affects the prediction. We finally discuss how this combination of evolutionary algorithms with fitness functions derived from the neural activations can be effective for explaining the decision process of deep models, and we suggest further research directions. © 2022 ACM.","deep neural networks; explainable AI; source code analysis; structured grammatical evolution","Chemical activation; Codes (symbols); Computer programming languages; Evolutionary algorithms; Network coding; Decision process; Explainable AI; Fitness functions; Grammatical evolution; Neural activation; Research issues; Software vulnerabilities; Source code analysis; Source codes; Structured grammatical evolution; Deep neural networks","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85136324287"
"Luo Y.; Xu W.; Xu D.","Luo, Yu (57437318900); Xu, Weifeng (35073269500); Xu, Dianxiang (7404073618)","57437318900; 35073269500; 7404073618","Compact Abstract Graphs for Detecting Code Vulnerability with GNN Models","2022","ACM International Conference Proceeding Series","","","","497","507","10","","10.1145/3564625.3564655","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144086642&doi=10.1145%2f3564625.3564655&partnerID=40&md5=a8df527c9c6a10e7f895ff4800eb4fc9","Source code representation is critical to the machine-learning-based approach to detecting code vulnerability. This paper proposes Compact Abstract Graphs (CAGs) of source code in different programming languages for predicting a broad range of code vulnerabilities with Graph Neural Network (GNN) models. CAGs make the source code representation aligned with the task of vulnerability classification and reduce the graph size to accelerate model training with minimum impact on the prediction performance. We have applied CAGs to six GNN models and large Java/C datasets with 114 vulnerability types in Java programs and 106 vulnerability types in C programs. The experiment results show that the GNN models have performed well, with accuracy ranging from 94.7% to 96.3% on the Java dataset and from 91.6% to 93.2% on the C dataset. The resultant GNN models have achieved promising performance when applied to more than 2,500 vulnerabilities collected from real-world software projects. The results also show that using CAGs for GNN models is significantly better than ASTs, CFGs (Control Flow Graphs), and PDGs (Program Dependence Graphs). A comparative study has demonstrated that the CAG-based GNN models can outperform the existing methods for machine learning-based vulnerability detection.  © 2022 ACM.","graph neural networks; machine learning; Software vulnerability; static code analysis","C (programming language); Computer software; Flow graphs; Graph neural networks; Java programming language; Large dataset; Machine learning; Neural network models; Graph neural networks; Graph sizes; Learning-based approach; Machine-learning; Neural network model; Software vulnerabilities; Source code representations; Source codes; Static code analysis; Vulnerability classifications; Graphic methods","Conference paper","Final","","Scopus","2-s2.0-85144086642"
"Chapman J.; Venugopalan H.","Chapman, Jon (57204814790); Venugopalan, Hari (57215081082)","57204814790; 57215081082","Open Source Software Computed Risk Framework","2022","International Scientific and Technical Conference on Computer Sciences and Information Technologies","2022-November","","","172","175","3","","10.1109/CSIT56902.2022.10000561","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146350175&doi=10.1109%2fCSIT56902.2022.10000561&partnerID=40&md5=4f4047f02a71e5e2b836776691ca3d47","The increased dissemination of open source software to a broader audience has led to a proportional increase in the dissemination of vulnerabilities. These vulnerabilities are introduced by developers, some intentionally or negligently. In this paper, we work to quantity the relative risk that a given developer represents to a software project. We propose using empirical software engineering based analysis on the vast data made available by GitHub to create a Developer Risk Score (DRS) for prolific contributors on GitHub. The DRS can then be aggregated across a project as a derived vulnerability assessment, we call this the Computational Vulnerability Assessment Score (CVAS). The CVAS represents the correlation between the Developer Risk score across projects and vulnerabilities attributed to those projects. We believe this to be a contribution in trying to quantity risk introduced by specific developers across open source projects. Both of the risk scores, those for contributors and projects, are derived from an amalgamation of data, both from GitHub and outside GitHub. We seek to provide this risk metric as a force multiplier for the project maintainers that are responsible for reviewing code contributions. We hope this will lead to a reduction in the number of introduced vulnerabilities for projects in the Open Source ecosystem.  © 2022 IEEE.","Big Data; Computer Security; Data Analysis; Prediction Methods","Big data; Open source software; Open systems; Risk assessment; Empirical Software Engineering; Open source projects; Open-source softwares; Prediction methods; Relative risks; Risk frameworks; Risk metric; Risk score; Software project; Vulnerability assessments; Security of data","Conference paper","Final","","Scopus","2-s2.0-85146350175"
"Hidalgo F.L.; Navarro M.; Molina S.","Hidalgo, Fernando Lopez (58097180300); Navarro, Manuel (7201561726); Molina, Sergio (55649570355)","58097180300; 7201561726; 55649570355","A new tool to simulate ground shaking and earthquake losses","2022","Revista Internacional de Metodos Numericos para Calculo y Diseno en Ingenieria","38","3","35","","","","","10.23967/J.RIMNI.2022.09.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147777319&doi=10.23967%2fJ.RIMNI.2022.09.007&partnerID=40&md5=f66cc26be06a8f93fff2a14c51835f13","The main purpose of this suite is Planning and Management of Seismic Emergencies before and after future damaging earthquake. This tool is written in ArcGIS software executing a fast and efficient determination of the estimated shakemaps and damage scenarios. The tool allows to select the earthquake source parameters through a defined database; moreover ground motion prediction equations can be chosen and they can be combined according to the study area features. The local site effects are characterized from Vs30 values, which have been achieved by topographic slope as a proxy (even with local correlations) obtained from digital elevation model. The elements exposed to risk are incorporated from the cadastral database after inputs has been refined through an automated analysis. Vulnerability and estimated losses can be determined either empirically (EMS-98 scale and Vulnerability Index, Iv) or analytically (Capacity spectrum). Additionally, a vulnerability modifier is implemented to account soil-structure resonance. Epistemic uncertainties are quantified in the input parameters using a logic tree. This tool has been validated through a representative seismic scenario: the 1910 Adra earthquake (southern Spain) with moment magnitude (Mw) 6.3 and macroseismic intensity VIII (EMS-98 scale) proving the reliability of this program. © 2022, Scipedia S.L.. All rights reserved.","Damage scenarios; Emergency planning; GIS; Shakemaps; Vulnerability","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85147777319"
"Liu S.; Lin G.; Qu L.; Zhang J.; De Vel O.; Montague P.; Xiang Y.","Liu, Shigang (56195519800); Lin, Guanjun (57195557433); Qu, Lizhen (57196124952); Zhang, Jun (57198771239); De Vel, Olivier (56429241700); Montague, Paul (7004572755); Xiang, Yang (57114147900)","56195519800; 57195557433; 57196124952; 57198771239; 56429241700; 7004572755; 57114147900","CD-VulD: Cross-Domain Vulnerability Discovery Based on Deep Domain Adaptation","2022","IEEE Transactions on Dependable and Secure Computing","19","1","","438","451","13","","10.1109/TDSC.2020.2984505","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123676423&doi=10.1109%2fTDSC.2020.2984505&partnerID=40&md5=c3faecb19e9bcb5db1b0f295734b4825","A major cause of security incidents such as cyber attacks is rooted in software vulnerabilities. These vulnerabilities should ideally be found and fixed before the code gets deployed. Machine learning-based approaches achieve state-of-the-art performance in capturing vulnerabilities. These methods are predominantly supervised. Their prediction models are trained on a set of ground truth data where the training data and test data are assumed to be drawn from the same probability distribution. However, in practice, the test data often differs from the training data in terms of distribution because they are from different projects or they differ in the types of vulnerability. In this article, we present a new system for Cross Domain Software Vulnerability Discovery (CD-VulD) using deep learning (DL) and domain adaptation (DA). We employ DL because it has the capacity of automatically constructing high-level abstract feature representations of programs, which are likely of more cross-domain useful than the handcrafted features driven by domain knowledge. The divergence between distributions is reduced by learning cross-domain representations. First, given software program representations, CD-VulD converts them into token sequences and learns the token embeddings for generalization across tokens. Next, CD-VulD employs a deep feature model to build abstract high-level presentations based on those sequences. Then, the metric transfer learning framework (MTLF) technique is employed to learn cross-domain representations by minimizing the distribution divergence between the source domain and the target domain. Finally, the cross-domain representations are used to build a classifier for vulnerability detection. Experimental results show that CD-VulD outperforms the state-of-the-art vulnerability detection approaches by a wide margin. We make the new datasets publicly available so that our work is replicable and can be further improved.  © 2004-2012 IEEE.","Cross-domain; deep learning; domain adaptation; machine learning; vulnerability detection/discovery","Abstracting; Deep learning; Domain Knowledge; Network security; Cross-domain; Deep learning; Domain adaptation; Domain representations; Software vulnerabilities; Test data; Training data; Vulnerability detection; Vulnerability detection/discovery; Vulnerability discovery; Probability distributions","Article","Final","","Scopus","2-s2.0-85123676423"
"Yin J.; Tang M.; Cao J.; Wang H.; You M.","Yin, Jiao (54884588500); Tang, MingJian (57215896761); Cao, Jinli (7403353999); Wang, Hua (57215111932); You, Mingshan (57215898907)","54884588500; 57215896761; 7403353999; 57215111932; 57215898907","A real-time dynamic concept adaptive learning algorithm for exploitability prediction","2022","Neurocomputing","472","","","252","265","13","","10.1016/j.neucom.2021.01.144","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119187113&doi=10.1016%2fj.neucom.2021.01.144&partnerID=40&md5=659aa16e4a183f0abfde72149761ec62","Exploitability prediction has become increasingly important in cybersecurity, as the number of disclosed software vulnerabilities and exploits are soaring. Recently, machine learning and deep learning algorithms, including Support Vector Machine (SVM), Decision Tree, deep Neural Networks and their ensemble models, have achieved great success in vulnerability evaluation and exploitability prediction. However, they make a strong assumption that the data distribution is static over time and therefore fail to consider the concept drift problems due to the evolving system behaviours. In this work, we propose a novel consecutive batch learning algorithm, called Real-time Dynamic Concept Adaptive Learning (RDCAL), to deal with the concept drift and dynamic class imbalance problems existing in exploitability prediction. Specifically, we develop a Class Rectification Strategy (CRS) to handle the ‘actual drift’ in sample labels and a Balanced Window Strategy (BWS) to boost the minority class during real-time learning. Experimental results conducted on the real-world vulnerabilities collected between 1988 to 2020 show that the overall performance of classifiers, including Neural Networks, SVM, HoeffdingTree and Logistic Regression (LR), improves over 3% by adopting our proposed RDCAL algorithm. Furthermore, RDCAL achieves state-of-the-art performance on exploitability prediction compared with other concept drift algorithms. © 2021 Elsevier B.V.","Balanced window; Class imbalance; Class rectification; Concept drift; Real-time learning","Decision trees; Deep neural networks; Learning algorithms; Logistic regression; Support vector regression; Adaptive learning; Adaptive learning algorithm; Balanced windows; Class imbalance; Class rectification; Concept drifts; Dynamic concepts; Real-time dynamics; Real-time learning; Support vectors machine; article; artificial neural network; learning algorithm; prediction; support vector machine; Forecasting","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85119187113"
"Khokhlov I.; Okutan A.; Bryla R.; Simmons S.; Mirakhorli M.","Khokhlov, Igor (57198777658); Okutan, Ahmet (55322858400); Bryla, Ryan (57211074807); Simmons, Steven (57306256600); Mirakhorli, Mehdi (23390242500)","57198777658; 55322858400; 57211074807; 57306256600; 23390242500","Automated Extraction of Software Names from Vulnerability Reports using LSTM and Expert System","2022","Proceedings - 2022 IEEE 29th Annual Software Technology Conference, STC 2022","","","","125","134","9","","10.1109/STC55697.2022.00024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143408212&doi=10.1109%2fSTC55697.2022.00024&partnerID=40&md5=0e484c35cb1fa6f092336211d8e7df93","Software vulnerabilities are closely monitored by the security community to timely address the security and privacy issues in software systems. Before a vulnerability is published by vulnerability management systems, it needs to be characterized to highlight its unique attributes, including affected software products and versions, to help security professionals prioritize their patches. Associating product names and versions with disclosed vulnerabilities may require a labor-intensive process that may delay their publication and fix, and thereby give attackers more time to exploit them. This work proposes a machine learning method to extract software product names and versions from unstructured CVE descriptions automatically. It uses Word2Vec and Char2Vec models to create context-aware features from CVE descriptions and uses these features to train a Named Entity Recognition (NER) model using bidirectional Long short-term memory (LSTM) networks. Based on the attributes of the product names and versions in previously published CVE descriptions, we created a set of Expert System (ES) rules to refine the predictions of the NER model and improve the performance of the developed method. Experiment results on real-life CVE examples indicate that using the trained NER model and the set of ES rules, software names and versions in unstructured CVE descriptions could be identified with F-Measure values above 0.95.  © 2022 IEEE.","Common Product Enumeration; Common Vulnerability and Exposures; Natural Language Processing; Software Product Name Extraction; Software Vulnerability","Expert systems; Learning algorithms; Learning systems; Long short-term memory; Natural language processing systems; Common product enumeration; Common vulnerabilities and exposures; Language processing; Named entity recognition; Natural language processing; Natural languages; Product name; Software product name extraction; Software products; Software vulnerabilities; Extraction","Conference paper","Final","","Scopus","2-s2.0-85143408212"
"Movahedi Y.; Cukier M.; Gashi I.","Movahedi, Yazdan (56582473200); Cukier, Michel (6603803353); Gashi, Ilir (6505943089)","56582473200; 6603803353; 6505943089","Predicting the Discovery Pattern of Publically Known Exploited Vulnerabilities","2022","IEEE Transactions on Dependable and Secure Computing","19","2","","1181","1193","12","","10.1109/TDSC.2020.3014872","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099558412&doi=10.1109%2fTDSC.2020.3014872&partnerID=40&md5=6265b7b73cc0b5340ce1d9a2ca17e3dc","Vulnerabilities with publically known exploits typically form 2-7 percent of all vulnerabilities reported for a given software version. With a smaller number of known exploited vulnerabilities compared with the total number of vulnerabilities, it is more difficult to model and predict when a vulnerability with a known exploit will be reported. In this article, we introduce an approach for predicting the discovery pattern of publically known exploited vulnerabilities using all publically known vulnerabilities reported for a given software. Eight commonly used vulnerability discovery models (VDMs) and one neural network model (NNM) were utilized to evaluate the prediction capability of our approach. We compared their predictions results with the scenario when only exploited vulnerabilities were used for prediction. Our results show that, in terms of prediction accuracy, out of eight software we analyzed, our approach led to more accurate results in seven cases. Only in one case, the accuracy of our approach was worse by 1.6 percent. © 2004-2012 IEEE.","all vulnerabilities; artificial neural network; exploited vulnerabilities; Prediction; time to next vulnerability; vulnerability discovery models","Network security; Neural network model; Prediction accuracy; Prediction capability; Software versions; Vulnerability discovery; Forecasting","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85099558412"
"Kumar R.; Subbiah G.","Kumar, Rajesh (57221077675); Subbiah, Geetha (57215956627)","57221077675; 57215956627","Zero-Day Malware Detection and Effective Malware Analysis Using Shapley Ensemble Boosting and Bagging Approach","2022","Sensors","22","7","2798","","","","","10.3390/s22072798","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127616126&doi=10.3390%2fs22072798&partnerID=40&md5=a839564d333d2f131ad3beede1f4b73c","Software products from all vendors have vulnerabilities that can cause a security concern. Malware is used as a prime exploitation tool to exploit these vulnerabilities. Machine learning (ML) methods are efficient in detecting malware and are state-of-art. The effectiveness of ML models can be augmented by reducing false negatives and false positives. In this paper, the performance of bagging and boosting machine learning models is enhanced by reducing misclassification. Shapley values of features are a true representation of the amount of contribution of features and help detect top features for any prediction by the ML model. Shapley values are transformed to probability scale to correlate with a prediction value of ML model and to detect top features for any prediction by a trained ML model. The trend of top features derived from false negative and false positive predictions by a trained ML model can be used for making inductive rules. In this work, the best performing ML model in bagging and boosting is determined by the accuracy and confusion matrix on three malware datasets from three different periods. The best performing ML model is used to make effective inductive rules using waterfall plots based on the probability scale of features. This work helps improve cyber security scenarios by effective detection of false-negative zero-day malware. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","artificial intelligence; bagging; boosting; computer security; cyber se-curity; machine learning; Shapley value; zero-day malware detection; zero-day vulnerability","Algorithms; Computer Security; Data Collection; Machine Learning; Software; Adaptive boosting; Cybersecurity; Feature extraction; Machine learning; Malware; Zero-day attack; Bagging; Boosting; Computer security; Cybe se-curity; False negatives; Machine learning models; Malware detection; Shapley value; Zero day vulnerabilities; Zero-day malware detection; algorithm; computer security; information processing; machine learning; software; Forecasting","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85127616126"
"Naveen; Sharma U.","Naveen (55416092300); Sharma, Uttam (57221964288)","55416092300; 57221964288","Reduction of IoT Security Vulnerabilities Using Machine Learning Algorithm","2022","Lecture Notes in Electrical Engineering","915","","","677","687","10","","10.1007/978-981-19-2828-4_60","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138784852&doi=10.1007%2f978-981-19-2828-4_60&partnerID=40&md5=a71c96527e55f14a2b5e6674d33e5383","The function of IoT-enabled spruce devices has increased to a big extent, it is estimated that in the last ten years, the use of spruce devices reached 20–21 billion, and this number will increase drastically in this decade. The use of smart devices makes human life easier as we are moving towards a connected world but we cannot shut our eyes towards the loopholes and vulnerabilities of IoT device security and network threats. The paper is lightening up some well-known and newly discovered security threats and we have provided a collective package of machine learning algorithms that possibly reduce the vulnerability to some extent. The concept which will work behind the proposed package of an algorithm is data which is being collected from several IoT-enabled device and main features which will play a significant role in the prediction about severity of attack and vulnerability are the type of attack, i.e. software, network, physical attack or encryption attack, the effect of the attack, time taken in the discovery of attack and the device or frame work or platform which is more vulnerable to attacks. The collected data need to be pre-processed using pre-process steps and stage, and then the algorithm implementation will give the prediction and using the predictions or we can say the set of rules the vulnerability can be detected and reduced. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Machine learning algorithms; Security threats; Vulnerability","Cryptography; Internet of things; Learning algorithms; Machine learning; Network security; Security systems; % reductions; Human lives; Machine learning algorithms; Network threats; Physical attacks; Security threats; Security vulnerabilities; Smart devices; Software network; Vulnerability; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85138784852"
"Kontoleon K.J.; Georgiadis-Filikas K.; Tsikaloudaki K.G.; Theodosiou T.G.; Giarma C.S.; Papanicolaou C.G.; Triantafillou T.C.; Asimakopoulou E.K.","Kontoleon, K.J. (8961043800); Georgiadis-Filikas, K. (55899415100); Tsikaloudaki, K.G. (6506206279); Theodosiou, T.G. (24463025600); Giarma, C.S. (36699471500); Papanicolaou, C.G. (8728408100); Triantafillou, T.C. (7004178697); Asimakopoulou, E.K. (55901143200)","8961043800; 55899415100; 6506206279; 24463025600; 36699471500; 8728408100; 7004178697; 55901143200","Vulnerability assessment of an innovative precast concrete sandwich panel subjected to the ISO 834 fire","2022","Journal of Building Engineering","52","","104479","","","","","10.1016/j.jobe.2022.104479","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128261303&doi=10.1016%2fj.jobe.2022.104479&partnerID=40&md5=0b1d0a49d2b0a96ec97a9358f25e469e","Development and use of preconstruction have been exhibited for several decades. Numerous modules, ranging from the simplest to the most advanced concepts, have been suggested to ameliorate the layout of building structures, with respect to a broad spectrum of needs. This study aims to unfold the fire defensiveness of an innovative precast concrete sandwich wall-system subjected to the ISO 834 fire, such as this is provided for in EN1991-1-2. In light of a rapidly evolving environment that should shield structures against fire, this investigation emphasises on the vulnerability of precast panels with a varying thickness of insulation by means of a numerical methodology and a versatile heat transfer-model. A finite-element analysis is carried out with COMSOL Multiphysics® simulation software. In a following step, as fire risk should be vigorously tackled, the research is extended to validate numerical predictions of the model by means of an experimental setup for wall specimens arranged in the laboratory. Therefore, an additional goal of this research is to assess temperature discrepancies for both addressed cases. Despite various approximations of the model, an excellent agreement between numerical and experimental results is shown, confirming the rationality of computational simulations in terms of temperatures’ precision. It has been revealed that for all examined cases, the insulation ability (I) has been maintained for more than 3 h regardless of the positioning of the insulation. Further evidence though suggested that is not the case for the loadbearing capacity (R), as the installation of a fire exposed insulation layer resulted in lower stability systems. Also, the effect of the insulation thickness is not that dominant as on average and maximum temperature deviations among marginal assemblies (dEPS = 2 cm and dEPS = 10 cm) did not exceed 5 °C and 10 °C at tfire ≈ 100 min. © 2022 Elsevier Ltd","Finite element analysis (FEA); Fire resistance; Heat transfer; High-temperature properties; Layered structures","Computer software; Fire resistance; Heat resistance; Heat transfer; Numerical models; Precast concrete; Sandwich structures; Thermal insulation; Walls (structural partitions); Broad spectrum; Building structure; Finite element analyse; High-temperature properties; Layered Structures; Pre-construction; Precast concrete sandwich panels; Simple++; Vulnerability assessments; Finite element method","Article","Final","","Scopus","2-s2.0-85128261303"
"Gurcan F.; Dalveren G.G.M.; Cagiltay N.E.; Roman D.; Soylu A.","Gurcan, Fatih (57194776706); Dalveren, Gonca Gokce Menekse (57201658878); Cagiltay, Nergiz Ercil (16237826800); Roman, Dumitru (23467687800); Soylu, Ahmet (35243744400)","57194776706; 57201658878; 16237826800; 23467687800; 35243744400","Evolution of Software Testing Strategies and Trends: Semantic Content Analysis of Software Research Corpus of the Last 40 Years","2022","IEEE Access","10","","","106093","106109","16","","10.1109/ACCESS.2022.3211949","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140200883&doi=10.1109%2fACCESS.2022.3211949&partnerID=40&md5=e35bbc7373bb12142c3e6ab23eca20e5","From the early days of computer systems to the present, software testing has been considered as a crucial process that directly affects the quality and reliability of software-oriented products and services. Accordingly, there is a huge amount of literature regarding the improvement of software testing approaches. However, there are limited reviews that show the whole picture of the software testing studies covering the topics and trends of the field. This study aims to provide a general figure reflecting topics and trends of software testing by analyzing the majority of software testing articles published in the last 40 years. A semi-automated methodology is developed for the analysis of software testing corpus created from core publication sources. The methodology of the study is based on the implementation of probabilistic topic modeling approach to discover hidden semantic patterns in the 14,684 published articles addressing software testing issues between 1980 and 2019. The results revealed 42 topics of the field, highlighting five software development ages, namely specification, detection, generation, evaluation, and prediction. The recent accelerations of the topics also showed a trend toward prediction-based software testing actions. Additionally, a higher trend on the topics concerning 'Security Vulnerability', 'Open Source' and 'Mobile Application' was identified. This study showed that the current trend of software testing is towards prediction-based testing strategies. Therefore, the findings of this study may provide valuable insights for the industry and software communities to be prepared for the possible changes in the software testing procedures using prediction-based approaches.  © 2013 IEEE.","Software testing; test strategies; topic modeling; trend analysis","Computer software selection and evaluation; Forecasting; Open source software; Quality control; Reliability analysis; Software design; Software reliability; Software testing; Content analysis; Market researches; Prediction-based; Software testing strategies; Software testings; Systematic; Test strategies; Topic Modeling; Trend analysis; Semantics","Article","Final","","Scopus","2-s2.0-85140200883"
"Fu M.; Tantithamthavorn C.; Le T.; Nguyen V.; Phung D.","Fu, Michael (57685110800); Tantithamthavorn, Chakkrit (55361007600); Le, Trung (57202557822); Nguyen, Van (57202983154); Phung, Dinh (7003397144)","57685110800; 55361007600; 57202557822; 57202983154; 7003397144","VulRepair: a T5-based automated software vulnerability repair","2022","ESEC/FSE 2022 - Proceedings of the 30th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering","","","","935","947","12","","10.1145/3540250.3549098","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143050564&doi=10.1145%2f3540250.3549098&partnerID=40&md5=52d0566a4ac2b2921c0e18880c382dcd","As software vulnerabilities grow in volume and complexity, researchers proposed various Artificial Intelligence (AI)-based approaches to help under-resourced security analysts to find, detect, and localize vulnerabilities. However, security analysts still have to spend a huge amount of effort to manually fix or repair such vulnerable functions. Recent work proposed an NMT-based Automated Vulnerability Repair, but it is still far from perfect due to various limitations. In this paper, we propose VulRepair, a T5-based automated software vulnerability repair approach that leverages the pre-training and BPE components to address various technical limitations of prior work. Through an extensive experiment with over 8,482 vulnerability fixes from 1,754 real-world software projects, we find that our VulRepair achieves a Perfect Prediction of 44%, which is 13%-21% more accurate than competitive baseline approaches. These results lead us to conclude that our VulRepair is considerably more accurate than two baseline approaches, highlighting the substantial advancement of NMT-based Automated Vulnerability Repairs. Our additional investigation also shows that our VulRepair can accurately repair as many as 745 out of 1,706 real-world well-known vulnerabilities (e.g., Use After Free, Improper Input Validation, OS Command Injection), demonstrating the practicality and significance of our VulRepair for generating vulnerability repairs, helping under-resourced security analysts on fixing vulnerabilities.  © 2022 ACM.","Software Vulnerability Repair","Repair; Command injections; Input validation; Pre-training; Real-world; Securities analysts; Software project; Software vulnerabilities; Software vulnerability repair; Technical limitations; Under-resourced; Automation","Conference paper","Final","","Scopus","2-s2.0-85143050564"
"He H.; Ji Y.; Huang H.H.","He, Haoyu (57797371000); Ji, Yuede (56017430500); Huang, H. Howie (51964011300)","57797371000; 56017430500; 51964011300","Illuminati: Towards Explaining Graph Neural Networks for Cybersecurity Analysis","2022","Proceedings - 7th IEEE European Symposium on Security and Privacy, Euro S and P 2022","","","","74","89","15","","10.1109/EuroSP53844.2022.00013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134071476&doi=10.1109%2fEuroSP53844.2022.00013&partnerID=40&md5=0ebe7c7ff969b66268c60e1752ab914c","Graph neural networks (GNNs) have been utilized to create multi-layer graph models for a number of cybersecurity applications from fraud detection to software vulnerability analysis. Unfortunately, like traditional neural networks, GNNs also suffer from a lack of transparency, that is, it is challenging to interpret the model predictions. Prior works focused on specific factor explanations for a GNN model. In this work, we have designed and implemented Illuminati, a comprehensive and accurate explanation framework for cybersecurity applications using GNN models. Given a graph and a pre-trained GNN model, Illuminati is able to identify the important nodes, edges, and attributes that are contributing to the prediction while requiring no prior knowledge of GNN models. We evaluate Illuminati in two cybersecurity applications, i.e., code vulnerability detection and smart contract vulnerability detection. The experiments show that Illuminati achieves more accurate explanation results than state-of-the-art methods, specifically, 87.6% of subgraphs identified by Illuminati are able to retain their original prediction, an improvement of 10.3% over others at 77.3%. Furthermore, the explanation of Illuminati can be easily understood by the domain experts, suggesting the significant usefulness for the development of cybersecurity applications. © 2022 IEEE.","","Application programs; Forecasting; Graph theory; Multilayer neural networks; Neural network models; Cyber security; Fraud detection; Graph model; Graph neural networks; Multi-layer graphs; Neural network model; Neural-networks; Software vulnerabilities; Vulnerability analysis; Vulnerability detection; Graph neural networks","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85134071476"
"Walter T.","Walter, Tobias (57747028100)","57747028100","Architectural Pen-Test Generation and Vulnerability Prediction for Cyber-Physical Systems","2022","2022 IEEE 19th International Conference on Software Architecture Companion, ICSA-C 2022","","","","45","46","1","","10.1109/ICSA-C54293.2022.00016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132165613&doi=10.1109%2fICSA-C54293.2022.00016&partnerID=40&md5=6b3259659ef36c8c8cce10564b9318e8","The security of cyber-physical systems (CPSs) gains in significance as they become more open and rely on communication. Penetration testing is a technique to assess the security of such systems. However, existing techniques depend on the experience and domain knowledge of the penetration tester. This paper outlines ideas for supporting penetration testers to reduce the influence of the experience and knowledge on the results of a pen test. Our goal is to generate attack paths and test cases based on a software architecture during design phase. The attack paths and test cases are used to support pen testers in conducting tests on a system under test (SUT) once the system is completely developed. Furthermore, we intend to use the generated test cases to analyze changes made by the software architect during the design phase. Our goal here is to provide feedback in terms of a vulnerability prediction. The prediction supports the software architect in making design decisions towards more secure software architectures.  © 2022 IEEE.","architectural model; cyber-physical systems; penetration testing; vulnerability prediction","Domain Knowledge; Embedded systems; Forecasting; Software architecture; Software design; Software testing; Testing; Architectural modeling; Attack path; Cybe-physical systems; Cyber-physical systems; Design phase; Penetration testing; Software architects; Test case; Test generations; Vulnerability prediction; Cyber Physical System","Conference paper","Final","","Scopus","2-s2.0-85132165613"
"Cheng X.; Zhang G.; Wang H.; Sui Y.","Cheng, Xiao (57211627500); Zhang, Guanqin (57202953646); Wang, Haoyu (55808022700); Sui, Yulei (54788439800)","57211627500; 57202953646; 55808022700; 54788439800","Path-sensitive code embedding via contrastive learning for software vulnerability detection","2022","ISSTA 2022 - Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis","","","","519","531","12","","10.1145/3533767.3534371","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136799873&doi=10.1145%2f3533767.3534371&partnerID=40&md5=e74c09d0c019b5dec610654bd695b380","Machine learning and its promising branch deep learning have shown success in a wide range of application domains. Recently, much effort has been expended on applying deep learning techniques (e.g., graph neural networks) to static vulnerability detection as an alternative to conventional bug detection methods. To obtain the structural information of code, current learning approaches typically abstract a program in the form of graphs (e.g., data-flow graphs, abstract syntax trees), and then train an underlying classification model based on the (sub)graphs of safe and vulnerable code fragments for vulnerability prediction. However, these models are still insufficient for precise bug detection, because the objective of these models is to produce classification results rather than comprehending the semantics of vulnerabilities, e.g., pinpoint bug triggering paths, which are essential for static bug detection. This paper presents ContraFlow, a selective yet precise contrastive value-flow embedding approach to statically detect software vulnerabilities. The novelty of ContraFlow lies in selecting and preserving feasible value-flow (aka program dependence) paths through a pretrained path embedding model using self-supervised contrastive learning, thus significantly reducing the amount of labeled data required for training expensive downstream models for path-based vulnerability detection. We evaluated ContraFlow using 288 real-world projects by comparing eight recent learning-based approaches. ContraFlow outperforms these eight baselines by up to 334.1%, 317.9%, 58.3% for informedness, markedness and F1 Score, and achieves up to 450.0%, 192.3%, 450.0% improvement for mean statement recall, mean statement precision and mean IoU respectively in terms of locating buggy statements.  © 2022 ACM.","code embedding; contrastive learning; Path sensitive; vulnerabilities","Classification (of information); Data flow analysis; Data flow graphs; Deep learning; Graph neural networks; Graphic methods; Semantics; Trees (mathematics); Bug detection; Code embedding; Contraflow; Contrastive learning; Embeddings; Path sensitives; Software vulnerabilities; Value flow; Vulnerability; Vulnerability detection; Embeddings","Conference paper","Final","","Scopus","2-s2.0-85136799873"
"Wang Y.; Hou X.; Ma X.; Lv Q.","Wang, Yan (56368700100); Hou, Xiaowei (57456904700); Ma, Xiu (57943803300); Lv, Qiujian (56765312900)","56368700100; 57456904700; 57943803300; 56765312900","A Software Security Entity Relationships Prediction Framework Based on Knowledge Graph Embedding Using Sentence-Bert","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13472 LNCS","","","501","513","12","","10.1007/978-3-031-19214-2_42","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142885163&doi=10.1007%2f978-3-031-19214-2_42&partnerID=40&md5=160cc1644ab2df7bfcb74b420897ea6a","Recently, the need for complex cyber attack knowledge is increasing with the rising risk of software vulnerabilities and weaknesses on the internet. To spread knowledge and strengthen software security defense, researchers record software vulnerabilities, weaknesses, and attack patterns through software databases, including CVE, CWE, and CAPEC, etc. However, software security databases are time delayed and thus miss unobserved facts. Attackers can take advantage of this problem to execute an attack successfully. Therefore, the reasoning task of predicting software security entity relation is critical to supplementing software security data. This paper constructs a software security knowledge graph and proposes a knowledge graph representation learning method combining Sentence-Bert and GAT. The way can implement link prediction and classification tasks for knowledge graph completion. We finally designed a large number of experiments to evaluate the effectiveness of our model in knowledge graph completion and knowledge graph classification. The experimental results demonstrate that the proposed method can effectively improve the effectiveness of prediction. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Entity relation prediction; Graph attention network; Knowledge graph embedding; Sentence-bert; Software security entity","Cybersecurity; Forecasting; Graph embeddings; Network security; Entity relation prediction; Entity-relationship; Graph attention network; Graph embeddings; Knowledge graph embedding; Knowledge graphs; Sentence-bert; Software security; Software security entity; Software vulnerabilities; Knowledge graph","Conference paper","Final","","Scopus","2-s2.0-85142885163"
"Binkley D.; Moonen L.; Isaacman S.","Binkley, David (7101905317); Moonen, Leon (7003285889); Isaacman, Sibren (26028863900)","7101905317; 7003285889; 26028863900","Featherweight assisted vulnerability discovery","2022","Information and Software Technology","146","","106844","","","","","10.1016/j.infsof.2022.106844","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124207373&doi=10.1016%2fj.infsof.2022.106844&partnerID=40&md5=7f3c3d974517aaf79f6e7e6aa4b2754b","Predicting vulnerable source code helps to focus the attention of a developer, or a program analysis technique, on those parts of the code that need to be examined with more scrutiny. Recent work proposed the use of function names as semantic cues that can be learned by a deep neural network (DNN) to aid in the hunt for vulnerability of functions. Combining identifier splitting, which we use to split each function name into its constituent words, with a novel frequency-based algorithm, we explore the extent to which the words that make up a function's name can be used to predict potentially vulnerable functions. In contrast to the lightweight prediction provided by a DNN considering only function names, avoiding the need for a DNN provides featherweight prediction. The underlying idea is that function names that contain certain “dangerous” words are more likely to accompany vulnerable functions. Of course, this assumes that the frequency-based algorithm can be properly tuned to focus on truly dangerous words. Because it is more transparent than a DNN, which behaves as a “black box” and thus provides no insight into the rationalization underlying its decisions, the frequency-based algorithm enables us to investigate the inner workings of the DNN. If successful, this investigation into what the DNN does and does not learn will help us train more effective future models. We empirically evaluate our approach on a heterogeneous dataset containing over 73000 functions labeled vulnerable, and over 950000 functions labeled benign. Our analysis shows that words alone account for a significant portion of the DNN's classification ability. We also find that words are of greatest value in the datasets with a more homogeneous vocabulary. Thus, when working within the scope of a given project, where the vocabulary is unavoidably homogeneous, our approach provides a cheaper, potentially complementary, technique to aid in the hunt for source-code vulnerabilities. Finally, this approach has the advantage that it is viable with orders of magnitude less training data. © 2022 The Authors","Identifier splitting; Model interpretability; Software security; Source code vocabulary; Vulnerability prediction","Computer programming languages; Deep neural networks; Network security; Semantics; Identifier splitting; Interpretability; Model interpretability; Program analysis; Software security; Source code vocabulary; Source codes; Splittings; Vulnerability discovery; Vulnerability prediction; Forecasting","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85124207373"
"Katsadouros E.; Patrikakis C.","Katsadouros, Evangelos (57194855535); Patrikakis, Charalampos (8244299800)","57194855535; 8244299800","A Survey on Vulnerability Prediction using GNNs","2022","ACM International Conference Proceeding Series","","","","38","43","5","","10.1145/3575879.3575964","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152132829&doi=10.1145%2f3575879.3575964&partnerID=40&md5=a4db4b5add1acd9a61e87b0a5144d7c1","The massive release of software products has led to critical incidents in the software industry due to low-quality software. Software engineers lack security knowledge which causes the development of insecure software. Traditional solutions for analysing code for vulnerabilities suffer from high false positives and negative rates. Researchers over the last decade have proposed mechanisms for analysing code for vulnerabilities using machine learning. The results are promising and could replace traditional static analysis tools or accompany them in the foreseeable future to produce more reliable results. This survey presents the work done so far in vulnerability detection using Graph Neural Networks (GNNs). Presents the GNNs architectures, the graph representations, the datasets, and the results of these studies. © 2022 ACM.","Privacy; Software; Software Quality","Computer software selection and evaluation; Graph neural networks; Critical incidents; False negative rate; Graph neural networks; Low qualities; Privacy; Quality software; Software; Software industry; Software products; Software Quality; Static analysis","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85152132829"
"Al Debeyan F.; Hall T.; Bowes D.","Al Debeyan, Fahad (57990643700); Hall, Tracy (56220907900); Bowes, David (25929085600)","57990643700; 56220907900; 25929085600","Improving the performance of code vulnerability prediction using abstract syntax tree information","2022","PROMISE 2022 - Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering, co-located with ESEC/FSE 2022","","","","2","11","9","","10.1145/3558489.3559066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143195753&doi=10.1145%2f3558489.3559066&partnerID=40&md5=054c462046df4d643b7e42f1c2d5b910","The recent emergence of the Log4jshell vulnerability demonstrates the importance of detecting code vulnerabilities in software systems. Software Vulnerability Prediction Models (VPMs) are a promising tool for vulnerability detection. Recent studies have focused on improving the performance of models to predict whether a piece of code is vulnerable or not (binary classification). However, such approaches are limited because they do not provide developers with information on the type of vulnerability that needs to be patched. We present our multiclass classification approach to improve the performance of vulnerability prediction models. Our approach uses abstract syntax tree n-grams to identify code clusters related to specific vulnerabilities. We evaluated our approach using real-world Java software vulnerability data. We report increased predictive performance compared to a variety of other models, for example, F-measure increases from 55% to 75% and MCC increases from 48% to 74%. Our results suggest that clustering software vulnerabilities using AST n-gram information is a promising approach to improve vulnerability prediction and enable specific information about the vulnerability type to be provided.  © 2022 ACM.","Machine learning; Software Security; Software Vulnerability","Classification (of information); Computational linguistics; Computer software; Information use; Machine learning; Security of data; Syntactics; Trees (mathematics); Abstract Syntax Trees; Binary classification; Machine-learning; N-grams; Performance; Prediction modelling; Software security; Software vulnerabilities; Software-systems; Vulnerability detection; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85143195753"
"Wu Z.; Cui C.; Leng Q.; Xu M.; Su J.","Wu, Zhihao (57397400900); Cui, Chunyi (7201920358); Leng, Qicheng (58098443400); Xu, Minze (57217082703); Su, Jian (57219991216)","57397400900; 7201920358; 58098443400; 57217082703; 57219991216","Seismic vulnerability analysis of vertical pile-supported wharf structure; [全直桩高桩码头结构的地震易损性分析]","2022","Shenzhen Daxue Xuebao (Ligong Ban)/Journal of Shenzhen University Science and Engineering","39","4","","432","439","7","","10.3724/SP.J.1249.2022.04432","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147803192&doi=10.3724%2fSP.J.1249.2022.04432&partnerID=40&md5=2526747b70d8452c1fd8b164430ebaf7","In order to study the seismic vulnerability of pile-supported wharf structure, taking a vertical pile-supported wharf as the research object and considering the influence of the site, seismic spectrum characteristics and the uncertainty of ground motion intensity, we establish a dynamic nonlinear numerical model of structure-foundation soil coupling of vertical pile-supported wharf by using the geotechnical finite element software of Midas GTS NX. Selecting the maximum strain of pile foundation in soil as the damage index, we obtain the seismic vulnerability curves and the corresponding damage state probability based on the the incremental dynamic analysis method. The analysis results show that when peak ground acceleration is less than 0.80g, the wharf structure is mainly in mild damage state or moderate damage state, otherwise the probability of serious damage basically exceeds 50% and the structure will lose operational capacity. Based on the vulnerability analysis of pile foundation damage in foundation soil, the influence of earthquake on the vertical pile-supported wharf is described from macroscopic and quantitative point of view, which can provide reference for seismic design and disaster prevention prediction of vertical pile-supported wharf. © 2022 Editorial Office of Journal of Shenzhen University. All rights reserved.","exceedance probability; incremental dynamic analysis; ocean engineering and technology; pile foundation damage; seismic vulnerability; vertical pile-supported wharf","","Article","Final","","Scopus","2-s2.0-85147803192"
"Yang Z.; Shi J.; He J.; Lo D.","Yang, Zhou (57222189359); Shi, Jieke (57225119268); He, Junda (57432691900); Lo, David (35269388000)","57222189359; 57225119268; 57432691900; 35269388000","Natural Attack for Pre-trained Models of Code","2022","Proceedings - International Conference on Software Engineering","2022-May","","","1482","1493","11","","10.1145/3510003.3510146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132990247&doi=10.1145%2f3510003.3510146&partnerID=40&md5=895fbdab6c929f8c3ef62ca5860565ff","Pre-trained models of code have achieved success in many important software engineering tasks. However, these powerful models are vulnerable to adversarial attacks that slightly perturb model inputs to make a victim model produce wrong outputs. Current works mainly attack models of code with examples that preserve operational program semantics but ignore a fundamental requirement for adversarial example generation: perturbations should be natural to human judges, which we refer to as naturalness requirement. In this paper, we propose ALERT (Naturalness Aware Attack), a black-box attack that adversarially transforms inputs to make victim models produce wrong outputs. Different from prior works, this paper considers the natural semantic of generated examples at the same time as preserving the operational semantic of original inputs. Our user study demonstrates that human developers consistently consider that adversarial examples generated by ALERT are more natural than those generated by the state-of-the-art work by Zhang et al. that ignores the naturalness requirement. On attacking CodeBERT, our approach can achieve attack success rates of 53.62%, 27.79%, and 35.78% across three downstream tasks: vulnerability prediction, clone detection and code authorship attribution. On GraphCodeBERT, our approach can achieve average success rates of 76.95%, 7.96% and 61.47% on the three tasks. The above outperforms the baseline by 14.07% and 18.56% on the two pretrained models on average. Finally, we investigated the value of the generated adversarial examples to harden victim models through an adversarial fine-tuning procedure and demonstrated the accuracy of CodeBERT and GraphCodeBERT against ALERT-generated adversarial examples increased by 87.59% and 92.32%, respectively. © 2022 ACM.","Adversarial Attack; Genetic Algorithm; Pre-Trained Models","Codes (symbols); Semantics; Software engineering; 'current; Adversarial attack; Attack modeling; Black boxes; Engineering tasks; Model inputs; Natural semantics; Operational programmes; Pre-trained model; Program semantics; Genetic algorithms","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85132990247"
"Duan B.; Zhou X.; Wu X.","Duan, Bingjie (57867027700); Zhou, Xu (56373491500); Wu, Xugang (57202256517)","57867027700; 56373491500; 57202256517","Improve Vulnerability Prediction Performance using Self-Attention Mechanism and Convolutional Neural Network","2022","Proceedings of SPIE - The International Society for Optical Engineering","12258","","122580F","","","","","10.1117/12.2639144","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136972981&doi=10.1117%2f12.2639144&partnerID=40&md5=7857cdd75b0df26d4b938070872b486a","With the vigorous development of the Internet, the number of commonly used software has also increased rapidly. The security and reliability of software have become important challenges that researchers must deal with. Fuzzing is a way of detecting vulnerabilities by providing unintended inputs to the target software and observing the final running results. During these years, fuzzing has proven its effectiveness in software security testing. But a large number of fuzzing tools rely solely on runtime information while testing software. Achieving static vulnerability prediction for programs in advance can greatly improve the efficiency of fuzzing. Vulnerability prediction aims to obtain the possibility of vulnerabilities in different parts of the program. The existing vulnerability prediction methods are relatively simple for feature extraction between basic blocks. We design a novel model combining self-attention mechanism and convolutional neural networks, which can extract and integrate the internal information of functions. Compared with the state-of-the-art V-Fuzz, our recall can be improved by 9.7 percentage points, and the accuracies of K-100~K-1000 can be higher than 90%. © 2022 SPIE.","Artificial Intelligence; Convolutional Neural Networks; Fuzzing; Self-Attention; Vulnerability Prediction","Convolution; Convolutional neural networks; Network security; Software reliability; Software testing; Attention mechanisms; Convolutional neural network; Fuzzing; Prediction performance; Run-time information; Security and reliabilities; Self-attention; Software security testing; Testing software; Vulnerability prediction; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85136972981"
"Kalouptsoglou I.; Siavvas M.; Kehagias D.; Chatzigeorgiou A.; Ampatzoglou A.","Kalouptsoglou, Ilias (57219327969); Siavvas, Miltiadis (57194500913); Kehagias, Dionysios (7003972544); Chatzigeorgiou, Alexandros (6701702023); Ampatzoglou, Apostolos (16027681600)","57219327969; 57194500913; 7003972544; 6701702023; 16027681600","An Empirical Evaluation of the Usefulness of Word Embedding Techniques in Deep Learning-Based Vulnerability Prediction","2022","Communications in Computer and Information Science","1596 CCIS","","","23","37","14","","10.1007/978-3-031-09357-9_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134297473&doi=10.1007%2f978-3-031-09357-9_3&partnerID=40&md5=f882a8c74e1e1a363be90baa94a39d36","Software security is a critical consideration for software development companies that want to provide their customers with high-quality and dependable software. The automated detection of software vulnerabilities is a critical aspect in software security. Vulnerability prediction is a mechanism that enables the detection and mitigation of software vulnerabilities early enough in the development cycle. Recently the scientific community has dedicated a lot of effort on the design of Deep learning models based on text mining techniques. Initially, Bag-of-Words was the most promising method but recently more complex models have been proposed focusing on the sequences of instructions in the source code. Recent research endeavors have started utilizing word embedding vectors, which are widely used in text classification tasks like semantic analysis, for representing the words (i.e., code instructions) in vector format. These vectors could be trained either jointly with the other layers of the neural network, or they can be pre-trained using popular algorithms like word2vec and fast-text. In this paper, we empirically examine whether the utilization of word embedding vectors that are pre-trained separately from the vulnerability predictor could lead to more accurate vulnerability prediction models. For the purposes of the present study, a popular vulnerability dataset maintained by NIST was utilized. The results of the analysis suggest that pre-training the embedding vectors separately from the neural network leads to better vulnerability predictors with respect to their effectiveness and performance. © 2022, The Author(s).","Deep learning; Natural language processing; Software security; Vulnerability prediction; Word embedding vectors","Classification (of information); Codes (symbols); Deep learning; Embeddings; Multilayer neural networks; Natural language processing systems; Network layers; Network security; Semantics; Software design; Text processing; Vectors; Deep learning; Embeddings; Language processing; Natural language processing; Natural languages; Neural-networks; Software security; Software vulnerabilities; Vulnerability prediction; Word embedding vector; Forecasting","Conference paper","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85134297473"
"Siavvas M.; Gelenbe E.; Tsoukalas D.; Kalouptsoglou I.; Mathioudaki M.; Nakip M.; Kehagias D.; Tzovaras D.","Siavvas, Miltiadis (57194500913); Gelenbe, Erol (7006026729); Tsoukalas, Dimitrios (57208865760); Kalouptsoglou, Ilias (57219327969); Mathioudaki, Maria (57279305500); Nakip, Mert (57212473263); Kehagias, Dionysios (7003972544); Tzovaras, Dimitrios (13105681700)","57194500913; 7006026729; 57208865760; 57219327969; 57279305500; 57212473263; 7003972544; 13105681700","The IoTAC Software Security-by-Design Platform: Concept, Challenges, and Preliminary Overview","2022","2022 18th International Conference on the Design of Reliable Communication Networks, DRCN 2022","","","","","","","","10.1109/DRCN53993.2022.9758028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129638004&doi=10.1109%2fDRCN53993.2022.9758028&partnerID=40&md5=eac1885d66e8ec5e848d3977df365d69","Critical everyday activities handled by modern IoT Systems imply that security is of major concern both for the end-users and the industry. Securing the IoT System Architecture is commonly used to strengthen its resilience to malicious attacks. However, the security of software running on the IoT must be considered as well, since the exploitation of its vulnerabilities can infringe the security of the overall system, regardless of how secure its architecture may be. Thus, we present an IoT Software Security-by-Design (SSD) Platform, which provides mechanisms for monitoring and optimizing the security of IoT software applications throughout their development lifecycle, to validate the broader security of the IoT software. This paper describes the proposed SSD platform that leverages security information from all phases of development, using some novel mechanisms that have been implemented, and which can lead to a holistic security evaluation and future security certification.  © 2022 IEEE.","Internet of Things; Requirements Engineering; Software Security; Static Analysis; Vulnerability Prediction","Application programs; Computer architecture; Internet of things; Life cycle; Network security; Design platform; End-users; ITS architecture; Malicious attack; Platform concept; Requirement engineering; Software applications; Software security; Systems architecture; Vulnerability prediction; Static analysis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85129638004"
"Kekül H.; Ergen B.; Arslan H.","Kekül, Hakan (57292094500); Ergen, Burhan (6508022484); Arslan, Halil (57213369355)","57292094500; 6508022484; 57213369355","A multiclass hybrid approach to estimating software vulnerability vectors and severity score","2021","Journal of Information Security and Applications","63","","103028","","","","","10.1016/j.jisa.2021.103028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116928322&doi=10.1016%2fj.jisa.2021.103028&partnerID=40&md5=8a5d7a30b5c6e67a94b71b74f22bd81e","Classifying detected software vulnerabilities is an important process. However, the metric values of security vectors are manually determined by humans, which takes time and may introduce errors stemming from human nature. These metrics are important because of their role in the calculation of vulnerability severity. It is necessary to use machine learning algorithms and data mining techniques to improve the quality and speed of vulnerability analysis and discovery processes. However, studies in this area are still limited. In this study, vulnerability vectors were estimated using the natural language processing techniques bag of words, term frequency–inverse document frequency, and n-gram for feature extraction together with various multiclass classification algorithms, namely Naïve Bayes, decision tree, k-nearest neighbors, multilayer perceptron, and random forest. Our experiments using a large public dataset facilitate assessment and provide a standard-compliant prediction model for classifying software vulnerability vectors. The results show that the joint use of different techniques and classification algorithms is a promising solution to a multi-probability and difficult-to-predict problem. In addition, our study fills an important gap in its field in terms of the size of the dataset used and because it covers a vulnerability scoring system version that has not yet been extensively studied. © 2021 Elsevier Ltd","Information security; Multiclass classification; Software security; Software vulnerability; Text analysis","Classification (of information); Data mining; Decision trees; Inverse problems; Large dataset; Learning algorithms; Machine learning; Natural language processing systems; Nearest neighbor search; Quality control; Security of data; Vectors; Classification algorithm; Estimating software; Human nature; Hybrid approach; Learning data; Machine learning algorithms; Metric values; Multi-class classification; Software security; Software vulnerabilities; Text processing","Article","Final","","Scopus","2-s2.0-85116928322"
"Xi Z.; Miao W.; Zeng Z.; Zhang B.","Xi, Zesheng (57204622888); Miao, Weiwei (57192541666); Zeng, Zeng (57876478700); Zhang, Bo (57205953618)","57204622888; 57192541666; 57876478700; 57205953618","Security Analysis of Power Internet of Things Application Based on Guided Fuzzy Test","2022","Proceedings of SPIE - The International Society for Optical Engineering","12331","","1233136","","","","","10.1117/12.2652284","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142447141&doi=10.1117%2f12.2652284&partnerID=40&md5=bc8af244c38de668de17aa6549df3e13","With the open sharing of power Internet of things, many IOT applications are in an uncontrollable environment, facing risks such as vulnerabilities/attacks and advanced sustainable threats. Through the software security detection before the application goes online to eliminate vulnerabilities, the resilience of IOT applications to threats can be greatly improved. However, the current software analysis methods have some problems, such as insufficient program coverage, high resource consumption, randomness of analysis results and so on. In view of the above problems, this paper studies the software security analysis method combining static and dynamic. Based on the control flow chart extracted by static analysis and combined with vulnerability potential evaluation, this paper generates the control flow chart with additional vulnerability prediction weight, so as to guide fuzzy testing to carry out dynamic analysis and improve the overall efficiency of fuzzy testing. Finally, a cross platform dynamic analysis architecture is proposed to improve the analysis efficiency by optimizing the intermediate language and code refactoring. © 2022 SPIE.","control flow graph analysis; fuzzy test guidance; power Internet of things; vulnerability potential assessment","Application programs; Data flow analysis; Data flow graphs; Efficiency; Flowcharting; Graphic methods; Monitoring; Security systems; Static analysis; Control flow graph analyse; Control-flow graphs; Fuzzy test guidance; Fuzzy tests; Graph analysis; IOT applications; Power; Power internet of thing; Security analysis; Vulnerability potential assessment; Internet of things","Conference paper","Final","","Scopus","2-s2.0-85142447141"
"Saletta M.; Ferretti C.","Saletta, Martina (57211179736); Ferretti, Claudio (57191135612)","57211179736; 57191135612","A Grammar-based Evolutionary Approach for Assessing Deep Neural Source Code Classifiers","2022","2022 IEEE Congress on Evolutionary Computation, CEC 2022 - Conference Proceedings","","","","","","","","10.1109/CEC55065.2022.9870317","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138675839&doi=10.1109%2fCEC55065.2022.9870317&partnerID=40&md5=612a949747daad59614dd3a4882cd578","Neural networks for source code processing have proven to be effective for solving multiple tasks, such as locating bugs or detecting vulnerabilities. In this paper, we propose an evolutionary approach for probing the behaviour of a deep neural source code classifier by generating instances that sample its input space. First, we apply a grammar-based genetic algorithm for evolving Python functions that minimise or maximise the probability of a function to be in a certain class, and we also produce programs that yield an output near to the classification threshold, namely for which the network does not express a clear classification preference. We then use such sets of evolved programs as initial popu-lations for an evolution strategy approach in which we apply, by following different policies, constrained small mutations to the individuals, so to both explore the decision boundary of the network and to identify the features that most contribute to a particular prediction. We furtherly point out how our approach can be effectively used for several tasks in the scope of the interpretable machine learning, such as for producing adversarial examples able to deceive a network, for identifying the most salient features, and further for characterising the abstract concepts learned by a neural model.  © 2022 IEEE.","decision boundaries; deep neural networks; evolution strategy; source code classifiers; structured grammatical evolution","Computer software; Genetic algorithms; Network coding; Python; Decision boundary; Evolution strategies; Evolutionary approach; Grammatical evolution; Input space; Multiple tasks; Neural-networks; Source code classifier; Source codes; Structured grammatical evolution; Deep neural networks","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85138675839"
"Khleel N.A.A.; Nehéz K.","Khleel, Nasraldeen Alnor Adam (57336497400); Nehéz, Károly (56996187000)","57336497400; 56996187000","Deep convolutional neural network model for bad code smells detection based on oversampling method","2022","Indonesian Journal of Electrical Engineering and Computer Science","26","3","","1725","1735","10","","10.11591/ijeecs.v26.i3.pp1725-1735","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131653104&doi=10.11591%2fijeecs.v26.i3.pp1725-1735&partnerID=40&md5=25e710f09432c0322fffbc7364ec9004","Code smells refers to any symptoms or anomalies in the source code that shows violation of design principles or implementation. Early detection of bad code smells improves software quality. Nowadays several artificial neural network (ANN) models have been used for different topics in software engineering: software defect prediction, software vulnerability detection, and code clone detection. It is not necessary to know the source of the data when using ANN models but require large training sets. Data imbalance is the main challenge of artificial intelligence techniques in detecting the code smells. To overcome these challenges, the objective of this study is to presents deep convolutional neural network (D-CNN) model with synthetic minority over-sampling technique (SMOTE) to detect bad code smells based on a set of Java projects. We considered four code-smell datasets which are God class, data class, feature envy and long method and the results were compared based on different performance measures. Experimental results show that the proposed model with oversampling techniques can provide better performance for code smells detection and prediction results can be further improved when the model is trained with more datasets. Moreover, more epochs and hidden layers help increase the accuracy of the model. © 2022 Institute of Advanced Engineering and Science. All rights reserved.","Artificial neural networks; Code smells; Deep convolutional neural network; Software metrics; Synthetic minority over-sampling technique","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85131653104"
"Hariharan M.; Sathish Kumar C.; Tanwar A.; Sundaresan K.; Ganesan P.; Ravi S.; Karthik R.","Hariharan, M. (57539476400); Sathish Kumar, C. (57668419300); Tanwar, Anshul (57219734758); Sundaresan, Krishna (57219739718); Ganesan, Prasanna (7007029978); Ravi, Sriram (57219734697); Karthik, R. (55953378300)","57539476400; 57668419300; 57219734758; 57219739718; 7007029978; 57219734697; 55953378300","Proximal Instance Aggregator networks for explainable security vulnerability detection","2022","Future Generation Computer Systems","134","","","303","318","15","","10.1016/j.future.2022.04.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129519264&doi=10.1016%2fj.future.2022.04.008&partnerID=40&md5=855b062fadbb81215ba5c5e4b50cbecd","Security vulnerabilities in software are the root cause of cyberattacks. Considering that these defects have huge associated costs, they should be proactively detected and resolved before shipping the software. Data-driven approaches like Artificial Intelligence (AI) are vastly explored for automatic vulnerability detection, given their potential to leverage large-scale vulnerability data feeds and learn from these scenarios. This work introduces a novel Proximal Instance Aggregator (PIA) neural network for accurately capturing insecure C code patterns from Abstract Syntax Tree (AST). It is built upon the concept of Multiple Instance Learning (MIL), which treats the AST representation of the code as a ‘bag’ of tree path ‘instances’. The security vulnerability can manifest in one or multiple such AST path instances. The PIA model dynamically learns a set of abstract concepts to describe the patterns associated with the AST paths. Specifically, the vulnerable nature of an AST path is characterized by its proximity to these concepts. The model also employs the attention mechanism to generate deep representations. By drawing cross-correlation of features between the path instances, the self-attention robustly weighs the relevance of each AST path towards vulnerability classification. The MIL utilizes these deep feature sets to construct the concept space. Thus, even without explicit supervision for localizing the line of defect, the AI automatically learns AST instance classification in a weakly supervised manner. Since AST-level prediction is formed as an aggregation of instance classifications, the AI is inherently explainable. The model outperforms state-of-the-art methods by a fair margin. It achieves 95.63% detection accuracy and 95.65% F1-score on the benchmarked NIST SARD, NVD datasets for a range of vulnerabilities. © 2022 Elsevier B.V.","Abstract Syntax Tree; Deep learning; Interpretability; Multiple-Instance learning; Vulnerability detection; Weakly supervised learning","Deep learning; Electronic data interchange; Network security; Syntactics; Trees (mathematics); Abstract Syntax Trees; Deep learning; Instance classifications; Interpretability; Learn+; Multiple-instance learning; Root cause; Security vulnerabilities; Vulnerability detection; Weakly supervised learning; C (programming language)","Article","Final","","Scopus","2-s2.0-85129519264"
"Cao X.; Liu T.; Zhang J.; Feng M.; Zhang X.; Cao W.; Sun H.; Zhang Y.","Cao, Xudong (57215898460); Liu, Tianwei (57848675300); Zhang, Jiayuan (57223896066); Feng, Mengyue (57485119200); Zhang, Xin (57847800400); Cao, Wanying (57292161800); Sun, Hongyu (57203889719); Zhang, Yuqing (56027290000)","57215898460; 57848675300; 57223896066; 57485119200; 57847800400; 57292161800; 57203889719; 56027290000","SbrPBert: A BERT-Based Model for Accurate Security Bug Report Prediction","2022","Proceedings - 52nd Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshop Volume, DSN-W 2022","","","","129","134","5","","10.1109/DSN-W54100.2022.00030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136151037&doi=10.1109%2fDSN-W54100.2022.00030&partnerID=40&md5=36ca12b0d6969f662eaabe8f3f22484c","Bidirectional Encoder Representation from Transformers (Bert) has achieved impressive performance in several Natural Language Processing (NLP) tasks. However, there has been limited investigation on its adaptation guidelines in specialized fields. Here we focus on the software security domain. Early identification of security-related reports in software bug reports is one of the essential means to prevent security accidents. However, the prediction of security bug reports (SBRs) is limited by the scarcity and imbalance of samples in this field and the complex characteristics of SBRs. So motivated, we constructed the largest dataset in this field and proposed a Security Bug Report Prediction Model Based on Bert (SbrPBert). By introducing a layer-based learning rate attenuation strategy and a fine-tuning method for freezing some layers, our model outperforms the baseline model on both our dataset and other small-sample datasets. This means the practical value of the model in BUG tracking systems or projects that lack samples. Moreover, our model has detected 56 hidden vulnerabilities through deployment on the Mozilla and RedHat projects so far.  © 2022 IEEE.","Bert; deep learning; security bug report; vulnerability","Deep learning; Natural language processing systems; Bidirectional encoder representation from transformer; Bug reports; Deep learning; Model-based OPC; Natural languages; Performance; Prediction modelling; Security bug report; Security bugs; Vulnerability; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85136151037"
"Bassi D.; Singh H.","Bassi, Deepali (57764927900); Singh, Hardeep (58376239500)","57764927900; 58376239500","Optimizing Hyperparameters for Improvement in Software Vulnerability Prediction Models","2022","Lecture Notes in Networks and Systems","427","","","533","544","11","","10.1007/978-981-19-1018-0_46","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136153615&doi=10.1007%2f978-981-19-1018-0_46&partnerID=40&md5=30e666f1ec08cbc829e810b65fb7ac39","Software vulnerability provides the gateway for attackers to breach the confidentiality, integrity, and availability of information systems. Therefore, the prediction of software vulnerability is an important concern for developing secure software. Software vulnerability prediction (SVP) models predict whether a software component is vulnerable or not. Studies have shown that the model’s efficiency is dependent on its hyperparameter settings. In order to build an effective model, their hyperparameters need to be optimized. In our study, we intend to find the impact of hyperparameter optimization on the performance of SVP models. We performed the experiments using python hyperparameter optimization framework ‘Optuna’ to find the best hyperparameters for eight machine learning algorithms on three public datasets Drupal, Moodle and PHPMyAdmin. We found the p-values to be less than 0.05 in 19 out of 24 cases. Hence, hyperparameter optimization is 79.17% effective in increasing the efficacy of SVP models in our study. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Hyperparameter optimization; Machine learning algorithms; Software vulnerability","","Conference paper","Final","","Scopus","2-s2.0-85136153615"
"Li Z.; Wen Y.; Dong F.; Xia H.; Peng L.; Zheng H.","Li, Zixuan (57203624479); Wen, Yaoke (55461897100); Dong, Fangdong (56158504900); Xia, Hailong (57927237200); Peng, Lei (57928029600); Zheng, Hao (56178563500)","57203624479; 55461897100; 56158504900; 57927237200; 57928029600; 56178563500","Assessment of Behind-Armor Trauma Based on Human Anatomical Structure; [基于人体解剖结构的单兵防护后损伤评估]","2022","Binggong Xuebao/Acta Armamentarii","43","9","","2190","2199","9","","10.12382/bgxb.2022.0383","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139867597&doi=10.12382%2fbgxb.2022.0383&partnerID=40&md5=201b3c91e7115b90196e913df34883b6","To evaluate the severity of damage to the human body caused by the blunt bullet or its penetrating through the body armor when the human target wears body armor, a digital model of human body wearing an armor is developed based on real human anatomy data. Then, the process of the bullet penetrating the protected gelatin target is captured by high-speed photography. The evolution data of the instantaneous space cavity inside the gelatin target is obtained. A simplified model of the instantaneous space cavity is established. The Abbreviated Injury Scale (AIS) method is used to evaluate tissue and organ damage. Damage scores of the corresponding parts of the human body under blunt hit and penetrating hit are obtained based on MAIS and NISS damage assessment algorithms, respectively. Besides, mortality rate is predicted. Human susceptibility assessment software is developed to obtain a severity score of damage caused to the human body when a small-caliber bullet bluntly hits and penetrates the body wearing an body armor at different speeds. When a small-caliber bullet hits the human chest with a body armor at 638 m/s, it causes blunt damage; the human injury score is MAIS = 3, NISS = 27, and the mortality prediction result is 12.88% . When a small-caliber bullet hits at 714 m/s, it penetrates the body armor and directly invades human tissue; the human injury score is MAIS = 5, NISS = 75, and the mortality prediction result is 97% . © 2022 China Ordnance Society. All rights reserved.","blunt trauma; body armor; human vulnerability; injury assessment; penetration","Damage detection; High speed photography; Tissue; Wear of materials; Anatomical structures; Blunt trauma; Body armour; Digital modeling; Human anatomy; Human bodies; Human injury; Human vulnerability; Injury assessment; Penetration; Armor","Article","Final","","Scopus","2-s2.0-85139867597"
"Mosolygó B.; Vándor N.; Hegedűs P.; Ferenc R.","Mosolygó, Balázs (57219332393); Vándor, Norbert (57219330885); Hegedűs, Péter (25926433300); Ferenc, Rudolf (6603559878)","57219332393; 57219330885; 25926433300; 6603559878","A Line-Level Explainable Vulnerability Detection Approach for Java","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13380 LNCS","","","106","122","16","","10.1007/978-3-031-10542-5_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135883758&doi=10.1007%2f978-3-031-10542-5_8&partnerID=40&md5=3ec5acf9e888ee43583cab8182cb8d4b","Given our modern society’s level of dependency on IT technology, high quality and security are not just desirable but rather vital properties of current software systems. Empirical methods leveraging the available rich open-source data and advanced data processing techniques of ML algorithms can help software developers ensure these properties. Nonetheless, state-of-the-art bug and vulnerability prediction methods are rarely used in practice due to numerous reasons. The predictions are not actionable in most of the cases due to their level of granularity (i.e., they mark entire classes/files to be buggy or vulnerable) and because the methods seldom provide explanation why a fragment of source code is problematic. In this paper, we present a novel Java vulnerability detection method that addresses both of these issues. It is an adaptation of our previous method for JavaScript that is capable of pinpointing vulnerable source code lines of a program together with a prototype-based explanation. The method relies on the word2vec similarity of code fragments to known vulnerable source code lines. Our empirical evaluation showed promising results, we could detect 61% and 41% of the vulnerable code lines by flagging only 43% and 22% of the program code lines, respectively, using two of the best detection configurations. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Empirical study; Explainable prediction model; Software security; Vulnerability prediction","Codes (symbols); Java programming language; Open source software; Open systems; Detection approach; Empirical studies; Explainable prediction model; Prediction modelling; Property; Software security; Source code lines; Vulnerability detection; Vulnerability prediction; Forecasting","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85135883758"
"Kalouptsoglou I.; Siavvas M.; Kehagias D.; Chatzigeorgiou A.; Ampatzoglou A.","Kalouptsoglou, Ilias (57219327969); Siavvas, Miltiadis (57194500913); Kehagias, Dionysios (7003972544); Chatzigeorgiou, Alexandros (6701702023); Ampatzoglou, Apostolos (16027681600)","57219327969; 57194500913; 7003972544; 6701702023; 16027681600","Examining the Capacity of Text Mining and Software Metrics in Vulnerability Prediction","2022","Entropy","24","5","651","","","","","10.3390/e24050651","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130152365&doi=10.3390%2fe24050651&partnerID=40&md5=8cf675d79bb289e7555c514310777cd4","Software security is a very important aspect for software development organizations who wish to provide high-quality and dependable software to their consumers. A crucial part of software security is the early detection of software vulnerabilities. Vulnerability prediction is a mechanism that facilitates the identification (and, in turn, the mitigation) of vulnerabilities early enough during the software development cycle. The scientific community has recently focused a lot of attention on developing Deep Learning models using text mining techniques for predicting the existence of vulnerabilities in software components. However, there are also studies that examine whether the utilization of statically extracted software metrics can lead to adequate Vulnerability Prediction Models. In this paper, both software metrics-and text mining-based Vulnerability Prediction Models are constructed and compared. A combination of software metrics and text tokens using deep-learning models is examined as well in order to investigate if a combined model can lead to more accurate vulnerability prediction. For the purposes of the present study, a vulnerability dataset containing vulnerabilities from real-world software products is utilized and extended. The results of our analysis indicate that text mining-based models outperform software metrics-based models with respect to their F2-score, whereas enriching the text mining-based models with software metrics was not found to provide any added value to their predictive performance. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","dataset extension; deep learning; ensemble learning; machine learning; software metrics; text mining; vulnerability prediction","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85130152365"
"Murthy G.S.; Kavitha V.; Chandrasekar V.; Kumar D.J.N.; Deivakani M.; Tamilarasan T.","Murthy, Garapati Sn (57191330102); Kavitha, V. (57224706613); Chandrasekar, V. (57198080826); Kumar, Dirisala J Nagendra (58068884100); Deivakani, M. (56026028900); Tamilarasan, T. (57999747500)","57191330102; 57224706613; 57198080826; 58068884100; 56026028900; 57999747500","Cyber-Attack Prediction in Virtual IoT using Leveraging Mechanism","2022","3rd International Conference on Smart Electronics and Communication, ICOSEC 2022 - Proceedings","","","","556","561","5","","10.1109/ICOSEC54921.2022.9952101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143652377&doi=10.1109%2fICOSEC54921.2022.9952101&partnerID=40&md5=5cd085708e90e702a6fa03a6d0e7404c","Internet of Things (IoT) adoption grows in numerous industries, cyber-security threats utilising low-cost end-user devices increase, compromising IoT implementation in a variety of situations. To solve this issue, developing Software Defined Networking (SDN) and Network Function Virtualization (NFV)bring new safety accelerators, providing IoT network systems with the versatility needed to deal with theIoT security deployments. Here, honeynets may be strengthened with support for SDN and NFV, allowing them to be used in IoT applications and boosting security. They provide service for virtualization that replicate the setup of actual IoT networks so that intruders might be diverted from their intended target.  © 2022 IEEE.","cyber-security; Internet of Things; intrusion detection; network deployment; vulnerability","Costs; Internet of things; Intrusion detection; Network function virtualization; Network security; Security systems; Virtual reality; Attack prediction; Cyber security; Cyber-attacks; Intrusion-Detection; Low-costs; Network deployment; Security threats; Software-defined networkings; Software-defined networks; Vulnerability; Cybersecurity","Conference paper","Final","","Scopus","2-s2.0-85143652377"
"Topcu B.; Oz I.","Topcu, Burak (57672081800); Oz, Isil (37097877800)","57672081800; 37097877800","Predicting the Soft Error Vulnerability of GPGPU Applications","2022","Proceedings - 30th Euromicro International Conference on Parallel, Distributed and Network-Based Processing, PDP 2022","","","","108","115","7","","10.1109/PDP55904.2022.00025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129624617&doi=10.1109%2fPDP55904.2022.00025&partnerID=40&md5=faf1ae8644cb923d4fe2e58555a6dbd6","As Graphics Processing Units (GPUs) have evolved to deliver performance increases for general-purpose computations as well as graphics and multimedia applications, soft error reliability becomes an important concern. The soft error vulnerability of the applications is evaluated via fault injection experiments. Since performing fault injection takes impractical times to cover the fault locations in complex GPU hardware structures, prediction-based techniques have been proposed to evaluate the soft error vulnerability of General-Purpose GPU (GPGPU) programs based on the hardware performance characteristics.In this work, we propose ML-based prediction models for the soft error vulnerability evaluation of GPGPU programs. We consider both program characteristics and hardware performance metrics collected from either the simulation or the profiling tools. While we utilize regression models for the prediction of the masked fault rates, we build classification models to specify the vulnerability level of the programs based on their silent data corruption (SDC) and crash rates. Our prediction models achieve maximum prediction accuracy rates of 96.6%, 82.6%, and 87% for masked fault rates, SDCs, and crashes, respectively.  © 2022 IEEE.","","Computer graphics; Computer graphics equipment; Errors; Forecasting; Graphics processing unit; Program processors; Regression analysis; Software testing; Fault injection; Fault rates; General-purpose computations; General-purpose GPUs; Graphics processing; Hardware performance; Performance; Prediction modelling; Processing units; Soft error; Radiation hardening","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85129624617"
"Maza E.; Sultana K.Z.","Maza, Erik (58153448400); Sultana, Kazi Zakia (23494078600)","58153448400; 23494078600","Identifying Evolution of Software Metrics by Analyzing Vulnerability History in Open Source Projects","2022","Proceedings - 2022 IEEE/ACM 9th International Conference on Big Data Computing, Applications and Technologies, BDCAT 2022","","","","223","232","9","","10.1109/BDCAT56447.2022.00039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150678045&doi=10.1109%2fBDCAT56447.2022.00039&partnerID=40&md5=5274a1cf4a37a3b7431db89a131bef02","Software developers mostly focus on functioning code while developing their software paying little attention to the software security issues. Now a days, security is getting priority not only during the development phase, but also during other phases of software development life cycle (starting from requirement specification till maintenance phase). To that end, research have been expanded towards dealing with security issues in various phases. Current research mostly focused on developing different prediction models and most of them are based on software metrics. The metrics based models showed higher precision but poor recall rate in prediction. Moreover, they did not analyze the roles of individual software metric on the occurrences of vulnerabilities separately. In this paper, we target to track the evolution of metrics within the life-cycle of a vulnerability starting from its born version through the last affected version till fixed version. In particular, we studied a total of 250 files from three major releases of Apache Tomcat (8, 9 , and 10). We found that four metrics: AvgCyclomatic, AvgCyclomaticStrict, CountDeclMethod, and CountLineCodeExe show significant changes over the vulnerability history of Tomcat. In addition, we discovered that Tomcat team prioritizes in fixing threatening vulnerabilities such as Denial of Service than less severe vulnerabilities. The results of our research will potentially motivate further research on building more accurate vulnerability prediction models based on the appropriate software metrics. It will also help to assess developer's mindset about fixing different types of vulnerabilities in open source projects. © 2022 IEEE.","Apache Tomcat; software metric; software security; vulnerability","Denial-of-service attack; Forecasting; Open source software; Open systems; Software design; Apache tomcats; Development phasis; Open source projects; Prediction modelling; Security issues; Software developer; Software development life-cycle; Software metrics; Software security; Vulnerability; Life cycle","Conference paper","Final","","Scopus","2-s2.0-85150678045"
"Tambe D.; Ragha L.","Tambe, Darshana (57874742800); Ragha, Lata (35753461900)","57874742800; 35753461900","Analysis of Software Bug Prediction and Tracing Models from a Statistical Perspective Using Machine Learning","2022","2022 2nd International Conference on Intelligent Technologies, CONIT 2022","","","","","","","","10.1109/CONIT55038.2022.9848385","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137274183&doi=10.1109%2fCONIT55038.2022.9848385&partnerID=40&md5=b809e2e25b9a04e563a7f70dadb35f2d","Software is the heart of over 99% of all modern-day devices which include smartphones, personal computers, internet of things (IoT) networks, etc. This software is built by a team of engineers which divide the final product into multiple smaller components and these components are integrated together to build the final software, due to which inherent interfacing vulnerabilities & bugs are injected into it. Multiple bugs are also injected into the system due to inexperience or mistakes made by software engineers & programmers. To identify these mistakes, a wide variety of bug prediction & tracing models are proposed by researchers, which assist programmers to predict & track these bugs. But these models have large variations in terms of accuracy, precision, recall, delay, computational complexity, cost of deployment and other performance metrics, due to which it is ambiguous for software designers to identify best bug tracing method(s) for their application deployments. To reduce this ambiguity, a discussion about design of different bug tracing & prediction models and their statistical comparison is done in this paper. This comparison includes evaluation of accuracy, precision, recall, computational complexity and scalability under different scenarios. Based on this comparison, in this paper experiments were performed on five publically available datasets from NASA MDP repository using different algorithms i.e. DRF, LSVM, LR, RF, and kNN. From the results it was observed that kNN algorithm outperforms average 98.8% accuracy on these five datasets and hence kNN were considered to be the most significant with its selected features. In the future, this performance can be improved via use of CNN & LSTM based models, which can utilize the base kNN layer, and estimate highly dense features for efficient classification performance. © 2022 IEEE.","Classification; Machine Learning; Software Bug Prediction; Tracing models","Application programs; Classification (of information); Complex networks; Computational complexity; Forecasting; Internet of things; Long short-term memory; NASA; Personal computers; Program debugging; Bug predictions; Complexity costs; Machine-learning; Performance metrices; Prediction modelling; Small components; Smart phones; Software bug; Software bug prediction; Tracing model; Learning algorithms","Conference paper","Final","","Scopus","2-s2.0-85137274183"
"Labrèche F.; Paquette S.-O.","Labrèche, François (57194209945); Paquette, Serge-Olivier (57454605200)","57194209945; 57454605200","Threat Class Predictor: An explainable framework for predicting vulnerability threat using topic and trend modeling","2022","CEUR Workshop Proceedings","3391","","","113","124","11","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160799597&partnerID=40&md5=adc4f07d7635e041655e02027dede0a8","Every day, an increasing number of new software is found to be vulnerable to exploitation. Such vulnerabilities are disclosed through publicly available databases, such as the National Vulnerability Database (NVD). However, the rate of disclosures now far outpaces the ability of any single research team or remediation team to handle them all. In this paper, we present a framework that not only predicts the vulnerabilities that will be exploited by malicious actors or malware, but also which vulnerabilities can go under the radar, escaping the trending discussions of online cybersecurity communities. This is achieved by leveraging topic modeling in a novel way, combining a threat score and a trend score. The interpretable nature of such topic models enables security teams to dig deeper into the predictions of our model, making it a valuable tool for their remediation and investigative work. © 2022 CEUR-WS. All rights reserved.","Attack prediction; Exploit prediction; Vulnerability prioritization","Cybersecurity; Malware; Attack prediction; Cyber security; Exploit prediction; Malwares; National vulnerability database; Research teams; Threat scores; Topic and trend modeling; Topic Modeling; Vulnerability prioritization; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85160799597"
"Coskun T.; Halepmollasi R.; Hanifi K.; Fouladi R.F.; De Cnudde P.C.; Tosun A.","Coskun, Tugce (57990579200); Halepmollasi, Rusen (56246366600); Hanifi, Khadija (57195217883); Fouladi, Ramin Fadaei (55807299700); De Cnudde, Pinar Comak (57990545100); Tosun, Ayse (26649652100)","57990579200; 56246366600; 57195217883; 55807299700; 57990545100; 26649652100","Profiling developers to predict vulnerable code changes","2022","PROMISE 2022 - Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering, co-located with ESEC/FSE 2022","","","","32","41","9","","10.1145/3558489.3559069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143197967&doi=10.1145%2f3558489.3559069&partnerID=40&md5=aa575bcbfb3041317cdc8d2bbdc8cad1","Software vulnerability prediction and management have caught the interest of researchers and practitioners, recently. Various techniques that are usually based on characteristics of the code artefacts are also offered to predict software vulnerabilities. While other studies achieve promising results, the role of developers in inducing vulnerabilities has not been studied yet. We aim to profile the vulnerability inducing and vulnerability fixing behaviors of developers in software projects using Heterogeneous Information Network (HIN) analysis. We also investigate the impact of developer profiles in predicting vulnerability inducing commits, and compare the findings against the approach based on the code metrics. We adopt Random Walk with Restart (RWR) algorithm on HIN and the aggregation of code metrics for extracting all the input features. We utilize traditional machine learning algorithms namely, Naive Bayes (NB), Support Vector Machine (SVM), Random Forest (RF) and eXtreme Gradient Boosting (XGBoost) to build the prediction models.We report our empirical analysis to predict vulnerability inducing commits of four Apache projects. The technique based on code metrics achieves 90% success for the recall measure, whereas the technique based on profiling developer behavior achieves 71% success. When we use the feature sets obtained with the two techniques together, we achieve 89% success.  © 2022 Owner/Author.","profiling developers; technical debt; vulnerability; vulnerability prediction","Adaptive boosting; Decision trees; Information services; Network security; Support vector machines; Code changes; Code metrics; Heterogeneous information; Information networks; Profiling developer; Software project; Software vulnerabilities; Technical debts; Vulnerability; Vulnerability prediction; Forecasting","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85143197967"
"Siavvas M.; Tsoukalas D.; Jankovic M.; Kehagias D.; Tzovaras D.","Siavvas, Miltiadis (57194500913); Tsoukalas, Dimitrios (57208865760); Jankovic, Marija (56800124800); Kehagias, Dionysios (7003972544); Tzovaras, Dimitrios (13105681700)","57194500913; 57208865760; 56800124800; 7003972544; 13105681700","Technical debt as an indicator of software security risk: a machine learning approach for software development enterprises","2022","Enterprise Information Systems","16","5","1824017","","","","","10.1080/17517575.2020.1824017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091426748&doi=10.1080%2f17517575.2020.1824017&partnerID=40&md5=cda4cfb39edc69c3987dbadb2f2f44bf","Vulnerability prediction facilitates the development of secure software, as it enables the identification and mitigation of security risks early enough in the software development lifecycle. Although several factors have been studied for their ability to indicate software security risk, very limited attention has been given to technical debt (TD), despite its potential relevance to software security. To this end, in the present study, we investigate the ability of common TD indicators to indicate security risks in software products, both at project-level and at class-level of granularity. Our findings suggest that TD indicators may potentially act as security indicators as well. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","decision making; Software engineering; software security; technical debt; vulnerability prediction","Life cycle; Machine learning; Risks; Turing machines; Limited attentions; Machine learning approaches; Project levels; Secure software; Software development life cycle; Software products; Software security; Technical debts; Software design","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85091426748"
"Ibrahimy A.M.; Dewanta F.; Aminanto M.E.","Ibrahimy, Arya Maulana (57221999981); Dewanta, Favian (55557118400); Aminanto, Muhammad Erza (56502182400)","57221999981; 55557118400; 56502182400","Lightweight Machine Learning Prediction Algorithm for Network Attack on Software Defined Network","2022","APWiMob 2022 - Proceedings: 2022 IEEE Asia Pacific Conference on Wireless and Mobile","","","","","","","","10.1109/APWiMob56856.2022.10014244","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147542488&doi=10.1109%2fAPWiMob56856.2022.10014244&partnerID=40&md5=be1feba76e44892a858860a3e7c6efa7","Nowadays, Software Defined Networking (SDN) technology is massively applied in network infrastructures. However, the SDN has several vulnerabilities to various network attacks, such as Denial of Service (DOS) for attacking the availability of the network and various web-based attacks, such as brute force and privilege escalation attacks. This research proposes a Machine Learning method to classify malicious traffic. The InSDN dataset will represent malicious traffic in the SDN environment. Feature correlation is used in this research to reduce the number of the features in InSDN dataset. The reduced dataset feature gives the fastest learning time with respect to the original dataset. The random forest gives the best metric with 99.9962% in accuracy with respect to learning methods, such as KNN and decision tree.  © 2022 IEEE.","cybersecurity; IDS; machine learning; SDN","Cybersecurity; Decision trees; Denial-of-service attack; Learning algorithms; Machine learning; Network security; Cyber security; IDS; Lightweight machines; Machine-learning; Malicious traffic; Network attack; Networking technology; Prediction algorithms; Software-defined networkings; Software-defined networks; Software defined networking","Conference paper","Final","","Scopus","2-s2.0-85147542488"
"Cheng X.; Nie X.; Li N.; Wang H.; Zheng Z.; Sui Y.","Cheng, Xiao (57211627500); Nie, Xu (57826212600); Li, Ningke (57825544700); Wang, Haoyu (55808022700); Zheng, Zheng (56454635400); Sui, Yulei (54788439800)","57211627500; 57826212600; 57825544700; 55808022700; 56454635400; 54788439800","How About Bug-Triggering Paths? - Understanding and Characterizing Learning-Based Vulnerability Detectors","2022","IEEE Transactions on Dependable and Secure Computing","","","","1","18","17","","10.1109/TDSC.2022.3192419","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135207559&doi=10.1109%2fTDSC.2022.3192419&partnerID=40&md5=665443e3bf468fb973569078eacdeb28","Machine learning and its promising branch deep learning have proven to be effective in a wide range of application domains. Recently, several efforts have shown success in applying deep learning techniques for automatic vulnerability discovery, as alternatives to traditional static bug detection. In principle, these learning-based approaches are built on top of classification models using supervised learning. Depending on the different granularities to detect vulnerabilities, these approaches rely on learning models which are typically trained with well-labeled source code to predict whether a program method, a program slice, or a particular code line contains a vulnerability or not. The effectiveness of these models is normally evaluated against conventional metrics including precision, recall and F1 score. In this paper, we show that despite yielding promising numbers, the above evaluation strategy can be insufficient and even misleading when evaluating the effectiveness of current learning-based approaches. This is because the underlying learning models only produce the classification results or report individual/isolated program statements, but are unable to pinpoint bug-triggering paths, which is an effective way for bug fixing and the main aim of static bug detection. Our key insight is that a program method or statement can only be stated as vulnerable in the context of a bug-triggering path. In this work, we systematically study the gap between recent learning-based approaches and conventional static bug detectors in terms of fine-grained metrics called BTP metrics using bug-triggering paths. We then characterize and compare the quality of the prediction results of existing learning-based detectors under different granularities. Finally, our comprehensive empirical study reveals several key issues and challenges in developing classification models to pinpoint bug-triggering paths and calls for more advanced learning-based bug detection techniques. IEEE","Bug-Triggering Paths; Codes; Computer bugs; Detectors; Empirical Study; Feature extraction; Machine Learning; Measurement; Predictive models; Software Vulnerabilities; Training","Codes (symbols); Deep learning; Learning systems; Program debugging; Bug detection; Bug-triggering path; Code; Computer bugs; Empirical studies; Features extraction; Learning-based approach; Machine-learning; Predictive models; Software vulnerabilities; Feature extraction","Article","Article in press","","Scopus","2-s2.0-85135207559"
"Jbair M.; Ahmad B.; Maple C.; Harrison R.","Jbair, Mohammad (57202992796); Ahmad, Bilal (43561123800); Maple, Carsten (6603067743); Harrison, Robert (23073397000)","57202992796; 43561123800; 6603067743; 23073397000","Threat modelling for industrial cyber physical systems in the era of smart manufacturing","2022","Computers in Industry","137","","103611","","","","","10.1016/j.compind.2022.103611","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123824750&doi=10.1016%2fj.compind.2022.103611&partnerID=40&md5=83da36c70f2ce23e74e7d4b20ce4b0dc","Cyber security risks are considered to be one of the foremost challenges that face organisations intending to leverage the benefits of the Smart Manufacturing paradigm. Due to the rising number of cyber-attacks that target critical Industrial Cyber-Physical Systems (ICPS), organisations are required to consider such attacks as severe business risks. Therefore, identifying potential cyber threats and analysing their impacts is crucial to business continuity planning. This paper proposes a structured threat modelling approach for ICPS that enables prediction and analysis of cyber risks to protect industrial assets from potential cyber-attacks. The method involves classifying ICPS assets based on criticality, and then analysing the cyber security vulnerabilities, threats, risks, impacts, and countermeasures. The proposed methodology enables end-to-end threat modelling through the development of a new framework that is integrated with VueOne digital twin tool to model and analyse threats throughout ICPS lifecycle, identifying cyber risks and proposing mitigation controls. Moreover, it uses meta-data extracted from VueOne tool to automatically generate the software code and hardware configurations that can be directly deployed on ICPS assets in order to implement the countermeasures, thereby protecting them from these potential cyber-attacks. The proposed solution has been implemented on a Festo test rig prototype production line. © 2022 Elsevier B.V.","Automatic code generation; Digital twins; Industrial cyber physical systems; Smart manufacturing; Threat modelling","Automatic programming; Codes (symbols); Computer crime; Crime; Cyber attacks; Cyber Physical System; Flow control; Life cycle; Manufacture; Network security; Program compilers; Risk assessment; Automatic code generations; Business risks; Cybe-physical systems; Cyber security; Cyber-physical systems; Industrial cybe physical system; Manufacturing paradigm; Security risks; Smart manufacturing; Threat modeling; Embedded systems","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85123824750"
"Croft R.; Babar M.A.; Li L.","Croft, Roland (57219534056); Babar, M. Ali (6602842620); Li, Li (57211233239)","57219534056; 6602842620; 57211233239","An Investigation into Inconsistency of Software Vulnerability Severity across Data Sources","2022","Proceedings - 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2022","","","","338","348","10","","10.1109/SANER53432.2022.00050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127048878&doi=10.1109%2fSANER53432.2022.00050&partnerID=40&md5=3776d80d35dfc05db44817ba7799629d","Software Vulnerability (SV) severity assessment is a vital task for informing SV remediation and triage. Ranking of SV severity scores is often used to advise prioritization of patching efforts. However, severity assessment is a difficult and subjective manual task that relies on expertise, knowledge, and standardized reporting schemes. Consequently, different data sources that perform independent analysis may provide conflicting severity rankings. Inconsistency across these data sources affects the reliability of severity assessment data, and can consequently impact SV prioritization and fixing. In this study, we investigate severity ranking inconsistencies over the SV reporting lifecycle. Our analysis helps characterize the nature of this problem, identify correlated factors, and determine the impacts of inconsistency on downstream tasks. Our findings observe that SV severity often lacks consideration or is underestimated during initial reporting, and such SVs consequently receive lower prioritization. We identify six potential attributes that are correlated to this misjudgment, and show that inconsistency in severity reporting schemes can severely degrade the performance of downstream severity prediction by up to 77%. Our findings help raise awareness of SV severity data inconsistencies and draw attention to this data quality problem. These insights can help developers better consider SV severity data sources, and improve the reliability of consequent SV prioritization. Furthermore, we encourage researchers to provide more attention to SV severity data selection.  © 2022 IEEE.","data quality; severity assessment; software vulnerability","Data quality; Data-source; Down-stream; Independent analysis; Performance; Prioritization; Severity assessment; Software vulnerabilities; Vulnerability prioritization; Vulnerability remediations; Life cycle","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85127048878"
"Croft R.; Newlands D.; Chen Z.; Babar A.M.","Croft, Roland (57219534056); Newlands, Dominic (57226241950); Chen, Ziyu (57226253029); Babar, Ali M. (57194048473)","57219534056; 57226241950; 57226253029; 57194048473","An empirical study of rule-based and learning-based approaches for static application security testing","2021","International Symposium on Empirical Software Engineering and Measurement","","","","","","","","10.1145/3475716.3475781","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112103956&doi=10.1145%2f3475716.3475781&partnerID=40&md5=af6d0b3ae9e04faa600e29af7ef43c0e","Background: Static Application Security Testing (SAST) tools purport to assist developers in detecting security issues in source code. These tools typically use rule-based approaches to scan source code for security vulnerabilities. However, due to the significant shortcomings of these tools (i.e., high false positive rates), learning-based approaches for Software Vulnerability Prediction (SVP) are becoming a popular approach. Aims: Despite the similar objectives of these two approaches, their comparative value is unexplored. We provide an empirical analysis of SAST tools and SVP models, to identify their relative capabilities for source code security analysis. Method: We evaluate the detection and assessment performance of several common SAST tools and SVP models on a variety of vulnerability datasets. We further assess the viability and potential benefits of combining the two approaches. Results: SAST tools and SVP models provide similar detection capabilities, but SVP models exhibit better overall performance for both detection and assessment. Unification of the two approaches is difficult due to lacking synergies. Conclusions: Our study generates 12 main findings which provide insights into the capabilities and synergy of these two approaches. Through these observations we provide recommendations for use and improvement. © 2021 IEEE Computer Society. All rights reserved.","Machine Learning; Security; Static Application Security Testing","Codes (symbols); Machine learning; Mergers and acquisitions; Application security; Machine-learning; Prediction modelling; Security; Security testing; Software vulnerabilities; Source codes; Static application security testing; Testing software; Testing tools; Computer programming languages","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85112103956"
"Ni X.; Zheng J.; Guo Y.; Jin X.; Li L.","Ni, Xuming (57872860500); Zheng, Jianxin (57872860400); Guo, Yu (58463701800); Jin, Xu (57872459600); Li, Ling (57872353700)","57872860500; 57872860400; 58463701800; 57872459600; 57872353700","Predicting severity of software vulnerability based on BERT-CNN","2022","Proceedings - 2022 International Conference on Computer Engineering and Artificial Intelligence, ICCEAI 2022","","","","711","715","4","","10.1109/ICCEAI55464.2022.00151","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137174859&doi=10.1109%2fICCEAI55464.2022.00151&partnerID=40&md5=3e14df4ec51deae6e6cb6eef1985f6d0","Software vulnerabilities threaten the security of computer system, and recently more and more loopholes have been discovered and disclosed. For the detected vulnerabilities, the relevant personnel will analyze the vulnerability characteristics, and combine the vulnerability scoring system to determine their severity level, so as to determine which vulnerabilities need to be dealt with first. In recent years, some characteristic description-based methods have been used to predict the severity level of vulnerability. However, the traditional text processing methods only grasp the superficial meaning of the text and ignore the important contextual information in the text. Therefore, this paper proposes an innovative method, called BERT-CNN, which combines the specific task layer of Bert with CNN to capture important contextual information in the text. First, we use Bert to process the vulnerability description and other information, including Access Gained, Attack Origin and Authentication Required, to generate the feature vectors. Then these feature vectors of vulnerabilities and their severity levels are input into a CNN network, and the parameters of the CNN are gotten. Next, the fine-Tuned Bert and the trained CNN are used to predict the severity level of a vulnerability. The results show that our method outperforms the state-of-The-Art method with 91.31% on F1-score. © 2022 IEEE.","Bert; CNN; software vulnerability; vulnerability severity prediction","Network security; Text processing; Bert; Contextual information; Features vector; Innovative method; Processing method; Scoring systems; Software vulnerabilities; Specific tasks; Text-processing; Vulnerability severity prediction; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85137174859"
"Kumar V.; Sinha D.","Kumar, Vikash (57196218752); Sinha, Ditipriya (26649059100)","57196218752; 26649059100","A robust intelligent zero-day cyber-attack detection technique","2021","Complex and Intelligent Systems","7","5","","2211","2234","23","","10.1007/s40747-021-00396-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127557811&doi=10.1007%2fs40747-021-00396-9&partnerID=40&md5=2e1e62b822ce7f69b1ecf95520efa59c","With the introduction of the Internet to the mainstream like e-commerce, online banking, health system and other day-to-day essentials, risk of being exposed to various are increasing exponentially. Zero-day attack(s) targeting unknown vulnerabilities of a software or system opens up further research direction in the field of cyber-attacks. Existing approaches either uses ML/DNN or anomaly-based approach to protect against these attacks. Detecting zero-day attacks through these techniques miss several parameters like frequency of particular byte streams in network traffic and their correlation. Covering attacks that produce lower traffic is difficult through neural network models because it requires higher traffic for correct prediction. This paper proposes a novel robust and intelligent cyber-attack detection model to cover the issues mentioned above using the concept of heavy-hitter and graph technique to detect zero-day attacks. The proposed work consists of two phases (a) Signature generation and (b) Evaluation phase. This model evaluates the performance using generated signatures at the training phase. The result analysis of the proposed zero-day attack detection shows higher performance for accuracy of 91.33% for the binary classification and accuracy of 90.35% for multi-class classification on real-time attack data. The performance against benchmark data set CICIDS18 shows a promising result of 91.62% for binary-class classification on this model. Thus, the proposed approach shows an encouraging result to detect zero-day attacks. © 2021, The Author(s).","Cyber-attacks; Heavy-hitters; High volume attack; Low volume attack; Signature generation; Token extraction; Zero-day attack","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85127557811"
"Goonetillake M.D.J.S.; Jayashanka R.; Rathnayaka S.V.","Goonetillake, M.D.J.S. (54420034500); Jayashanka, Rangana (57203638565); Rathnayaka, S.V. (58130504500)","54420034500; 57203638565; 58130504500","Predicting Security-Vulnerable Developers Based on Their Techno-Behavioral Characteristics","2022","International Journal of Information Security and Privacy","16","1","","","","","","10.4018/IJISP.2022010103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149443639&doi=10.4018%2fIJISP.2022010103&partnerID=40&md5=b90b4f801aed9f3d9aa18be068da80a9","Assigning developers for highly secured software projects requires identifying developers' tendency to contribute towards vulnerable software codes called developer-centric security vulnerability to mitigate issues on human resource management, financial, and project timelines. There are problems in assessing the previous codebases in evaluating the developer-centric security vulnerability level of each developer. Thus, this paper suggests a method to evaluate this through the techno-behavioral features of their previous projects. Consequently, the authors present results of an exploratory study of the developer-centric security vulnerability level prediction using a dataset of 1,827 developers by logically selecting 13 techno-behavioral features. The results depict that there is a correlation between techno-behavioral features and developer-centric security vulnerability with 89.46% accuracy. This model enables to predict developer-centric security vulnerability level of any developer if the required techno-behavioral features are available, avoiding the analysis of his/her previous codebases.  Copyright © 2022, IGI Global.","Decision Tree; Developer-Centric Security Vulnerability; Logic Regression; Machine; Naïve Bayes; Random Forest; Support Vector Vector Machine; Techno-Behavioral Features","Forecasting; Human resource management; Support vector machines; Behavioral features; Developer-centric security vulnerability; Logic regression; Machine; Naive bayes; Random forests; Security vulnerabilities; Support vector; Support vector vector machine; Techno-behavioral feature; Vector machines; Decision trees","Article","Final","","Scopus","2-s2.0-85149443639"
"Liu Y.; Zhu M.; Zhang Y.; Chen Y.","Liu, Yiyi (57861641400); Zhu, Minjie (57223040426); Zhang, Yilian (57223114533); Chen, Yan (57219343675)","57861641400; 57223040426; 57223114533; 57219343675","Software Vulnerability Prediction based on Statistical Learning","2022","2022 IEEE International Conference on Artificial Intelligence and Computer Applications, ICAICA 2022","","","","390","394","4","","10.1109/ICAICA54878.2022.9844560","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136679176&doi=10.1109%2fICAICA54878.2022.9844560&partnerID=40&md5=65843f44bde741b7b12b35c730f3798c","Predicting vulnerabilities through source code analysis and using it to guide software maintenance can effectively improve software security. One effective way to predict vulnerabilities is by analyzing library references and function calls used in code. In this paper, we extract library references and function calls from project files through source code analysis, generate sample sets for statistical learning based on these data. Design and train an integrated learning model that can be used for prediction. The designed model has a high accuracy rate and accomplishes the prediction task well. It also proves the correlation between vulnerabilities and library references and function calls.  © 2022 IEEE.","software security; source code analysis; statistical learning; vulnerability prediction","Codes (symbols); Computer programming languages; Function calls; Integrated learning; Prediction-based; Project file; Sample sets; Software security; Software vulnerabilities; Source code analysis; Statistical learning; Vulnerability prediction; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85136679176"
"Şahin S.E.; Özyedierler E.M.; Tosun A.","Şahin, Sefa Eren (57208426892); Özyedierler, Ecem Mine (57416296900); Tosun, Ayse (26649652100)","57208426892; 57416296900; 26649652100","Predicting vulnerability inducing function versions using node embeddings and graph neural networks","2022","Information and Software Technology","145","","106822","","","","","10.1016/j.infsof.2022.106822","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123002701&doi=10.1016%2fj.infsof.2022.106822&partnerID=40&md5=faca0fa5b8ecb873804f1428477ad855","Context: Predicting software vulnerabilities over code changes is a difficult task due to obtaining real vulnerability data and their associated code fixes from software projects as software organizations are often reluctant to report those. Objective: We aim to propose a vulnerability prediction model that runs after every code change, and identifies vulnerability inducing functions in that version. We also would like to assess the success of node and token based source code representations over abstract syntax trees (ASTs) on predicting vulnerability inducing functions. Method: We train neural networks to represent node embeddings and token embeddings over ASTs in order to obtain feature representations. Then, we build two Graph Neural Networks (GNNs) with node embeddings, and compare them against Convolutional Neural Network (CNN) and Support Vector Machine (SVM) with token representations. Results: We report our empirical analysis over the change history of vulnerability inducing functions of Wireshark project. GraphSAGE model using source code representation via ASTs achieves the highest AUC rate, while CNN models using token representations achieves the highest recall, precision and F1 measure. Conclusion: Representing functions with their structural information extracted from ASTs, either in token form or in complete graph form, is great at predicting vulnerability inducing function versions. Transforming source code into token frequencies as a natural language text fails to build successful models for vulnerability prediction in a real software project. © 2022 Elsevier B.V.","Abstract syntax trees; Graph embeddings; Graph neural networks; Software vulnerabilities","Abstracting; Codes (symbols); Graph embeddings; Graph neural networks; Support vector machines; Syntactics; Trees (mathematics); Abstract Syntax Trees; Code changes; Convolutional neural network; Embeddings; Graph embeddings; Graph neural networks; Software organization; Software project; Software vulnerabilities; Source code representations; Forecasting","Article","Final","","Scopus","2-s2.0-85123002701"
"Stewart H.","Stewart, Harrison (57196440874)","57196440874","Security versus Compliance: An Empirical Study of the Impact of Industry Standards Compliance on Application Security","2022","International Journal of Software Engineering and Knowledge Engineering","32","3","","363","393","30","","10.1142/S0218194022500152","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129987093&doi=10.1142%2fS0218194022500152&partnerID=40&md5=b072fd6d83cf64dcf11f15a614368d73","The integration of security aspects into software development is an open topic, especially in highly regulated industries where standards are accompanied by a high degree of complexity. The research question of this paper relates to the misconception of industry standards compliance and security in the field of software development. Cyber attackers are constantly inventing new tools to penetrate systems and exploit even the most minor flaws, and adherence to an industry standard is not a solution. In this study, an empirical investigation is conducted over a six-month period to observe various customer relationship management (CRM) systems. To analyze and anticipate the vulnerabilities of various CRMs, penetration testing methodologies and cross-project prediction approaches are employed. Classification using multiple machine learning approaches is utilized in the study to increase the discovery of vulnerable components in each CRM. The Student t-test is also used to assess if the mean values of the two CRM datasets are substantially different from each other in order to evaluate the efficacy of overall security and its features. The results show that security best practices during application development have a significant influence on applications created in regulated environments. The action research approach used to validate this study provided positive results and its feasibility in practice to optimize security throughout the application development. This study adds to the literature on information security management systems (ISMS) and best practices in application development in terms of creating and implementing opportunities based on broader information security management measures.  © 2022 World Scientific Publishing Company.","application security; Information security management system (ISMS); reformed ISMS; regulatory standards; security sustainability; technology error-related information security incident","Cybersecurity; Information management; Public relations; Software design; Application security; Information security incidents; Information security management system; Information security managements; Reformed information security management system; Regulatory standards; Security management systems; Security sustainability; Technology error-related information security incident; Regulatory compliance","Article","Final","","Scopus","2-s2.0-85129987093"
"Vándor N.; Mosolygó B.; Hegelűs P.","Vándor, Norbert (57219330885); Mosolygó, Balázs (57219332393); Hegelűs, Péter (57844802800)","57219330885; 57219332393; 57844802800","Comparing ML-Based Predictions and Static Analyzer Tools for Vulnerability Detection","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13380 LNCS","","","92","105","13","","10.1007/978-3-031-10542-5_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135880239&doi=10.1007%2f978-3-031-10542-5_7&partnerID=40&md5=9da2f1c73842881315f2746ce901f8c6","Finding and eliminating security issues early in the development process is critical as software systems are shaping many aspects of our daily lives. There are numerous approaches for automatically detecting security vulnerabilities in the source code from which static analysis and machine learning based methods are the most popular. However, we lack comprehensive benchmarking of vulnerability detection methods across these two popular categories. In one of our earlier works, we proposed an ML-based line-level vulnerability prediction method with the goal of finding vulnerabilities in JavaScript systems. In this paper, we report results on a systematic comparison of this ML-based vulnerability detection technique with three widely used static checker tools NodeJSScan (https://github.com/ajinabraham/nodejsscan ), ESLint (https://eslint.org ), and CodeQL (https://codeql.github.com ) using the OSSF CVE Benchmark (https://github.com/ossf-cve-benchmark/ossf-cve-benchmark ). We found that our method was more than capable of finding vulnerable lines, managing to find 60% of all vulnerabilities present in the examined dataset, which corresponds to the best recall of all tools. Nonetheless, our method had higher false-positive rate and running time than that of the static checkers. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","ML models; OSSF benchmark; Static analyzers; Vulnerability detection","HTTP; Daily lives; Development process; ML model; OSSF benchmark; Security issues; Security vulnerabilities; Software-systems; Source codes; Static analyzers; Vulnerability detection; Static analysis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85135880239"
"Achillopoulou D.V.; Stamataki N.K.","Achillopoulou, Dimitra V. (56100636100); Stamataki, Nikoleta K. (57303803600)","56100636100; 57303803600","Seismic design and assessment of the response of a glass pavilion","2022","Journal of Building Engineering","47","","103825","","","","","10.1016/j.jobe.2021.103825","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121107810&doi=10.1016%2fj.jobe.2021.103825&partnerID=40&md5=eec93bd4118a1973f2550ce4cf675d1e","This research investigates the design of a transparent pavilion (museum) made entirely of structural glass. The main objective of this research is focused on the seismic behavior and the assessment of the response of the structural glass members in regions with high seismic hazard. The fundamental concepts of structural glass and how they can be used in public buildings are described. Particularly, a detailed selection of the structural glass types and the structural members’ cross-sections is presented, based on the natural and mechanical properties of the material. The high importance of the museum and its demands in terms of safety are increased, especially in high seismicity zones. According to EC8 instructions the design should ensure that no collapse occurs. The glass pavilion's response is mainly defined by the elastic nature and brittle failure of glass, which happens in small ranges of deformations. As such, in order to limit them and to prevent failures from extreme earthquake events the basic principles of seismic isolation are taken into account and applied in the model building. The response of the pavilion has been investigated numerically with the use of finite elements. Two different models have been simulated in ANSYS software: a) reference model and b) base-isolated model. A spectrum analysis has been performed, using both EC8 elastic spectrum and a synthetic one from 27 different real seismic excitations. The effect of the damping factor was also taken into account in the analyses. Analytical results show that the elastic behavior of structural glass is not dominant to the overall response of the building under seismic events. The vulnerability of the models has been investigated and the fragility curves show that for limited damages of the structural members the probability is low for the peak ground acceleration set by EC, whereas progressive failure occurs only in extremely intense seismic events. The properly isolated structure has a remarkable effect in the pavilion's seismic behavior. Especially, glass structural members' deformation is almost eliminated. The pseudo-plastic design of beams works favourably to the response and therefore the safety level is increased and no fractures or collapse occurs. Finally, the glass columns are proven to have enough resistance to deformations as the base isolation system enhances the overall performance. The base isolation system also increases the reliability level of the predictions, illustrated also in the fragility curves. © 2021 Elsevier Ltd","Base isolation; Design; Fragility; Public building; Seismic assessment; Structural glass; Transparent & recyclable material","Architectural design; Deformation; Earthquakes; Museums; Seismic design; Seismic response; Spectrum analysis; Base isolation; Fragility; Fragility curves; Public buildings; Recyclable material; Seismic assessment; Seismic behaviour; Seismic event; Structural glass; Transparent material; Glass","Article","Final","","Scopus","2-s2.0-85121107810"
"Prasetyo S.Y.J.; Simanjuntak B.H.; Hartomo K.D.; Sulistyo W.","Prasetyo, Sri Yulianto Joko (57095954500); Simanjuntak, Bistok Hasiholan (57096020300); Hartomo, Kristoko Dwi (56543586000); Sulistyo, Wiwin (57210625722)","57095954500; 57096020300; 56543586000; 57210625722","Computer model for tsunami vulnerability using sentinel 2a and srtm images optimized by machine learning","2021","Bulletin of Electrical Engineering and Informatics","10","5","","2821","2835","14","","10.11591/eei.v10i5.3100","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116644576&doi=10.11591%2feei.v10i5.3100&partnerID=40&md5=5cc75cf2acce4b45aa4265e368dbe6f3","This study aims to develop a software framework for modeling of tsunami vulnerability using DEM and Sentinel 2 images. The stages of study, are: 1) extraction Sentinel 2 images using algorithms NDVI, NDBI, NDWI, MSAVI, and MNDWI; 2) prediction vegetation indices using machine learning algorithms. 3) accuracy testing using the MSE, ME, RMSE, MAE, MPE, and MAPE; 4) spatial prediction using Kriging function and 5) modeling tsunami vulnerability indicators. The results show that in 2021 the area was dominated by vegetation density between (-0.1-0.3) with moderate to high vulnerability and risk of land use tsunami as a result of the decreasing of vegetation. The prediction results for 2021 show a low canopy density of vegetation and a high degree of land surface slope. Based on the prediction results in 2021, the study area mostly shows the existence of built-up lands with a high tsunami vulnerability risk (>0.1). Vegetation population had decreased to 67% from the original areas in 2017 with an area of 135 km2. Forest vegetation had decreased by 45% from 116 km2 in 2017. Land use for fisheries had increased to the area of 86 km2 from 2017 with an area of 24 km2. © 2021, Institute of Advanced Engineering and Science. All rights reserved.","Kriging; Machine learning; Remote sensing; Tsunami; Vegetation indices","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85116644576"
"Zagane M.; Alenezi M.; Abdi M.K.","Zagane, Mohammed (57216807396); Alenezi, Mamdouh (55854089000); Abdi, Mustapha Kamel (56016662800)","57216807396; 55854089000; 56016662800","Hybrid Representation to Locate Vulnerable Lines of Code","2022","International Journal of Software Innovation","10","1","","","","","","10.4018/IJSI.292020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149474335&doi=10.4018%2fIJSI.292020&partnerID=40&md5=49c3e2fe707087d8ba3cf31b1dcced97","Locating vulnerable lines of code in large software systems requires effort from human experts. This explains the high costs in terms of budget and time needed to correct vulnerabilities. To minimize these costs, automatic solutions of vulnerabilities prediction have been proposed. Existing machine learning (ML)-based solutions face difficulties in predicting vulnerabilities in coarse granularity and in defining suitable code features that limit their effectiveness. To address these limitations, in the present work, the authors propose an improved ML-based approach using slice-based code representation and the technique of TF-IDF to automatically extract effective features. The obtained results showed that combining these two techniques with ML techniques allows building effective vulnerability prediction models (VPMs) that locate vulnerabilities in a finer granularity and with excellent performances (high precision [>98%], low FNR [<2%], and low FPR [<3%]), which outperforms software metrics and are equivalent to the best performing recent deep learning-based approaches. Copyright © 2022, IGI Global.","Automatic Features Extraction; Automatic Vulnerability Prediction; Code Slicing; TF-IDF","","Article","Final","","Scopus","2-s2.0-85149474335"
"Zhang B.; Gao Y.; Wu J.; Wang N.; Wang Q.; Ren J.","Zhang, Bing (56587016400); Gao, Yuan (57219235628); Wu, Jingyi (57954711600); Wang, Ning (58264285500); Wang, Qian (56237686800); Ren, Jiadong (56012213300)","56587016400; 57219235628; 57954711600; 58264285500; 56237686800; 56012213300","Approach to Predict Software Vulnerability Based on Multiple-Level N -gram Feature Extraction and Heterogeneous Ensemble Learning","2022","International Journal of Software Engineering and Knowledge Engineering","32","10","","1559","1582","23","","10.1142/S0218194022500620","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141296616&doi=10.1142%2fS0218194022500620&partnerID=40&md5=b10804f2521469607fec1f4267683471","Software vulnerabilities are one of the roots of computer security problems. The traditional static analysis and dynamic analysis methods based on software source code mainly have some deficiencies, such as high false positive rate, high false negative rate and insufficient semantic information captured. Nevertheless, the application of machine learning, Natural Language Processing and other technologies in software vulnerability prediction can effectively mitigate such issues. This paper proposed a vulnerability prediction method based on multiple-level N-gram feature extraction and heterogeneous ensemble learning. First, by code intermediate representation and constructing a multiple-level N-gram feature generation model, two kinds of N-gram semantic features with different window size and different granularity at word and char level were extracted to retain the semantic and structural information of code. Second, TF-IDF was used to construct the vector space model as the input of prediction model. As a single classifier was prone to overfitting and poor generalization, this paper conducted benchmark testing on five classical machine learning algorithms (NB, SVM, DT, LR, RF), and then combined four (SVM, DT, LR, RF) among them, which had better performance as the base classifiers to form the stacking heterogeneous ensemble method to build the vulnerability prediction model. Finally, the proposed method was verified on buffer overflow vulnerability and resource management vulnerability datasets, with a lowest false positive rate and false negative rate which can reach 1.58% and 4.06%, respectively.  © 2022 World Scientific Publishing Company.","feature extraction; heterogeneous ensemble learning; Multiple-level N -gram; vulnerability prediction","Application programs; Benchmarking; Classification (of information); Computational linguistics; Extraction; Feature extraction; Learning algorithms; Learning systems; Natural language processing systems; Semantics; Static analysis; Support vector machines; Vector spaces; Ensemble learning; False positive rates; Features extraction; Heterogeneous ensemble learning; Heterogeneous ensembles; Multiple levels; Multiple-level N -gram; N-grams; Software vulnerabilities; Vulnerability prediction; Forecasting","Article","Final","","Scopus","2-s2.0-85141296616"
"Zheng Z.; Zhang B.; Liu Y.; Ren J.; Zhao X.; Wang Q.","Zheng, Zhangqi (57200407791); Zhang, Bing (56587016400); Liu, Yongshan (56194246500); Ren, Jiadong (56012213300); Zhao, Xuyang (57388367700); Wang, Qian (56237686800)","57200407791; 56587016400; 56194246500; 56012213300; 57388367700; 56237686800","An approach for predicting multiple-type overflow vulnerabilities based on combination features and a time series neural network algorithm","2022","Computers and Security","114","","102572","","","","","10.1016/j.cose.2021.102572","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121914091&doi=10.1016%2fj.cose.2021.102572&partnerID=40&md5=707ade553223502eaf0f05a170953701","Overflow vulnerability is a common and dangerous software vulnerability that can lead to information theft, resource control, system collapse and other hazards. However, recent studies on predicting software overflow vulnerability have failed to specifically analyze factors and features that can lead to each type of overflow vulnerability and have only focused on binary classification problems rather than multiclassification problems, which are inefficient and time-consuming. Therefore, this paper proposes a multiple-type overflow vulnerability prediction method based on a combination of features and a time series neural network algorithm. First, by analyzing software overflow vulnerability features, a method is proposed to extract the internal vulnerability features of program source code. Then, an IFS set of internal vulnerability features of software overflow vulnerability is constructed. Second, an EFS set of external vulnerability features of software overflow vulnerability is extracted using a source code static analysis tool. A software overflow vulnerability feature library is constructed based on the IFS set and the EFS set. Finally, a multiple-type overflow vulnerability prediction method is constructed based on a time series bidirectional recurrent neural network after the symbol transformation and vector transformation of software overflow vulnerability features. Experiments show that the proposed method offers a higher precision, accuracy, recall rate, and F1 value. Moreover, this method can accurately detect the overflow vulnerability in actual software vulnerability predictions. © 2021","combination features; multitype; overflow vulnerability; recurrent neural network; time series","Codes (symbols); Forecasting; Network security; Static analysis; Time series; Combination feature; Information thefts; Multitype; Neural networks algorithms; Overflow vulnerability; Prediction methods; Resource control; Software vulnerabilities; System collapse; Times series; Recurrent neural networks","Article","Final","","Scopus","2-s2.0-85121914091"
"Suciu O.; Nelson C.; Lyu Z.; Bao T.; Dumitras T.","Suciu, Octavian (57204828060); Nelson, Connor (57222277681); Lyu, Zhuoer (57222276225); Bao, Tiffany (57194975123); Dumitras, Tudor (13609104900)","57204828060; 57222277681; 57222276225; 57194975123; 13609104900","Expected Exploitability: Predicting the Development of Functional Vulnerability Exploits","2022","Proceedings of the 31st USENIX Security Symposium, Security 2022","","","","377","394","17","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140956908&partnerID=40&md5=d0e5c6fc9ea7f302d683ce8cd4cdfbfb","Assessing the exploitability of software vulnerabilities at the time of disclosure is difficult and error-prone, as features extracted via technical analysis by existing metrics are poor predictors for exploit development. Moreover, exploitability assessments suffer from a class bias because “not exploitable” labels could be inaccurate. To overcome these challenges, we propose a new metric, called Expected Exploitability (EE), which reflects, over time, the likelihood that functional exploits will be developed. Key to our solution is a time-varying view of exploitability, a departure from existing metrics. This allows us to learn EE using data-driven techniques from artifacts published after disclosure, such as technical write-ups and proof-of-concept exploits, for which we design novel feature sets. This view also allows us to investigate the effect of the label biases on the classifiers. We characterize the noise-generating process for exploit prediction, showing that our problem is subject to the most challenging type of label noise, and propose techniques to learn EE in the presence of noise. On a dataset of 103,137 vulnerabilities, we show that EE increases precision from 49% to 86% over existing metrics, including two state-of-the-art exploit classifiers, while its precision substantially improves over time. We also highlight the practical utility of EE for predicting imminent exploits and prioritizing critical vulnerabilities. We develop EE into an online platform which is publicly available at https://exploitability.app/. © USENIX Security Symposium, Security 2022.All rights reserved.","","Classification (of information); Data driven technique; Error prones; Features sets; Learn+; Proof of concept; Software vulnerabilities; State of the art; Technical analysis; Time varying; Two-state; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85140956908"
"Şahin C.B.; Dinler Ö.B.; Abualigah L.","Şahin, Canan Batur (57207688292); Dinler, Özlem Batur (57195217030); Abualigah, Laith (57190984712)","57207688292; 57195217030; 57190984712","Prediction of software vulnerability based deep symbiotic genetic algorithms: Phenotyping of dominant-features","2021","Applied Intelligence","51","11","","8271","8287","16","","10.1007/s10489-021-02324-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103423186&doi=10.1007%2fs10489-021-02324-3&partnerID=40&md5=8b309bac373ddda08f7dc04ff1055f86","The detection of software vulnerabilities is considered a vital problem in the software security area for a long time. Nowadays, it is challenging to manage software security due to its increased complexity and diversity. So, vulnerability detection applications play a significant part in software development and maintenance. The ability of the forecasting techniques in vulnerability detection is still weak. Thus, one of the efficient defining features methods that have been used to determine the software vulnerabilities is the metaheuristic optimization methods. This paper proposes a novel software vulnerability prediction model based on using a deep learning method and SYMbiotic Genetic algorithm. We are first to apply Diploid Genetic algorithms with deep learning networks on software vulnerability prediction to the best of our knowledge. In this proposed method, a deep SYMbiotic-based genetic algorithm model (DNN-SYMbiotic GAs) is used by learning the phenotyping of dominant-features for software vulnerability prediction problems. The proposed method aimed at increasing the detection abilities of vulnerability patterns with vulnerable components in the software. Comprehensive experiments are conducted on several benchmark datasets; these datasets are taken from Drupal, Moodle, and PHPMyAdmin projects. The obtained results revealed that the proposed method (DNN-SYMbiotic GAs) enhanced vulnerability prediction, which reflects improving software quality prediction. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Deep learning; Dominance mechanism; Genetic algorithms; Software vulnerability; Symbiotic learning","Application programs; Computer software selection and evaluation; Deep learning; Forecasting; Learning algorithms; Learning systems; Predictive analytics; Software design; Software quality; Benchmark datasets; Forecasting techniques; Meta-heuristic optimizations; Prediction problem; Software development and maintenances; Software quality prediction; Software vulnerabilities; Vulnerability detection; Genetic algorithms","Article","Final","","Scopus","2-s2.0-85103423186"
"Chieffo N.; Fasan M.; Romanelli F.; Formisano A.; Mochi G.","Chieffo, Nicola (57192897296); Fasan, Marco (57193403059); Romanelli, Fabio (57202660183); Formisano, Antonio (54421738200); Mochi, Giovanni (56496989800)","57192897296; 57193403059; 57202660183; 54421738200; 56496989800","Physics-based ground motion simulations for the prediction of the seismic vulnerability of masonry building compounds in Mirandola (Italy)","2021","Buildings","11","12","667","","","","","10.3390/buildings11120667","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121634889&doi=10.3390%2fbuildings11120667&partnerID=40&md5=3562403e685f08127a0e3a0cce9fc25f","The current paper aims at investigating the seismic capacity of a masonry building aggregate in the historical centre of Mirandola based on a reliable ground motion simulation procedure. The examined clustered building is composed of eleven structural units (SUs) mutually interconnected to each other, which are made of brick walls and are characterized by wooden floors poorly connected to the vertical structures. Non-linear static analyses are performed by adopting the 3Muri software to characterize the seismic capacity of both the entire aggregate and the individual SUs. In this framework, a multi-scenario physics-based approach is considered for the definition of the seismic input in terms of broadband seismic signals inclusive of source and site effects. Finally, the incidence of the seismic input variability is discussed for the prediction of the global capacity response of the case study building. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Fragility curves; Masonry building aggregates; Non-linear static analysis; Seismic hazard analysis; Seismic vulnerability assessment","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85121634889"
"Ahmad M.W.; Akram M.U.; Saghar K.; Butt W.H.; Ahmad R.; Hassan A.","Ahmad, Muhammad Waqas (57211361752); Akram, Muhammad Usman (24474159700); Saghar, Kashif (36497260200); Butt, Wasi Haider (26665905200); Ahmad, Rashid (57210525905); Hassan, Ali (57184855600)","57211361752; 24474159700; 36497260200; 26665905200; 57210525905; 57184855600","Machine Learning based Theoretical Framework for Failure Prediction, Detection, and Correction of Mission-Critical Flight Software","2022","2022 2nd International Conference on Digital Futures and Transformative Technologies, ICoDT2 2022","","","","","","","","10.1109/ICoDT255437.2022.9787424","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133183663&doi=10.1109%2fICoDT255437.2022.9787424&partnerID=40&md5=b35b8e4479a5735aa023f224856727c5","Mission-critical flight software acts as the control mechanism for autonomous flights and lies at the heart of next-generation developments in the aviation industry. Most state-of-the-art technological evolution is realized through the use of contemporary software which implements the essentially required, novel, innovative, and featuring value additions. Real-time physical exposure and the data-driven flying nature of aerial vehicles make them vulnerable to an ever-evolving new threat spectrum of cyber security. Nation or state-sponsored cyber attacks through sensors' data corruption, hardware Trojans, or counterfeit wireless signals may exploit dormant and residual software vulnerabilities. It may lead to severe and catastrophic consequences including but not limited to serious injury or death of the crew, extreme damage or loss to equipment and environment. We have proposed a machine learning based theoretical framework for real-time monitoring and failure analysis of autonomous flight software. It has been introduced to protect the mission-critical flight software from run-time data-driven semantic bugs and exploitation that may be caused by missing, jammed, or spoofed data values, due to malicious online cyber activities. The effectiveness of the proposed framework has been demonstrated by the evaluation of a real-world incident of grounding an aerial vehicle by the actors in their vicinity without the intent of the original equipment manufacturer (OEM). The results show that the reported undesired but successful cyber attack may has been avoided by the effective utilization of our proposed cyber defense approach, which is targeted at software failure prediction, detection, and correction for autonomous aerial vehicles.  © 2022 IEEE.","Failure Correction; Failure Detection; Failure Prediction; Flight Software; Machine Learning","Antennas; Crime; Cyber attacks; Failure (mechanical); Forecasting; Malware; Network security; Program debugging; Semantics; Aerial vehicle; Autonomous flight; Data driven; Failure correction; Failure detection; Failures prediction; Flight Software; Machine-learning; Mission critical; Theoretical framework; Machine learning","Conference paper","Final","","Scopus","2-s2.0-85133183663"
"Shi J.; Yang Z.; Xu B.; Kang H.J.; Lo D.","Shi, Jieke (57225119268); Yang, Zhou (57222189359); Xu, Bowen (57189036787); Kang, Hong Jin (57215115758); Lo, David (35269388000)","57225119268; 57222189359; 57189036787; 57215115758; 35269388000","Compressing Pre-trained Models of Code into 3 MB","2022","ACM International Conference Proceeding Series","","","24","","","","","10.1145/3551349.3556964","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144861997&doi=10.1145%2f3551349.3556964&partnerID=40&md5=fc82f94294e2d34bb9a2783fbfd8e4d3","Although large pre-trained models of code have delivered significant advancements in various code processing tasks, there is an impediment to the wide and fluent adoption of these powerful models in software developers' daily workflow: these large models consume hundreds of megabytes of memory and run slowly on personal devices, which causes problems in model deployment and greatly degrades the user experience. It motivates us to propose Compressor, a novel approach that can compress the pre-trained models of code into extremely small models with negligible performance sacrifice. Our proposed method formulates the design of tiny models as simplifying the pre-trained model architecture: searching for a significantly smaller model that follows an architectural design similar to the original pre-trained model. Compressor proposes a genetic algorithm (GA)-based strategy to guide the simplification process. Prior studies found that a model with higher computational cost tends to be more powerful. Inspired by this insight, the GA algorithm is designed to maximize a model's Giga floating-point operations (GFLOPs), an indicator of the model computational cost, to satisfy the constraint of the target model size. Then, we use the knowledge distillation technique to train the small model: unlabelled data is fed into the large model and the outputs are used as labels to train the small model. We evaluate Compressor with two state-of-the-art pre-trained models, i.e., CodeBERT and GraphCodeBERT, on two important tasks, i.e., vulnerability prediction and clone detection. We use our method to compress pre-trained models to a size (3 MB), which is 160 × smaller than the original size. The results show that compressed CodeBERT and GraphCodeBERT are 4.31 × and 4.15 × faster than the original model at inference, respectively. More importantly, they maintain and of the original performance on the vulnerability prediction task. They even maintain higher ratios ( and ) of the original performance on the clone detection task.  © 2022 ACM.","Genetic Algorithm; Model Compression; Pre-Trained Models","Cloning; Codes (symbols); Compressors; Digital arithmetic; Distillation; Clone detection; Computational costs; Fluents; Large models; Model compression; Performance; Personal devices; Pre-trained model; Software developer; Work-flows; Genetic algorithms","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85144861997"
"Yu Z.; Theisen C.; Williams L.; Menzies T.","Yu, Zhe (57189687898); Theisen, Christopher (57216482869); Williams, Laurie (35565101900); Menzies, Tim (7003835495)","57189687898; 57216482869; 35565101900; 7003835495","Improving Vulnerability Inspection Efficiency Using Active Learning","2021","IEEE Transactions on Software Engineering","47","11","","2401","2420","19","","10.1109/TSE.2019.2949275","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119607779&doi=10.1109%2fTSE.2019.2949275&partnerID=40&md5=bf8415307c5f63548f8587e6711c5372","Software engineers can find vulnerabilities with less effort if they are directed towards code that might contain more vulnerabilities. HARMLESS is an incremental support vector machine tool that builds a vulnerability prediction model from the source code inspected to date, then suggests what source code files should be inspected next. In this way, HARMLESS can reduce the time and effort required to achieve some desired level of recall for finding vulnerabilities. The tool also provides feedback on when to stop (at that desired level of recall) while at the same time, correcting human errors by double-checking suspicious files. This paper evaluates HARMLESS on Mozilla Firefox vulnerability data. HARMLESS found 80, 90, 95, 99 percent of the vulnerabilities by inspecting 10, 16, 20, 34 percent of the source code files. When targeting 90, 95, 99 percent recall, HARMLESS could stop after inspecting 23, 30, 47 percent of the source code files. Even when human reviewers fail to identify half of the vulnerabilities (50 percent false negative rate), HARMLESS could detect 96 percent of the missing vulnerabilities by double-checking half of the inspected files. Our results serve to highlight the very steep cost of protecting software from vulnerabilities (in our case study that cost is, for example, the human effort of inspecting 28,750 × 20% = 5,750 source code files to identify 95 percent of the vulnerabilities). While this result could benefit the mission-critical projects where human resources are available for inspecting thousands of source code files, the research challenge for future work is how to further reduce that cost. The conclusion of this paper discusses various ways that goal might be achieved. © 1976-2012 IEEE.","Active learning; error correction; security; software engineering; vulnerabilities","Codes (symbols); Computer programming languages; Costs; Inspection; Machine tools; Software engineering; Support vector machines; Active Learning; Errors correction; Human errors; Incremental support vector machine; Inspection efficiency; Mozilla firefox; Prediction modelling; Security; Source codes; Vulnerability; Error correction","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85119607779"
"Du Q.; Kuang X.; Zhao G.","Du, Qianjin (58138448800); Kuang, Xiaohui (7006865075); Zhao, Gang (57207402944)","58138448800; 7006865075; 57207402944","Code Vulnerability Detection via Nearest Neighbor Mechanism","2022","Findings of the Association for Computational Linguistics: EMNLP 2022","","","","6202","6207","5","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149847887&partnerID=40&md5=ae3f2fff9ceb2fce10f2fa71b02f41d9","Code vulnerability detection is a fundamental and challenging task in the software security field. Existing research works aim to learn semantic information from the source code by utilizing NLP technologies. However, in vulnerability detection tasks, some vulnerable samples are very similar to non-vulnerable samples, which are difficult to identify. To address this issue and improve detection performance, we introduce the k-nearest neighbor mechanism which retrieves multiple neighbor samples and utilizes label information of retrieved neighbor samples to provide help for model predictions. Besides, we use supervised contrastive learning to make the model learn the discriminative representation and ensure that label information of retrieved neighbor samples is as consistent as possible with the label information of testing samples. Extensive experiments show that our method can achieve obvious performance improvements compared to baseline models. © 2022 Association for Computational Linguistics.","","Computational linguistics; Nearest neighbor search; Detection performance; Detection tasks; Label information; Learn+; Nearest-neighbour; Security fields; Semantics Information; Software security; Source codes; Vulnerability detection; Semantics","Conference paper","Final","","Scopus","2-s2.0-85149847887"
"Kaur I.; Kaur A.","Kaur, Inderpreet (58192959200); Kaur, Arvinder (57548731500)","58192959200; 57548731500","Correlation Between Code Smells for Open Source Java Projects","2022","Lecture Notes in Electrical Engineering","783","","","7","15","8","","10.1007/978-981-16-3690-5_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119821211&doi=10.1007%2f978-981-16-3690-5_2&partnerID=40&md5=9c5581cd638ab9f5e890121e98c9f1cc","The increasing demands and excessive use of computers and applications worldwide makes it challenging for the application and software developers to deliver the product on time and without faults. Looking at applications built for bugs, and thus minimizing the faults becomes tedious. Different researchers have extensively studied the maintenance of software and prediction of vulnerabilities. Code smells, and bug prediction is one way to find and locate the bugs through smell. These smells are beneficial in bug prediction if the smells are known beforehand. This paper studies the correlation for six code smells compiled for three open source java projects. The study results show that three smells are strongly correlated with code smells, namely God class, Brain method, and Dispersed Coupling. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Bug prediction; Code smells; Correlation; Machine learning; Open-source projects","Application programs; Codes (symbols); Forecasting; Java programming language; Object oriented programming; Odors; Open source software; Open systems; Program debugging; Application builds; Application developers; Bug predictions; Code smell; Correlation; Open source projects; Open-source; Software developer; Machine learning","Conference paper","Final","","Scopus","2-s2.0-85119821211"
"Öz I.; Arslan S.","Öz, Işıl (37097877800); Arslan, Sanem (57188688851)","37097877800; 57188688851","Predicting the Soft Error Vulnerability of Parallel Applications Using Machine Learning","2021","International Journal of Parallel Programming","49","3","","410","439","29","","10.1007/s10766-021-00707-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103371927&doi=10.1007%2fs10766-021-00707-0&partnerID=40&md5=fc0012cdc93646cf277203cf10cf4d32","With the widespread use of the multicore systems having smaller transistor sizes, soft errors become an important issue for parallel program execution. Fault injection is a prevalent method to quantify the soft error rates of the applications. However, it is very time consuming to perform detailed fault injection experiments. Therefore, prediction-based techniques have been proposed to evaluate the soft error vulnerability in a faster way. In this work, we present a soft error vulnerability prediction approach for parallel applications using machine learning algorithms. We define a set of features including thread communication, data sharing, parallel programming, and performance characteristics; and train our models based on three ML algorithms. This study uses the parallel programming features, as well as the combination of all features for the first time in vulnerability prediction of parallel programs. We propose two models for the soft error vulnerability prediction: (1) A regression model with rigorous feature selection analysis that estimates correct execution rates, (2) A novel classification model that predicts the vulnerability level of the target programs. We get maximum prediction accuracy rate of 73.2% for the regression-based model, and achieve 89% F-score for our classification model. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Fault injection; Machine Learning; Parallel programming; Soft error analysis","Data Sharing; Error correction; Forecasting; Machine learning; Multicore programming; Parallel programming; Radiation hardening; Regression analysis; Software testing; Classification models; Multi-core systems; Parallel application; Parallel program execution; Performance characteristics; Prediction accuracy; Prediction-based techniques; Regression-based model; Learning algorithms","Article","Final","","Scopus","2-s2.0-85103371927"
"Lyu J.; Bai Y.; Xing Z.; Li X.; Ge W.","Lyu, Jinghui (57292814000); Bai, Yude (57215362079); Xing, Zhenchang (8347413500); Li, Xiaohong (57022407900); Ge, Weimin (57191293942)","57292814000; 57215362079; 8347413500; 57022407900; 57191293942","A Character-Level Convolutional Neural Network for Predicting Exploitability of Vulnerability","2021","Proceedings - 2021 International Symposium on Theoretical Aspects of Software Engineering, TASE 2021","","","","119","126","7","","10.1109/TASE52547.2021.00014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116897421&doi=10.1109%2fTASE52547.2021.00014&partnerID=40&md5=19168c36eb04470aa90f7fa9605be5a3","The continuous discovery of software vulnerabilities have brought great challenges to the cyber security, which will lead to severe systematical or individual losses after being exploited. But the harshly increasing of software vulnerabilities overwhelms the time consuming vulnerability analysis. Security experts must pay more attention to the ones which have the highest priority to be repaired. In general, both severity and exploitability determine the severity of a software vulnerability. Compared with the severity evaluated by the Common Vulnerability Scoring System (CVSS score), the exploitability is still lack of a well-accepted standard. Furthermore, based on the perspective of attack and defense, we found that the exploitability of vulnerabilities is more attractive to hackers so that system or individual is severely affected by the exploitability rather than the severity. In this paper, we propose a deep learning based approach to predict the exploitability of the vulnerability by using the correlated textual description and characteristics. Specifically, our approach takes character-level Convolutional Neural Network (charCNN) to fetch more fine-grained character-level features from the vulnerability description instead of the word-level features considered by the previous literatures. And we highlight the importance of vulnerability characteristics such as Confidentiality Impact, Integrity Impact, Attack Vector etc. during the determination of vulnerability exploitability. Extensive experiments are set to prove the effectiveness of the given charCNN approach through the comparison on both different levels of features and different neural network models. Our approach achieves the best F1 values 93.1% (at least 2.2% more than the baselines). And we also investigate the efficiency of charCNN trained by historical vulnerability when predicting the exploitability of the newly published vulnerabilities. Finally, we further explore the robustness of the proposed model by changing the scale of training sets. For the prediction of vulnerability exploitability, we recommend to adopt 40.0% to 50.0% vulnerabilities to train a robust charCNN model.  © 2021 IEEE.","Deep Learning; Mining Software Repositories; Vulnerability description; Vulnerability Exploitability Prediction","Convolution; Convolutional neural networks; Deep learning; Network security; Personal computing; Character level; Convolutional neural network; Deep learning; Mining software; Mining software repository; Neural network model; Software repositories; Software vulnerabilities; Vulnerability description; Vulnerability exploitability prediction; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85116897421"
"Huang S.-Y.; Wu Y.","Huang, Shin-Ying (55842631300); Wu, Yiju (57219982125)","55842631300; 57219982125","POSTER: Dynamic Software Vulnerabilities Threat Prediction through Social Media Contextual Analysis","2020","Proceedings of the 15th ACM Asia Conference on Computer and Communications Security, ASIA CCS 2020","","","","892","894","2","","10.1145/3320269.3405435","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096377601&doi=10.1145%2f3320269.3405435&partnerID=40&md5=b30ef1a925fc746de5ab7c90069d3a21","Publicly available software vulnerabilities and exploit codes are often utilized by malicious actors to launch cyberattack to vulnerable targets. Therefore, organizations not only need to update their software to the latest version, they need to do effective patch management and also prioritize the patching schedule. In order to prevent future cyber threat based on publicly available resources on the Internet, this study propose a dynamic vulnerability threat assessment model to predict the exploited tendency for each vulnerability (i.e., CVE). The model considers many aspects of vulnerability which are gathered from multiple sources. Features range from profile information to contextual information of Twitter discussion about these vulnerabilities. When applied to predict the vulnerabilities exploitation in real world data, it showed better prediction accuracy using our approach and has been deployed into our threat intelligence platform as one of the analytic functions.  © 2020 Owner/Author.","machine learning; social media vulnerability mentions; topic modeling; vulnerability exploit prediction","Forecasting; Security of data; Social networking (online); Analytic functions; Contextual analysis; Contextual information; Dynamic softwares; Prediction accuracy; Software vulnerabilities; Threat assessment models; Threat prediction; Computer software","Conference paper","Final","","Scopus","2-s2.0-85096377601"
"Ivaki N.; Antunes N.","Ivaki, Naghmeh (55556863000); Antunes, Nuno (57217858593)","55556863000; 57217858593","SIDE: Security-aware Integrated Development Environment","2020","Proceedings - 2020 IEEE 31st International Symposium on Software Reliability Engineering Workshops, ISSREW 2020","","","9307743","149","150","1","","10.1109/ISSREW51248.2020.00056","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099795220&doi=10.1109%2fISSREW51248.2020.00056&partnerID=40&md5=392cd8835ccbb969493598b24c9f1862","An effective way for building secure software is to embed security into software in the early stages of software development. Thus, we aim to study several evidences of code anomalies introduced during the software development phase, that may be indicators of security issues in software, such as code smells, structural complexity represented by diverse software metrics, the issues detected by static code analysers, and finally missing security best practices. To use such evidences for vulnerability prediction and removal, we first need to understand how they are correlated with security issues. Then, we need to discover how these imperfect raw data can be integrated to achieve a reliable, accurate and valuable decision about a portion of code. Finally, we need to construct a security actuator providing suggestions to the developers to remove or fix the detected issues from the code. All of these will lead to the construction of a framework, including security monitoring, security analyzer, and security actuator platforms, that are necessary for a security-aware integrated development environment (SIDE).  © 2020 IEEE.","Best Practices; Code Smells; Integrated Development Environment; Software Metrics; Software Security","Actuators; Odors; Best practices; Code smell; Integrated development environment; Secure software; Security issues; Security-aware; Software metrics; Software security; Static codes; Structural complexity; Software design","Conference paper","Final","","Scopus","2-s2.0-85099795220"
"Bhatt N.; Anand A.; Yadavalli V.S.S.","Bhatt, Navneet (57203618455); Anand, Adarsh (56647624300); Yadavalli, V.S.S. (8883072700)","57203618455; 56647624300; 8883072700","Exploitability prediction of software vulnerabilities","2021","Quality and Reliability Engineering International","37","2","","648","663","15","","10.1002/qre.2754","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090439118&doi=10.1002%2fqre.2754&partnerID=40&md5=9122ae28ae4b08e0abb1e9cabd00935c","The number of security failure discovered and disclosed publicly are increasing at a pace like never before. Wherein, a small fraction of vulnerabilities encountered in the operational phase are exploited in the wild. It is difficult to find vulnerabilities during the early stages of software development cycle, as security aspects are often not known adequately. To counter these security implications, firms usually provide patches such that these security flaws are not exploited. It is a daunting task for a security manager to prioritize patches for vulnerabilities that are likely to be exploitable. This paper fills this gap by applying different machine learning techniques to classify the vulnerabilities based on previous exploit-history. Our work indicates that various vulnerability characteristics such as severity, type of vulnerabilities, different software configurations, and vulnerability scoring parameters are important features to be considered in judging an exploit. Using such methods, it is possible to predict exploit-prone vulnerabilities with an accuracy >85%. Finally, with this experiment, we conclude that supervised machine learning approach can be a useful technique in predicting exploit-prone vulnerabilities. © 2020 John Wiley & Sons Ltd.","exploits; machine learning; patches; security; vulnerability","Forecasting; Learning systems; Software design; Supervised learning; Important features; Machine learning techniques; Operational phase; Security implications; Software configuration; Software development cycles; Software vulnerabilities; Supervised machine learning; Security of data","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85090439118"
"Bhandari G.; Naseer A.; Moonen L.","Bhandari, Guru (57226407995); Naseer, Amara (57216947089); Moonen, Leon (7003285889)","57226407995; 57216947089; 7003285889","CVEfixes: Automated collection of vulnerabilities and their fixes from open-source software","2021","PROMISE 2021 - Proceedings of the 17th International Conference on Predictive Models and Data Analytics in Software Engineering, co-located with ESEC/FSE 2021","","","","30","39","9","","10.1145/3475960.3475985","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113649225&doi=10.1145%2f3475960.3475985&partnerID=40&md5=587cce7d3192fade582f1bfb832c322f","Data-driven research on the automated discovery and repair of security vulnerabilities in source code requires comprehensive datasets of real-life vulnerable code and their fixes. To assist in such research, we propose a method to automatically collect and curate a comprehensive vulnerability dataset from Common Vulnerabilities and Exposures (CVE) records in the National Vulnerability Database (NVD). We implement our approach in a fully automated dataset collection tool and share an initial release of the resulting vulnerability dataset named CVEfixes. The CVEfixes collection tool automatically fetches all available CVE records from the NVD, gathers the vulnerable code and corresponding fixes from associated open-source repositories, and organizes the collected information in a relational database. Moreover, the dataset is enriched with meta-data such as programming language, and detailed code and security metrics at five levels of abstraction. The collection can easily be repeated to keep up-to-date with newly discovered or patched vulnerabilities. The initial release of CVEfixes spans all published CVEs up to 9 June 2021, covering 5365 CVE records for 1754 open-source projects that were addressed in a total of 5495 vulnerability fixing commits. CVEfixes supports various types of data-driven software security research, such as vulnerability prediction, vulnerability classification, vulnerability severity prediction, analysis of vulnerability-related code changes, and automated vulnerability repair.  © 2021 Owner/Author.","dataset; Security vulnerabilities; software repository mining; source code repair; vulnerability classification; vulnerability prediction","Automation; Open systems; Predictive analytics; Automated collection; Common vulnerabilities and exposures; Levels of abstraction; National vulnerability database; Open source projects; Open source repositories; Security vulnerabilities; Vulnerability classifications; Open source software","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85113649225"
"Tang G.; Meng L.; Wang H.; Ren S.; Wang Q.; Yang L.; Cao W.","Tang, Gaigai (57215544716); Meng, Lianxiao (57215545789); Wang, Huiqiang (54790335900); Ren, Shuangyin (57220888025); Wang, Qiang (58114321200); Yang, Lin (57945734100); Cao, Weipeng (57195309486)","57215544716; 57215545789; 54790335900; 57220888025; 58114321200; 57945734100; 57195309486","A Comparative Study of Neural Network Techniques for Automatic Software Vulnerability Detection","2020","Proceedings - 2020 International Symposium on Theoretical Aspects of Software Engineering, TASE 2020","","","9405265","1","8","7","","10.1109/TASE49443.2020.00010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105412850&doi=10.1109%2fTASE49443.2020.00010&partnerID=40&md5=d032dad7b596aeaf1ab7ed3377f62111","Software vulnerabilities are usually caused by design flaws or implementation errors, which could be exploited to cause damage to the security of the system. At present, the most commonly used method for detecting software vulnerabilities is static analysis. Most of the related technologies work based on rules or code similarity (source code level) and rely on manually defined vulnerability features. However, these rules and vulnerability features are difficult to be defined and designed accurately, which makes static analysis face many challenges in practical applications. To alleviate this problem, some researchers have proposed to use neural networks that have the ability of automatic feature extraction to improve the intelligence of detection. However, there are many types of neural networks, and different data preprocessing methods will have a significant impact on model performance. It is a great challenge for engineers and researchers to choose a proper neural network and data preprocessing method for a given problem. To solve this problem, we have conducted extensive experiments to test the performance of the two most typical neural networks (i.e., Bi-LSTM and RVFL) with the two most classical data preprocessing methods (i.e., the vector representation and the program symbolization methods) on software vulnerability detection problems and obtained a series of interesting research conclusions, which can provide valuable guidelines for researchers and engineers. Specifically, we found that 1) the training speed of RVFL is always faster than Bi-LSTM, but the prediction accuracy of Bi-LSTM model is higher than RVFL; 2) using doc2vec for vector representation can make the model have faster training speed and generalization ability than using word2vec; and 3) multi-level symbolization is helpful to improve the precision of neural network models. © 2020 IEEE.","Bi-LSTM; machine learning; neural network; RVFL; software vulnerability","Feature extraction; Network security; Software testing; Static analysis; Automatic feature extraction; Comparative studies; Generalization ability; Implementation error; Neural network model; Neural network techniques; Software vulnerabilities; Vector representations; Long short-term memory","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85105412850"
"Wang J.; Huang M.; Nie Y.; Li J.","Wang, Jingjing (57237621400); Huang, Minhuan (14631981500); Nie, Yuanping (56047562300); Li, Jin (57209630146)","57237621400; 14631981500; 56047562300; 57209630146","Static Analysis of Source Code Vulnerability Using Machine Learning Techniques: A Survey","2021","2021 4th International Conference on Artificial Intelligence and Big Data, ICAIBD 2021","","","9459075","76","86","10","","10.1109/ICAIBD51990.2021.9459075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113763247&doi=10.1109%2fICAIBD51990.2021.9459075&partnerID=40&md5=c7b0dc9148548f7213785a861ee0889b","With the rapid increase of practical problem complexity and code scale, the threat of software security is increasingly serious. Consequently, it is crucial to pay attention to the analysis of software source code vulnerability in the development stage and take efficient measures to detect the vulnerability as soon as possible. Machine learning techniques have made remarkable achievements in various fields. However, the application of machine learning in the domain of vulnerability static analysis is still in its infancy and the characteristics and performance of diverse methods are quite different. In this survey, we focus on a source code-oriented static vulnerability analysis method using machine learning techniques. We review the studies on source code vulnerability analysis based on machine learning in the past decade. We systematically summarize the development trends and different technical characteristics in this field from the perspectives of the intermediate representation of source code and vulnerability prediction model and put forward several feasible research directions in the future according to the limitations of the current approaches. © 2021 IEEE.","machine learning; software security; source code vulnerability; static analysis","Big data; Computer programming languages; Machine learning; Predictive analytics; Surveys; Development stages; Development trends; Intermediate representations; Machine learning techniques; Practical problems; Software security; Software source codes; Vulnerability analysis; Static analysis","Conference paper","Final","","Scopus","2-s2.0-85113763247"
"Ferenc R.; Viszkok T.; Aladics T.; Jász J.; Hegedűs P.","Ferenc, Rudolf (6603559878); Viszkok, Tamás (57217150009); Aladics, Tamás (57217147140); Jász, Judit (8534868000); Hegedűs, Péter (25926433300)","6603559878; 57217150009; 57217147140; 8534868000; 25926433300","Deep-water framework: The Swiss army knife of humans working with machine learning models","2020","SoftwareX","12","","100551","","","","","10.1016/j.softx.2020.100551","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086427001&doi=10.1016%2fj.softx.2020.100551&partnerID=40&md5=b6f6e179b37ca5b00fed0fce1f578388","Working with machine learning models has become an everyday task not only for software engineers, but for a much wider spectrum of researchers and professionals. Training such models involves finding the best learning methods and their best hyper-parameters for a specific task, keeping track of the achieved performance measures, comparing the results visually, etc. If we add feature extraction methods – that precede the learning phase and depend on many hyper-parameters themselves – into the mixture, like source code embedding that is quite common in the field of software analysis, the task cries out for supporting tools. We propose a framework called Deep-Water that works similarly to a configuration management tool in the area of software engineering. It supports defining arbitrary feature extraction and learning methods for an input dataset and helps in executing all the training tasks with different hyper-parameters in a distributed manner. The framework stores all circumstances, parameters and results of training, which can be filtered and visualized later. We successfully used the tool in several software analysis based prediction tasks, like vulnerability or bug prediction, but it is general enough to be applicable in other areas as well, e.g. NLP, image processing, or even other non-IT fields. © 2020 The Authors","Data visualization; Deep-learning; Machine learning; Model management; Software analysis","Deep learning; Extraction; Feature extraction; Image processing; Software engineering; Bug predictions; Configuration management tools; Feature extraction methods; Learning methods; Machine learning models; Performance measure; Prediction tasks; Software analysis; Learning systems","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85086427001"
"Sangodoyin A.O.; Akinsolu M.O.; Pillai P.; Grout V.","Sangodoyin, Abimbola O. (57201212374); Akinsolu, Mobayode O. (56964309900); Pillai, Prashant (7102162572); Grout, Vic (20433449000)","57201212374; 56964309900; 7102162572; 20433449000","Detection and Classification of DDoS Flooding Attacks on Software-Defined Networks: A Case Study for the Application of Machine Learning","2021","IEEE Access","9","","9526586","122495","122508","13","","10.1109/ACCESS.2021.3109490","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114795209&doi=10.1109%2fACCESS.2021.3109490&partnerID=40&md5=c76de2a2727d3551f4e3d1fee9a6d1ae","Software-defined networks (SDNs) offer robust network architectures for current and future Internet of Things (IoT) applications. At the same time, SDNs constitute an attractive target for cyber attackers due to their global network view and programmability. One of the major vulnerabilities of typical SDN architectures is their susceptibility to Distributed Denial of Service (DDoS) flooding attacks. DDoS flooding attacks can render SDN controllers unavailable to their underlying infrastructure, causing service disruption or a complete outage in many cases. In this paper, machine learning-based detection and classification of DDoS flooding attacks on SDNs is investigated using popular machine learning (ML) algorithms. The ML algorithms, classifiers and methods investigated are quadratic discriminant analysis (QDA), Gaussian Naïve Bayes (GNB), k -nearest neighbor (k-NN), and classification and regression tree (CART). The general principle is illustrated through a case study, in which, experimental data (i.e. jitter, throughput, and response time metrics) from a representative SDN architecture suitable for typical mid-sized enterprise-wide networks is used to build classification models that accurately identify and classify DDoS flooding attacks. The SDN model used was emulated in Mininet and the DDoS flooding attacks (i.e. hypertext transfer protocol (HTTP), transmission control protocol (TCP), and user datagram protocol (UDP) attacks) have been launched on the SDN model using low orbit ion cannon (LOIC). Although all the ML methods investigated show very good efficacy in detecting and classifying DDoS flooding attacks, CART demonstrated the best performance on average in terms of prediction accuracy (98%), prediction speed ( 5.3,,{times },,10{5} observations per second), training time (12.4 ms), and robustness. © 2013 IEEE.","DDoS flooding attack; machine learning; network security; SDN security","Application programs; Denial-of-service attack; Discriminant analysis; Floods; HTTP; Hypertext systems; Internet of things; Nearest neighbor search; Network architecture; Network security; Software defined networking; Transmission control protocol; Classification and regression tree; Classification models; Distributed denial of service; Enterprise-wide network; Prediction accuracy; Quadratic discriminant analysis; Service disruptions; User datagram protocol; Machine learning","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85114795209"
"Mehta P.; Thaker T.P.","Mehta, Payal (57214890534); Thaker, T.P. (37054675700)","57214890534; 37054675700","Development of Empirical Correlation Between Standard Penetration Test and Shear Wave Velocity","2021","Lecture Notes in Civil Engineering","138","","","483","496","13","","10.1007/978-981-33-6564-3_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106407472&doi=10.1007%2f978-981-33-6564-3_41&partnerID=40&md5=30e6aa33208243e0ef8bb5b0e7deb512","Bhuj 2001 earthquake was alarmed for the seismic hazard assessment of different regions of the country. Detailed geotechnical and geophysical study provide important inputs for the seismic microzonation study. In the present paper, the correlation has been developed between standard penetration test number and shear wave velocity for different categories of soil in Vadodara city. The shear wave velocity models have been obtained through multichannel analysis of surface wave (MASW), which is an effective technique for geophysical site characterization. The field program is set in 2 km × 2 km grid size covering 67 test locations in the entire Vadodara city. The test data are analyzed through the package of SeisImager/SW software. Further, 430 borelog data were collected from various private and government agencies for the investigation of soil properties. Normalized consistency ratio and residual plots have been generated for the validation of the predicted correlations. These correlations are also compared with the relations developed by other researchers. The capability of the proposed correlation is also checked by plotting the map of the scaled relative error versus cumulative frequency. It is found that developed correlations give good prediction performance. The proposed correlations will be the advantageous for the dense locations of the busy city where testing is not feasible and economical. These empirical relations will be further used for similar geotechnical and geological site conditions after proper validation. The proposed correlations can be further used for the ground response analysis and vulnerability assessment of the study region. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Correlation; Seismic hazard; Shear wave velocity; Standard penetration test number","Acoustic wave velocity; Earthquakes; Shear waves; Software testing; Soil testing; Surface waves; Wave propagation; Cumulative frequencies; Ground response analysis; Multi-channel analysis of surface waves; Prediction performance; Seismic hazard assessment; Shear wave velocity models; Standard penetration test; Vulnerability assessments; Shear flow","Conference paper","Final","","Scopus","2-s2.0-85106407472"
"Siavvas M.; Kehagias D.; Tzovaras D.; Gelenbe E.","Siavvas, Miltiadis (57194500913); Kehagias, Dionysios (7003972544); Tzovaras, Dimitrios (13105681700); Gelenbe, Erol (7006026729)","57194500913; 7003972544; 13105681700; 7006026729","A hierarchical model for quantifying software security based on static analysis alerts and software metrics","2021","Software Quality Journal","29","2","","431","507","76","","10.1007/s11219-021-09555-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106207514&doi=10.1007%2fs11219-021-09555-0&partnerID=40&md5=b0ecf71985d4ef4c10438eabe73da349","Despite the acknowledged importance of quantitative security assessment in secure software development, current literature still lacks an efficient model for measuring internal software security risk. To this end, in this paper, we introduce a hierarchical security assessment model (SAM), able to assess the internal security level of software products based on low-level indicators, i.e., security-relevant static analysis alerts and software metrics. The model, following the guidelines of ISO/IEC 25010, and based on a set of thresholds and weights, systematically aggregates these low-level indicators in order to produce a high-level security score that reflects the internal security level of the analyzed software. The proposed model is practical, since it is fully automated and operationalized in the form of a standalone tool and as part of a broader Computer-Aided Software Engineering (CASE) platform. In order to enhance its reliability, the thresholds of the model were calibrated based on a repository of 100 popular software applications retrieved from Maven Repository. Furthermore, its weights were elicited in a way to chiefly reflect the knowledge expressed by the Common Weakness Enumeration (CWE), through a novel weights elicitation approach grounded on popular decision-making techniques. The proposed model was evaluated on a large repository of 150 open-source software applications retrieved from GitHub and 1200 classes retrieved from the OWASP Benchmark. The results of the experiments revealed the capacity of the proposed model to reliably assess internal security at both product level and class level of granularity, with sufficient discretion power. They also provide preliminary evidence for the ability of the model to be used as the basis for vulnerability prediction. To the best of our knowledge, this is the first fully automated, operationalized and sufficiently evaluated security assessment model in the modern literature. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Security Assessment; Software Quality Evaluation; Software Security","Application programs; Benchmarking; Computer aided software engineering; Decision making; Gages; Hierarchical systems; Open source software; Open systems; Risk assessment; Software design; Software reliability; Hierarchical model; Internal security; Quantitative security assessments; Secure software development; Security assessment; Software applications; Software products; Software security; Static analysis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85106207514"
"Trisolini M.; Lewis H.G.; Colombo C.","Trisolini, Mirko (57191593854); Lewis, Hugh G. (7201755709); Colombo, Camilla (16052232100)","57191593854; 7201755709; 16052232100","Predicting the vulnerability of spacecraft components: Modelling debris impact effects through vulnerable-zones","2020","Advances in Space Research","65","11","","2692","2710","18","","10.1016/j.asr.2020.03.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083044696&doi=10.1016%2fj.asr.2020.03.010&partnerID=40&md5=e3fb5ea3f1ae5c53f21ba5ae8de30597","The space environment around the Earth is populated by more than 130 million objects of 1 mm in size and larger, and future predictions shows that this amount is destined to increase, even if mitigation measures are implemented at a far better rate than today. These objects can hit and damage a spacecraft or its components. It is thus necessary to assess the risk level for a satellite during its mission lifetime. Few software packages perform this analysis, and most of them employ time-consuming ray-tracing methodology, where particles are randomly sampled from relevant distributions. In addition, they tend not to consider the risk associated with the secondary debris clouds. The paper presents the development of a vulnerability assessment model, which relies on a fully statistical procedure: the debris fluxes are directly used combining them with the concept of vulnerable zone, avoiding the random sampling the debris fluxes. A novel methodology is presented to predict damage on internal components. It models the interaction between the components and the secondary debris cloud through basic geometrical operations, considering mutual shielding and shadowing between internal components. The methodologies are tested against state-of-the-art software for relevant test cases, comparing results on external structures and internal components. © 2020 COSPAR","Debris cloud; Debris impact; Penetration probability; Space debris; Spacecraft vulnerability; Vulnerable zone","Debris; Earth (planet); Forecasting; Particle size analysis; Ray tracing; Sampling; Software testing; Future predictions; Geometrical operations; Mitigation measures; Novel methodology; Space environment; Spacecraft components; State of the art; Vulnerability assessments; Risk assessment","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85083044696"
"Salimi S.; Ebrahimzadeh M.; Kharrazi M.","Salimi, Solmaz (57215524569); Ebrahimzadeh, Maryam (57220206634); Kharrazi, Mehdi (6701798485)","57215524569; 57220206634; 6701798485","Improving real-world vulnerability characterization with vulnerable slices","2020","PROMISE 2020 - Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering, Co-located with ESEC/FSE 2020","","","","11","20","9","","10.1145/3416508.3417120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097300247&doi=10.1145%2f3416508.3417120&partnerID=40&md5=bb2b6d97b2e98cd6e0741308c36a04cd","Vulnerability detection is an important challenge in the security community. Many different techniques have been proposed, ranging from symbolic execution to fuzzing in order to help in identifying vulnerabilities. Even though there has been considerable improvement in these approaches, they perform poorly on a large scale code basis. There has also been an alternate approach, where software metrics are calculated on the overall code structure with the hope of predicting code segments more likely to be vulnerable. The logic has been that more complex code with respect to the software metrics, will be more likely to contain vulnerabilities. In this paper, we conduct an empirical study with a large dataset of vulnerable codes to discuss if we can change the way we measure metrics to improve vulnerability characterization. More specifically, we introduce vulnerable slices as vulnerable code units to measure the software metrics and then use these new measured metrics to characterize vulnerable codes. The result shows that vulnerable slices significantly increase the accuracy of vulnerability characterization. Further, we utilize vulnerable slices to analyze the dataset of known vulnerabilities, particularly to observe how by using vulnerable slices the size and complexity changes in real-world vulnerabilities. © 2020 ACM.","Program Slicing; Static Analysis; Vulnerability Characterization; Vulnerability Prediction","Large dataset; Predictive analytics; Alternate approaches; Code segments; Code structure; Empirical studies; Security community; Software metrics; Symbolic execution; Vulnerability detection; Software engineering","Conference paper","Final","","Scopus","2-s2.0-85097300247"
"Gencer K.; Başçiftçi F.","Gencer, K. (57218318170); Başçiftçi, F. (7004277227)","57218318170; 7004277227","Time series forecast modeling of vulnerabilities in the android operating system using ARIMA and deep learning methods","2021","Sustainable Computing: Informatics and Systems","30","","100515","","","","","10.1016/j.suscom.2021.100515","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099829450&doi=10.1016%2fj.suscom.2021.100515&partnerID=40&md5=3ba8705747bbb50cead684e7d084ab27","Security vulnerability prediction models allow estimation of the number of potential vulnerabilities and evaluation of the risks caused by these vulnerabilities. In particular, for modeling the vulnerabilities that may occur in software versions over time, it is appropriate to take the necessary countermeasures. These models are crucial in areas such as determining the number of resources required to cope with security vulnerabilities. These reported vulnerabilities, we anticipate the actions of OS companies to make strategic and operational decisions such as secure deployment. The operating system includes backup provisioning, disaster recovery. Although many vulnerability predictions models have been constructed, most of these are for operating systems and internet browsers, and non-exist for the Android mobile operating system, which has the highest number of users. In contrast to other studies, the present study investigated Android vulnerabilities that directly depend on time. Time series, multilayer perceptron (MLP), convolutional neural network (CNN), long short term memory (LSTM), Convolutional LSTM (ConvLSTM) and CNN-LSTM based models were generated, and the best model, providing the lowest error rates for the prediction of future security vulnerabilities, was selected. Data for the creation of the models were obtained by filtering security vulnerabilities published in the National Vulnerability Database (NVD) using the keyword Android. It was observed that the LSTM model has an error rate of 26.830 and the ARIMA model has an error rate of 18.449. Finally, it has been determined that LSTM based algorithms reach error rates that can compete with classical time series models despite limited data. © 2021 Elsevier Inc.","Android vulnerabilities; LSTM; NVD; Software security; Time series; Vulnerability discovery model","Android (operating system); Autoregressive moving average model; Convolution; Convolutional neural networks; Deep learning; Errors; Forecasting; Learning systems; Long short-term memory; Multilayer neural networks; Predictive analytics; Risk perception; Security of data; Time series; Android mobile operating systems; Multi layer perceptron; National vulnerability database; Operational decisions; Security vulnerabilities; Software versions; Time series forecasts; Time series models; Mobile security","Article","Final","","Scopus","2-s2.0-85099829450"
"Scacco J.; Milani G.; Lourenço P.B.","Scacco, J. (57210639953); Milani, G. (9741778200); Lourenço, P.B. (7004615647)","57210639953; 9741778200; 7004615647","A micro-modeling approach for the prediction of TRM bond performance on curved masonry substrates","2021","Composite Structures","256","","113065","","","","","10.1016/j.compstruct.2020.113065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093644326&doi=10.1016%2fj.compstruct.2020.113065&partnerID=40&md5=e4ceeea151123c22ab1541dd56ecf387","Retrofitting of arches, vaults and domes is needed for vulnerability mitigation of historical structures. Lately, the external application of composite materials embedded within an inorganic matrix addressed the compatibility issues related to FRP applications. Great efforts have been made for a correct identification of tensile and bond properties of TRM reinforcement, but scarce data are available for understanding the effect of curvature in debonding. This paper provides a numerical approach that enables to predict the reinforcement performance on curved masonry elements, when the same is designed without having specific single lap shear tests for the adopted materials. Such an approach relies on a separate modeling of each constituent material of the reinforcement and the substrate: matrix, fibers and the interface between them, and bricks and mortar joints. The FE-based software Abaqus is used for the simulations, addressing the non-linear material properties by means of the CDP constitutive model. In order to calibrate and validate the proposed numerical approach, experimental data are used and through a comparison of the load–displacement curves obtained the capabilities of the method are discussed. Finally, an extension of such approach to curvatures not experimentally tested is presented, opening the way to future applications of the approach. © 2020 Elsevier Ltd","Curved elements; Masonry; Non-linear analysis; TRM bond","ABAQUS; Masonry materials; Matrix algebra; Numerical methods; Substrates; Constituent materials; Effect of curvature; Future applications; Historical structures; Nonlinear materials; Numerical approaches; Single-lap shear tests; Vulnerability mitigation; Reinforcement","Article","Final","","Scopus","2-s2.0-85093644326"
"Zheng W.; Chen J.-Z.; Wu X.-X.; Chen X.; Xia X.","Zheng, Wei (56650413300); Chen, Jun-Zheng (57208836270); Wu, Xiao-Xue (56650282600); Chen, Xiang (57189091783); Xia, Xin (54586248800)","56650413300; 57208836270; 56650282600; 57189091783; 54586248800","Empirical Studies on Deep-learning-based Security Bug Report Prediction Methods; [基于深度学习的安全缺陷报告预测方法实证研究]","2020","Ruan Jian Xue Bao/Journal of Software","31","5","","1294","1313","19","","10.13328/j.cnki.jos.005954","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086640468&doi=10.13328%2fj.cnki.jos.005954&partnerID=40&md5=5eea732cfa79be807ab40e9c41475d03","The occurrence of software security issues can cause serious consequences in most cases. Early detection of security issues is one of the key measures to prevent security incidents. Security bug report prediction (SBR) can help developers identify hidden security issues in the bug tracking system and fix them as early as possible. However, since the number of security bug reports in real software projects is small, and the features are complex (i.e., there are many types of security vulnerabilities with different types of features), this makes the manual extraction of security features relatively difficult and lead to low accuracy of security bug report prediction with traditional machine learning classification algorithms. To solve this problem, a deep-learning-based security bug report prediction method is proposed. The text mining models TextCNN and TextRNN via deep learning are used to construct security bug report prediction models. For extracting textual features of security bug reports, the Skip-Gram method is used to construct a word embedding matrix. The constructed model has been empirically evaluated on five classical security bug report datasets with different scales. The results show that the deep learning model is superior to the traditional machine learning classification algorithm in 80% of the experimental cases, and the performance of the constructed models can improve 0.258 on average and 0.535 at most in terms of F1-score performance measure. Furthermore, different re-sampling strategies are applied to deal with class imbalance problem in gathered SBR prediction datasets, and the experiment results are discussed. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.","Deep learning; Security bug; Security bug report prediction; Text mining","Forecasting; Learning algorithms; Learning systems; Program debugging; Text mining; Bug tracking system; Class imbalance problems; Embedding matrices; Machine learning classification; Performance measure; Prediction methods; Security features; Security vulnerabilities; Deep learning","Article","Final","","Scopus","2-s2.0-85086640468"
"Jankovic M.; Siavvas M.; Tsoukalas D.; Kehagias D.","Jankovic, Marija (56800124800); Siavvas, Miltiadis (57194500913); Tsoukalas, Dimitrios (57208865760); Kehagias, Dionysios (7003972544)","56800124800; 57194500913; 57208865760; 7003972544","Evaluating and improving the internal security of OPC-UA based software applications","2020","CEUR Workshop Proceedings","2900","","","","","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109771535&partnerID=40&md5=c309fdae2db6f34f160caf545340ffcf","High complexity, extensibility, and interconnectivity of industry 4.0 software systems introduce critical software security issues. Open Platform Communications Unified Architecture (OPC-UA) standard specification highlights the need to provide adequate support for the implementation of Confidentiality, Integrity, and Availability (CIA triad) aspects. However, developers and engineers often overlook these critical security concerns, leading to software systems bundled with severe vulnerabilities. The exploitation of a single vulnerability may lead to far-reaching consequences to the compromised software's owing enterprise. Measuring and evaluating software security is crucial for secure software development. The paper gives the theoretical and technical background of the Quantitative Security Assessment and Vulnerability Prediction services, which are part of the SDK4ED Dependability toolbox. Moreover, it presents the results of the performed security evaluation of an OPC-UA based open-source application. Finally, it discusses the refactoring recommendations on the source-code level, leading to improved security. © 2020 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org)","OPC-UA; Security assessment; Static analysis; Vulnerability prediction","Interoperability; Open source software; Software design; Open source application; Quantitative security assessments; Secure software development; Single vulnerabilities; Software applications; Standard specifications; Technical background; Unified architecture; Application programs","Conference paper","Final","","Scopus","2-s2.0-85109771535"
"Tyagi H.; Kumar R.","Tyagi, Himani (57218498393); Kumar, Rajendra (57306930000)","57218498393; 57306930000","Analyzing Security Approaches for Threats, Vulnerabilities, and attacks in an IoT Environment","2021","2021 International Conference on Computational Performance Evaluation, ComPE 2021","","","","227","233","6","","10.1109/ComPE53109.2021.9752151","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128968834&doi=10.1109%2fComPE53109.2021.9752151&partnerID=40&md5=13aeb1ca3517a654423f77b248962cac","The importance of the Internet of things in every sphere of human life is quite evident. However, with applications, many serious threats, vulnerabilities, and attacks are emerging from time to time. Thus, it is required to discuss the vulnerabilities and attacks to fully adopt this popular technology with security solutions intact. Therefore, this paper includes a survey from three aspects such as IoT market opportunities with security challenges, recently identified threats, vulnerabilities, and attacks on IoT with proposed solutions, and the importance of modern technologies such as machine learning, cloud computing, fog computing, edge computing and, blockchain for IoT security solutions. The main contribution of this work is to provide insights into IoT security challenges from various aspects like device and sensor based, software - application based, communication channel based, and future predictions.  © 2021 IEEE.","attacks; blockchain; cloud computing; edge computing; fog computing; Internet of Things; machine learning; vulnerabilities","Application programs; Blockchain; Cloud security; Fog; Fog computing; Internet of things; Machine learning; Attack; Block-chain; Cloud-computing; Edge computing; Human lives; Market opportunities; Security approach; Security challenges; Security solutions; Vulnerability; Edge computing","Conference paper","Final","","Scopus","2-s2.0-85128968834"
"Song S.; Wu Q.; Zheng X.; Wang P.; Dou Y.; Li Z.; Zhai L.","Song, Shiwen (57611470400); Wu, Qiong (57613472000); Zheng, Xin (57745542000); Wang, Peng (58401609900); Dou, Yuchen (57439888400); Li, Zhongwen (58406482000); Zhai, Lidong (23976947700)","57611470400; 57613472000; 57745542000; 58401609900; 57439888400; 58406482000; 23976947700","Focus on the Stability of Large Systems: Toward Automatic Prediction and Analysis of Vulnerability Threat Intelligence","2021","Proceedings - 2021 IEEE 6th International Conference on Data Science in Cyberspace, DSC 2021","","","","445","449","4","","10.1109/DSC53577.2021.00070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128738290&doi=10.1109%2fDSC53577.2021.00070&partnerID=40&md5=aa99c7b0a532218f397a3114468d0e9d","With the increase in the number of users and business volume, the business systems of Internet companies are becoming more and more complex, resulting in a surge in the number of alarms. A large number of dirty alarms add a huge workload to security operations, which indirectly pose a large number of threats to business systems. At present, most systems use the method of accessing third-party Threat Intelligence to assist operators to realize automatic handling of alarms. However, this method has lagging and accuracy problems, making this work always difficult to meet the requirements of fast and accurate. This article proposes a new method for gathering vulnerability Threat Intelligence, which can obtain vulnerability information in advance of security announcements issued by security vendors. By analyzing the vulnerability disclosure process, this method obtains vulnerability information from the original source submitted by open source mail group, of developers. We used NLP technology and XGBoost model to automatically analyze the vulnerability information, and finally generate FINTEL. The experimental result shows that this method has an accuracy of 93%, and can obtain vulnerability information 10h to 7 days before security vendors release. The scope of application covers all open source code repositories and some closed source repositories.  © 2021 IEEE.","","Open source software; Open systems; Accuracy problems; Automatic analysis; Automatic handling; Automatic prediction; Business systems; Large system; Prediction and analysis; Security operations; System use; Third parties; Alarm systems","Conference paper","Final","","Scopus","2-s2.0-85128738290"
"Malhotra R.; Vidushi","Malhotra, Ruchika (15758058000); Vidushi (57215669647)","15758058000; 57215669647","Severity Prediction of Software Vulnerabilities Using Textual Data","2021","Advances in Intelligent Systems and Computing","1245","","","453","464","11","","10.1007/978-981-15-7234-0_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096410139&doi=10.1007%2f978-981-15-7234-0_41&partnerID=40&md5=ce74b04d2fa0a433ed117aac23a0845f","Nowadays, all essential activities carried out by society are dependent on software systems. These systems with time have become more and more complex. Increase in complexity leads to introduction of vulnerabilities in the software system. Vulnerabilities are being reported every year and are increasing exponentially with time. These vulnerabilities required to be patched and fixed before getting exploited. To achieve this goal, textual data available on vulnerabilities is needed to be exploited to retrieve valuable information from it. Therefore, in this paper, we aim to develop a prediction model which will take textual description of Apache Tomcat vulnerabilities as input and will predict the severity of the vulnerabilities. Text mining techniques and machine learning algorithms are used to perform this task. The huge volume of textual data available on vulnerabilities has to be prioritized so that more severe vulnerabilities can be targeted first and limited resources can be put to its best use. Hence, it is essential to use text mining techniques for feature selection to reduce the volume of the data. The model being developed uses chi-square and information gain for feature selection, and among the machine learning algorithm, bagging technique, random forest, naïve Bayes, support vector machine are used for prediction. From results, it is observed that comparatively information gain gave better results among feature selection technique. Model based on naïve Bayes algorithm along with information gain as feature selection techniques performed good with the average values of 0.91, 90.90%, and 92.30%, respectively, for AUC, sensitivity, specificity, respectively. © 2021, Springer Nature Singapore Pte Ltd.","Feature selection; Machine learning; Prediction model; Text mining; Vulnerability","Computer software; Decision trees; Feature extraction; Forecasting; Internet of things; Learning systems; Predictive analytics; Smart city; Support vector machines; Text mining; Bayes algorithms; Information gain; Prediction model; Selection techniques; Software systems; Software vulnerabilities; Text mining techniques; Textual description; Learning algorithms","Conference paper","Final","","Scopus","2-s2.0-85096410139"
"Filus K.; Boryszko P.; Domańska J.; Siavvas M.; Gelenbe E.","Filus, Katarzyna (57217079434); Boryszko, Paweł (57221331159); Domańska, Joanna (16038785400); Siavvas, Miltiadis (57194500913); Gelenbe, Erol (7006026729)","57217079434; 57221331159; 16038785400; 57194500913; 7006026729","Efficient feature selection for static analysis vulnerability prediction","2021","Sensors (Switzerland)","21","4","1133","1","25","24","","10.3390/s21041133","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100490591&doi=10.3390%2fs21041133&partnerID=40&md5=63a8b7be9a761303be22a297e15bd4a3","Common software vulnerabilities can result in severe security breaches, financial losses, and reputation deterioration and require research effort to improve software security. The acceleration of the software production cycle, limited testing resources, and the lack of security expertise among programmers require the identification of efficient software vulnerability predictors to highlight the system components on which testing should be focused. Although static code analyzers are often used to improve software quality together with machine learning and data mining for software vulnerability prediction, the work regarding the selection and evaluation of different types of relevant vulnerability features is still limited. Thus, in this paper, we examine features generated by SonarQube and CCCC tools, to identify those that can be used for software vulnerability prediction. We investigate the suitability of thirty-three different features to train thirteen distinct machine learning algorithms to design vulnerability predictors and identify the most relevant features that should be used for training. Our evaluation is based on a comprehensive feature selection process based on the correlation analysis of the features, together with four well-known feature selection techniques. Our experiments, using a large publicly available dataset, facilitate the evaluation and result in the identification of small, but efficient sets of features for software vulnerability prediction. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Feature selection; Machine learning; Software vulnerability prediction; Static analysis","Computer software selection and evaluation; Data mining; Deterioration; Feature extraction; Forecasting; Large dataset; Learning algorithms; Losses; Machine learning; Petroleum reservoir evaluation; Quality control; Software quality; Software testing; Correlation analysis; Design vulnerabilities; Efficient feature selections; Security breaches; Selection and evaluations; Selection techniques; Software production; Software vulnerabilities; algorithm; article; correlation analysis; feature selection; prediction; software; Static analysis","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85100490591"
"Sahin C.B.","Sahin, Canan Batur (57207688292)","57207688292","DCW-RNN: Improving class level metrics for software vulnerability detection using artificial immune system with clock-work recurrent neural network","2021","2021 International Conference on INnovations in Intelligent SysTems and Applications, INISTA 2021 - Proceedings","","","","","","","","10.1109/INISTA52262.2021.9548609","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116610484&doi=10.1109%2fINISTA52262.2021.9548609&partnerID=40&md5=7c05062450fd6f8550b7b8852e661583","As the defenses evolve, so do the solutions to a software vulnerability. The primary reason for security incidents, e.g., cyber-attacks, originates from software vulnerabilities. It is challenging to enhance the performance of software processes and determine and eliminate software vulnerabilities. Thus, the development of algorithms with higher security to be applied to possible security issues in software represents a significant research subject for researchers in the domain of software security. The basis of the Dendritic Cell Algorithm (DCA), which is an emerging evolutionary algorithm, constitutes the behavior of specific immune agents, called dendritic cells (DCs). Till now, no strategy or idea has already been adopted on the Clock-Work Recurrent Neural Network (RNN) based Dendritic cell algorithm on vulnerability detection problems. In the present research, the first Clock-Work RNN based Dendritic Cell Algorithm (DCA) was suggested to identify complex dependencies between vulnerable object-oriented software metrics. The suggested method establishes immunity in software vulnerability prediction models to analyze the comparison of the Artificial Immune System Algorithms. The current paper involves the enhanced Clock-Work RNN based Dendritic Cell Algorithm, Genetic Algorithm (GA), and Clonal Selection Algorithm (CLONALG). Furthermore, comparison some studies was made on the basis Artificial Immune System (AIS) algorithms, such as Negative Selection Algorithm (NSA), Cellular Automata (CA), Membrane Computing (P-Systems). The experimental findings of our study demonstrate that our approach was computationally efficient on three different Java projects: Apache Tomcat (releases 6 and 7), Apache CXF, and the Stanford SecuriBench datasets. © 2021 IEEE.","Clock-work; Dendritic cell; Object-oriented metrics; Optimization; Recurrent neural network; Software vulnerability","Cells; Cytology; Genetic algorithms; Immune system; Network security; Object oriented programming; Artificial immune system algorithms; Clock-work; Dendritic cell; Dendritic cell algorithms; Dendritics; Network-based; Object oriented metrics; Optimisations; Software vulnerabilities; Vulnerability detection; Clocks","Conference paper","Final","","Scopus","2-s2.0-85116610484"
"Boudko S.; Abie H.; Nigussie E.; Savola R.","Boudko, Svetlana (25421246200); Abie, Habtamu (6505691222); Nigussie, Ethiopia (8586643800); Savola, Reijo (23010245100)","25421246200; 6505691222; 8586643800; 23010245100","Towards federated learning-based collaborative adaptive cybersecurity for multi-microgrids","2021","Proceedings of the 18th International Conference on Wireless Networks and Mobile Systems, WINSYS 2021","","","","83","90","7","","10.5220/0010580700830090","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111870221&doi=10.5220%2f0010580700830090&partnerID=40&md5=6ce700d1c6c0b046f9a94da4fa872dd0","Multi-microgrids (MMGs) provide economic and environmental benefits to society by improving operational flexibility, stability and reliability of a smart grid. MMGs have greater complexity than conventional power networks due to the use of multiple infrastructures, communication protocols, controllers, and intelligent electronic devices. The distributed and heterogeneous connectivity technologies of the MMGs and their need to exchange information with external sources as well as the vulnerabilities in the communication networks and software-based components, make MMGs susceptible to cyberattacks. In this work, we present a conceptual framework for collaborative adaptive cybersecurity that is able to proactively detect security incidents. The framework utilizes federated learning for collaborative training of shared prediction models in a decentralized manner. The methodology used in this research is mainly analytical. This involves analysis of how the principles of a collaborative adaptive cybersecurity can be applied to the MMG environments resulting in the development of theoretical models which can then be validated in practice by prototyping and using real time simulation. Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved","Adaptive Mechanisms; Collaborative Protocols; Cybersecurity; Federated Learning; Machine Learning; Multi-microgrid","Electric power transmission networks; Microgrids; Predictive analytics; Wireless networks; Collaborative training; Conceptual frameworks; Conventional power; Economic and environmental benefits; Intelligent electronic device; Operational flexibility; Real time simulations; Stability and reliabilities; Security of data","Conference paper","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85111870221"
"Kudjo P.K.; Chen J.; Mensah S.; Amankwah R.; Kudjo C.","Kudjo, Patrick Kwaku (57195678643); Chen, Jinfu (56485257600); Mensah, Solomon (57191254462); Amankwah, Richard (57205487973); Kudjo, Christopher (57213159416)","57195678643; 56485257600; 57191254462; 57205487973; 57213159416","The effect of Bellwether analysis on software vulnerability severity prediction models","2020","Software Quality Journal","28","4","","1413","1446","33","","10.1007/s11219-019-09490-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077588152&doi=10.1007%2fs11219-019-09490-1&partnerID=40&md5=1e30317eeed3cf3b60d90f3841984222","Vulnerability severity prediction (VSP) models provide useful insight for vulnerability prioritization and software maintenance. Previous studies have proposed a variety of machine learning algorithms as an important paradigm for VSP. However, to the best of our knowledge, there are no other existing research studies focusing on investigating how a subset of features can be used to improve VSP. To address this deficiency, this paper presents a general framework for VSP using the Bellwether analysis (i.e., exemplary data). First, we apply the natural language processing techniques to the textual descriptions of software vulnerability. Next, we developed an algorithm termed Bellvul to identify and select an exemplary subset of data (referred to as Bellwether) to be considered as the training set to yield improved prediction accuracy against the growing portfolio, within-project cases, and the k-fold cross-validation subset. Finally, we assessed the performance of four machine learning algorithms, namely, deep neural network, logistic regression, k-nearest neighbor, and random forest using the sampled instances. The prediction results of the suggested models and the benchmark techniques were assessed based on the standard classification evaluation metrics such as precision, recall, and F-measure. The experimental result shows that the Bellwether approach achieves F-measure ranging from 14.3% to 97.8%, which is an improvement over the benchmark techniques. In conclusion, the proposed approach is a promising research direction for assisting software engineers when seeking to predict instances of vulnerability records that demand much attention prior to software release. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Bellwether; Feature selection; Machine learning algorithms; Severity; Software vulnerability","Decision trees; Deep learning; Deep neural networks; Feature extraction; Forecasting; Genetic algorithms; Learning systems; Logistic regression; Natural language processing systems; Nearest neighbor search; Predictive analytics; Bellwether; Classification evaluation; K fold cross validations; NAtural language processing; Prediction accuracy; Severity; Software vulnerabilities; Vulnerability prioritization; Learning algorithms","Article","Final","","Scopus","2-s2.0-85077588152"
"Hoque M.S.; Jamil N.; Amin N.; Lam K.-Y.","Hoque, Mohammad Shamsul (57220806665); Jamil, Norziana (36682671900); Amin, Nowshad (7102424614); Lam, Kwok-Yan (7403657062)","57220806665; 36682671900; 7102424614; 7403657062","An improved vulnerability exploitation prediction model with novel cost function and custom trained word vector embedding","2021","Sensors","21","12","4220","","","","","10.3390/s21124220","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108114510&doi=10.3390%2fs21124220&partnerID=40&md5=94a6e0312f1fab8c61ddaacc941d9583","Successful cyber-attacks are caused by the exploitation of some vulnerabilities in the software and/or hardware that exist in systems deployed in premises or the cloud. Although hundreds of vulnerabilities are discovered every year, only a small fraction of them actually become exploited, thereby there exists a severe class imbalance between the number of exploited and non-exploited vulnerabilities. The open source national vulnerability database, the largest repository to index and maintain all known vulnerabilities, assigns a unique identifier to each vulnerability. Each registered vulnerability also gets a severity score based on the impact it might inflict upon if compromised. Recent research works showed that the cvss score is not the only factor to select a vulnerability for exploitation, and other attributes in the national vulnerability database can be effectively utilized as predictive feature to predict the most exploitable vulnerabilities. Since cybersecurity management is highly resource savvy, organizations such as cloud systems will benefit when the most likely exploitable vulnerabilities that exist in their system software or hardware can be predicted with as much accuracy and reliability as possible, to best utilize the available resources to fix those first. Various existing research works have developed vulnerability exploitation prediction models by addressing the existing class imbalance based on algorithmic and artificial data resampling techniques but still suffer greatly from the overfitting problem to the major class rendering them practically unreliable. In this research, we have designed a novel cost function feature to address the existing class imbalance. We also have utilized the available large text corpus in the extracted dataset to develop a custom-trained word vector that can better capture the context of the local text data for utilization as an embedded layer in neural networks. Our developed vulnerability exploitation prediction models powered by a novel cost function and custom-trained word vector have achieved very high overall performance metrics for accuracy, precision, recall, F1-Score and AUC score with values of 0.92, 0.89, 0.98, 0.94 and 0.97, respectively, thereby outperforming any existing models while successfully overcoming the existing overfitting problem for class imbalance. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Cloud security management; Cost function; Supervised machine learning, modelling and prediction; Vulnerability exploitation prediction","Algorithms; Computer Security; Machine Learning; Neural Networks, Computer; Reproducibility of Results; Cost functions; Forecasting; Large dataset; Multilayer neural networks; Network layers; Network security; Open source software; Software reliability; National vulnerability database; Over fitting problem; Performance metrics; Prediction model; Recent researches; Resampling technique; System softwares; Unique identifiers; algorithm; computer security; machine learning; reproducibility; Predictive analytics","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85108114510"
"Feng Q.; Feng C.; Hong W.","Feng, Qi (57220061733); Feng, Chendong (57202889084); Hong, Weijiang (57194016383)","57220061733; 57202889084; 57194016383","Graph Neural Network-based Vulnerability Predication","2020","Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020","","","9240607","800","801","1","","10.1109/ICSME46990.2020.00096","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096658254&doi=10.1109%2fICSME46990.2020.00096&partnerID=40&md5=c5b9c0290496d31a4e99dd5ec5ba325a","Automatic vulnerability detection is challenging. In this paper, we report our in-progress work of vulnerability prediction based on graph neural network (GNN). We propose a general GNN-based framework for predicting the vulnerabilities in program functions. We study the different instantiations of the framework in representative program graph representations, initial node encodings, and GNN learning methods. The preliminary experimental results on a representative benchmark indicate that the GNN-based method can improve the accuracy and recall rates of vulnerability prediction. © 2020 IEEE.","AST; CFG; CPG; GNN; Vulnerability predication","Computer software maintenance; Forecasting; Learning systems; Network security; Encodings; Graph neural networks; Graph representation; Learning methods; Prediction-based; Program functions; Recall rate; Vulnerability detection; Neural networks","Conference paper","Final","","Scopus","2-s2.0-85096658254"
"Porter C.; Mururu G.; Barua P.; Pande S.","Porter, Chris (57217650542); Mururu, Girish (56358596300); Barua, Prithayan (57205888949); Pande, Santosh (7103227888)","57217650542; 56358596300; 57205888949; 7103227888","BlankIt Library Debloating","2020","Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)","","","","164","180","16","","10.1145/3385412.3386017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086829863&doi=10.1145%2f3385412.3386017&partnerID=40&md5=7b56c7bcaa3983e0e22eb04cc355ee1f","Modern software systems make extensive use of libraries derived from C and C++. Because of the lack of memory safety in these languages, however, the libraries may suffer from vulnerabilities, which can expose the applications to potential attacks. For example, a very large number of return-oriented programming gadgets exist in glibc that allow stitching together semantically valid but malicious Turing-complete and -incomplete programs. While CVEs get discovered and often patched and remedied, such gadgets serve as building blocks of future undiscovered attacks, opening an ever-growing set of possibilities for generating malicious programs. Thus, significant reduction in the quantity and expressiveness (utility) of such gadgets for libraries is an important problem. In this work, we propose a new approach for handling an application's library functions that focuses on the principle of ""getting only what you want."" This is a significant departure from the current approaches that focus on ""cutting what is unwanted."" Our approach focuses on activating/deactivating library functions on demand in order to reduce the dynamically linked code surface, so that the possibilities of constructing malicious programs diminishes substantially. The key idea is to load only the set of library functions that will be used at each library call site within the application at runtime. This approach of demand-driven loading relies on an input-aware oracle that predicts a near-exact set of library functions needed at a given call site during the execution. The predicted functions are loaded just in time and unloaded on return. We present a decision-tree based predictor, which acts as an oracle, and an optimized runtime system, which works directly with library binaries like GNU libc and libstdc++. We show that on average, the proposed scheme cuts the exposed code surface of libraries by 97.2%, reduces ROP gadgets present in linked libraries by 97.9%, achieves a prediction accuracy in most cases of at least 97%, and adds a runtime overhead of 18% on all libraries (16% for glibc, 2% for others) across all benchmarks of SPEC 2006. Further, we demonstrate BlankIt on two real-world applications, sshd and nginx, with a high amount of debloating and low overheads. © 2020 ACM.","Program Security; Software debloating","Decision trees; Libraries; Open source software; Building blockes; Library functions; Potential attack; Prediction accuracy; Return-oriented programming; Runtime overheads; Runtime systems; Software systems; C++ (programming language)","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85086829863"
"Nguyen H.N.; Teerakanok S.; Inomata A.; Uehara T.","Nguyen, Hai Ngoc (57221013039); Teerakanok, Songpon (37015690500); Inomata, Atsuo (8950100000); Uehara, Tetsutaro (8215150700)","57221013039; 37015690500; 8950100000; 8215150700","The Comparison of Word Embedding Techniques in RNNs for Vulnerability Detection","2021","International Conference on Information Systems Security and Privacy","","","","109","120","11","","10.5220/0010232301090120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176352502&doi=10.5220%2f0010232301090120&partnerID=40&md5=84aa7d05413abe5042244989fbf0a1d4","Many studies have combined Deep Learning and Natural Language Processing (NLP) techniques in security systems in performing tasks such as bug detection, vulnerability prediction, or classification. Most of these works relied on NLP embedding methods to generate input vectors for the deep learning models. However, there are many existing embedding methods to encode software text files into vectors, and the structures of neural networks are immense and heuristic. This leads to a challenge for the researcher to choose the appropriate combination of embedding techniques and the model structure for training the vulnerability detection classifiers. For this task, we propose a system to investigate the use of four popular word embedding techniques combined with four different recurrent neural networks (RNNs), including both bidirectional RNNs (BRNNs) and unidirectional RNNs. We trained and evaluated the models by using two types of vulnerable function datasets written in C code. Our results showed that the FastText embedding technique combined with BRNNs produced the most efficient detection rate, compared to other combinations, on a real-world but not on an artificially-produced dataset. Further experiments on other datasets are necessary to confirm this result. © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.","Deep Learning; RNNs; Vulnerability Detection; Word Embeddings","","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85176352502"
"Zhang Y.; Zhao L.; Jin Y.","Zhang, Yichen (57216442396); Zhao, Lei (56461651900); Jin, Yinshan (57216437044)","57216442396; 56461651900; 57216437044","Sensitive Region Prediction based on Neural Network in Fuzzy Test Algorithm Research; [模糊测试中基于神经网络的敏感区域预测算法研究]","2020","Journal of Cyber Security","5","1","","10","19","9","","10.19363/J.cnki.cn10-1380/tn.2020.01.02","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083509925&doi=10.19363%2fJ.cnki.cn10-1380%2ftn.2020.01.02&partnerID=40&md5=c7b9b6b374ac8e71e48dfa67bb8e3c58","Software vulnerabilities are the root cause of computer security problems. Due to highly efficiency and easy expansion, fuzzy test becomes the most widely used vulnerabilities detection technology. However, previous fuzzy test technology can not deal with highly-structed problems, and has low efficiency on blinding mutation. For these problems, this paper proposes a fuzzy test algorithm based on sensitive region prediction via Neural Network. This method takes the phenomenon of small changing of some regions causing great change on software behavior as starting point. We use the conception of sensitive region and use Neural Network which has great performance on learning data features to detect these regions. After sensitive region detection, this paper uses enhanced learning strategy and optimizes the mutation strategy which improve efficiency and depth of detection. In order to verify the validity of the proposed method, this study was conducted on the programs dealing with three widely used format file as PNG, TIFF and XML, and it shows 8%~20% improvement on fuzzing test coverage which verifies the validity and feasibility of proposed method. © Copyright 2020, Institute of Information Engineering, the Chinese Academy of Sciences. All rights reserved.","Fuzzy test; Neural network; Sensitive region; Software vulnerability","","Article","Final","","Scopus","2-s2.0-85083509925"
"Fan M.; Jia A.; Liu J.; Liu T.; Chen W.","Fan, Ming (56259274200); Jia, Ang (57216456446); Liu, Jingwen (57258728800); Liu, Ting (55835301800); Chen, Wei (57219137044)","56259274200; 57216456446; 57258728800; 55835301800; 57219137044","When representation learning meets software analysis","2020","RL+SE and PL 2020 - Proceedings of the 1st ACM SIGSOFT International Workshop on Representation Learning for Software Engineering and Program Languages, Co-located with ESEC/FSE 2020","","","","17","18","1","","10.1145/3416506.3423578","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096959121&doi=10.1145%2f3416506.3423578&partnerID=40&md5=f67868ef3fc7b4187221131f8479e56c","In recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). Especially, representation learning, which can learn vectors from the syntactic and semantics of the code, offers much convenience and promotion for the downstream tasks such as code search and vulnerability detection. In this work, we introduce our two applications of leveraging representation learning for software analysis, including defect prediction and vulnerability detection.  © 2020 Owner/Author.","defect prediction; representation learning; vulnerability detection","Application programs; Semantics; Code search; Defect prediction; Software analysis; Vulnerability detection; Deep learning","Conference paper","Final","","Scopus","2-s2.0-85096959121"
"Bassi D.; Singh H.","Bassi, Deepali (57764927900); Singh, Hardeep (58376239500)","57764927900; 58376239500","A Comparative Study on Hyperparameter Optimization Methods in Software Vulnerability Prediction","2021","Proceedings - 2021 2nd International Conference on Computational Methods in Science and Technology, ICCMST 2021","","","","181","184","3","","10.1109/ICCMST54943.2021.00046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132920471&doi=10.1109%2fICCMST54943.2021.00046&partnerID=40&md5=0f88df1cced3693768786b8a8fa433aa","Hyperparameter optimization (HPO) is the procedure to find the optimal hyperparameters for predictive modelling. In order to build an efficient software vulnerability prediction (SVP) model, effective HPO method is required. Recent studies have explored the area of machine learning for SVP models' construction. Different machine learning algorithms have distinct hyperparameters to be tuned. In this paper, we have compared four HPO methods for six machine learning based SVP models using an open-source public dataset 'Drupal'. The performance of the models is evaluated by the accuracy of the model and the computational time. Experiments are performed using various python libraries such as sklearn, skopt, hyperopt, and tpot. After comparative analysis, we found that bayesian optimization and random search has shown the performance improvement for SVP models with effective computational time.  © 2021 IEEE.","hyperparameter optimization; machine learning algorithms; Software vulnerability","Learning algorithms; Open source software; Python; Comparatives studies; Computational time; Hyper-parameter; Hyper-parameter optimizations; Machine learning algorithms; Machine-learning; Optimization method; Performance; Prediction modelling; Software vulnerabilities; Machine learning","Conference paper","Final","","Scopus","2-s2.0-85132920471"
"Jiang Y.; Lu P.; Su X.; Wang T.","Jiang, Yuan (57216415290); Lu, Pengcheng (57216416636); Su, Xiaohong (7402181881); Wang, Tiantian (55430891400)","57216415290; 57216416636; 7402181881; 55430891400","LTRWES: A new framework for security bug report detection","2020","Information and Software Technology","124","","106314","","","","","10.1016/j.infsof.2020.106314","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083464754&doi=10.1016%2fj.infsof.2020.106314&partnerID=40&md5=d599e785a3c89d581884b85aa58d502f","Context: Security bug reports (SBRs) usually contain security-related vulnerabilities in software products, which could be exploited by malicious attackers. Hence, it is important to identify SBRs quickly and accurately among bug reports (BRs) that have been disclosed in bug tracking systems. Although a few methods have been already proposed for the detection of SBRs, challenging issues still remain due to noisy samples, class imbalance and data scarcity. Object: This motivates us to reveal the potential challenges faced by the state-of-the-art SBRs prediction methods from the viewpoint of data filtering and representation. Furthermore, the purpose of this paper is also to provide a general framework and new solutions to solve these problems. Method: In this study, we propose a novel approach LTRWES that incorporates learning to rank and word embedding into the identification of SBRs. Unlike previous keyword-based approaches, LTRWES is a content-based data filtering and representation framework that has several desirable properties not shared in other methods. Firstly, it exploits ranking model to efficiently filter non-security bug reports (NSBRs) that have higher content similarity with respect to SBRs. Secondly, it applies word embedding technology to transform the rest of NSBRs, together with SBRs, into low-dimensional real-value vectors. Result: Experiment results on benchmark and large real-world datasets show that our proposed method outperforms the state-of-the-art method. Conclusion: Overall, the LTRWES is valid with high performance. It will help security engineers to identify SBRs from thousands of NSBRs more accurately than existing algorithms. Therefore, this will positively encourage the research and development of the content-based methods for security bug report detection. © 2020 Elsevier B.V.","Content-based filtering; Machine learning; Security bug report; Word embedding","Embeddings; Large dataset; Bug tracking system; Content similarity; Content-based methods; Prediction methods; Real-world datasets; Research and development; Security engineers; State-of-the-art methods; Learning to rank","Article","Final","","Scopus","2-s2.0-85083464754"
"Perumal S.; Kola Sujatha P.","Perumal, Seethalakshmi (57236796800); Kola Sujatha, P. (57190389440)","57236796800; 57190389440","Stacking Ensemble-based XSS Attack Detection Strategy Using Classification Algorithms","2021","Proceedings of the 6th International Conference on Communication and Electronics Systems, ICCES 2021","","","9489177","897","901","4","","10.1109/ICCES51350.2021.9489177","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113792640&doi=10.1109%2fICCES51350.2021.9489177&partnerID=40&md5=fe4d4d7e32537939bc310e5751c2ab2e","The accessibility of the internet and mobile platforms has risen dramatically due to digital technology innovations. Web applications have opened up a variety of market possibilities by supplying consumers with a wide variety of digital technologies that benefit from high accessibility and functionality. Around the same time, web application protection continues to be an important challenge on the internet, and security must be taken seriously in order to secure confidential data. The threat is caused by inadequate validation of user input information, software developed without strict adherence to safety standards, vulnerability of reusable software libraries, software weakness, and so on. Through abusing a website's vulnerability, introduers are manipulating the user's information in order to exploit it for their own benefit. Then introduers inject their own malicious code, stealing passwords, manipulating user activities, and infringing on customers' privacy. As a result, information is leaked, applications malfunction, confidential data is accessed, etc. To mitigate the aforementioned issues, stacking ensemble based classifier model for Cross-site scripting (XSS) attack detection is proposed. Furthermore, the stacking ensembles technique is used in combination with different machine learning classification algorithms like k-Means, Random Forest and Decision Tree as base-learners to reliably detect XSS attack. Logistic Regression is used as meta-learner to predict the attack with greater accuracy. The classification algorithms in stacking model explore the problem in their own way and its results are given as input to the meta-learner to make final prediction, thus improving the overall detection accuracy of XSS attack in stacking than the individual models. The simulation findings demonstrate that the proposed model detects XSS attack successfully. © 2021 IEEE.","Classification; Cross-site scripting (XSS) attack; Decision Tree; k-Means clustering; Logistic Regression; Random Forest; Stacking Ensembles; XSS payload","Classification (of information); Computer software reusability; Decision trees; K-means clustering; Logistic regression; Machine learning; Privacy by design; Classification algorithm; Classifier models; Cross-site scripting; Detection accuracy; Digital technologies; Individual models; Machine learning classification; Reusable softwares; Learning algorithms","Conference paper","Final","","Scopus","2-s2.0-85113792640"
"Yin J.; Tang M.; Cao J.; Wang H.","Yin, Jiao (54884588500); Tang, MingJian (57215896761); Cao, Jinli (7403353999); Wang, Hua (57215111932)","54884588500; 57215896761; 7403353999; 57215111932","Apply transfer learning to cybersecurity: Predicting exploitability of vulnerabilities by description","2020","Knowledge-Based Systems","210","","106529","","","","","10.1016/j.knosys.2020.106529","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092712488&doi=10.1016%2fj.knosys.2020.106529&partnerID=40&md5=51b4182cc5f1d9da82fe17a523fd519e","Thousands of software vulnerabilities are archived and disclosed to the public each year, posing severe cybersecurity threats to the whole society. Predicting the exploitability of vulnerabilities is crucial for decision-makers to prioritize their efforts and patch the most critical vulnerabilities. Software vulnerability descriptions are accessible features in early stage and contain rich semantic information. Therefore, descriptions are wildly used for exploitability prediction in both industry and academia. However, comparing with other corpora, the size of vulnerability description corpus is too small to train a comprehensive Natural Language Processing (NLP) model. To gain a better performance, this paper proposes a framework named ExBERT to accurately predict if a vulnerability will be exploited or not. ExBERT essentially is an improved Bidirectional Encoder Representations from Transformers (BERT) model for exploitability prediction. First, we fine-tune a pre-trained BERT using collected domain-specific corpus. Then, we design a Pooling Layer and a Classification Layer on top of the fine-tuned BERT model to extract sentence-level semantic features and predict the exploitability of vulnerabilities. Results on 46,176 real-word vulnerabilities have demonstrated that the proposed ExBERT framework achieves 91.12% on accuracy and 91.82% on precision, outperforming the state-of-the-art approach with 89.0% on accuracy and 81.8% on precision. © 2020 Elsevier B.V.","ExBERT; Exploitability prediction; Software vulnerability; Transfer learning","Decision making; Natural language processing systems; Security of data; Semantics; Transfer learning; Decision makers; Domain specific; NAtural language processing; Semantic features; Semantic information; Software vulnerabilities; State-of-the-art approach; Vulnerability description; Forecasting","Article","Final","","Scopus","2-s2.0-85092712488"
"Bhuiyan F.A.; Shakya R.; Rahman A.","Bhuiyan, Farzana Ahamed (57218862900); Shakya, Raunak (57218864575); Rahman, Akond (57188647874)","57218862900; 57218864575; 57188647874","Can we use software bug reports to identify vulnerability discovery strategies?","2020","ACM International Conference Proceeding Series","","","","52","61","9","","10.1145/3384217.3385618","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090499669&doi=10.1145%2f3384217.3385618&partnerID=40&md5=ffb359696d67ef5ec9cc24a34f944d67","Daily horror stories related to software vulnerabilities necessitates the understanding of how vulnerabilities are discovered. Identification of data sources that can be leveraged to understand how vulnerabilities are discovered could aid cybersecurity researchers to characterize exploitation of vulnerabilities. The goal of the paper is to help cybersecurity researchers in characterizing vulnerabilities by conducting an empirical study of software bug reports. We apply qualitative analysis on 729, 908, and 5336 open source software (OSS) bug reports respectively, collected from Gentoo, LibreOffice, and Mozilla to investigate if bug reports include vulnerability discovery strategies i.e. sequences of computation and/or cognitive activities that an attacker performs to discover vulnerabilities, where the vulnerability is indexed by a credible source, such as the National Vulnerability Database (NVD). We evaluate two approaches namely, text feature-based approach and regular expression-based approach to automatically identify bug reports that include vulnerability discovery strategies. We observe the Gentoo, LibreOffice, and Mozilla bug reports to include vulnerability discovery strategies. Using text feature-based prediction models, we observe the highest prediction performance for the Mozilla dataset with a recall of 0.78. Using the regular expression-based approach we observe recall to be 0.83 for the same dataset. Findings from our paper provide the groundwork for cybersecurity researchers to use OSS bug reports as a data source for advancing the science of vulnerabilities. © 2020 ACM.","Bug report; Empirical study; Ethical hacking; Strategy; Vulnerability","Information dissemination; Open systems; Pattern matching; Predictive analytics; Security of data; Cognitive activities; Empirical studies; National vulnerability database; Prediction performance; Qualitative analysis; Regular expressions; Software vulnerabilities; Vulnerability discovery; Open source software","Conference paper","Final","","Scopus","2-s2.0-85090499669"
"Brauckmann A.; Goens A.; Castrillon J.","Brauckmann, Alexander (57210584782); Goens, Andres (56252187300); Castrillon, Jeronimo (24779298800)","57210584782; 56252187300; 24779298800","ComPy-Learn: A toolbox for exploring machine learning representations for compilers","2020","Forum on Specification and Design Languages","2020-September","","9232946","","","","","10.1109/FDL50818.2020.9232946","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096107918&doi=10.1109%2fFDL50818.2020.9232946&partnerID=40&md5=e835ad547e9cf1fd235357bd6cf867fc","Deep Learning methods have not only shown to improve software performance in compiler heuristics, but also e.g. to improve security in vulnerability prediction or to boost developer productivity in software engineering tools. A key to the success of such methods across these use cases is the expressiveness of the representation used to abstract from the program code. Recent work has shown that different such representations have unique advantages in terms of performance. However, determining the best-performing one for a given task is often not obvious and requires empirical evaluation. Therefore, we present ComPy-Learn, a toolbox for conveniently defining, extracting, and exploring representations of program code. With syntax-level language information from the Clang compiler frontend and low-level information from the LLVM compiler backend, the tool supports the construction of linear and graph representations and enables an efficient search for the best-performing representation and model for tasks on program code.  © 2020 IEEE.","Clang; Code Representations; Compilers; LLVM; Machine Learning","Codes (symbols); Deep learning; Heuristic methods; Learning systems; Specifications; Empirical evaluations; Graph representation; Language informations; Learning methods; LLVM compilers; Software engineering tools; Software performance; Tool support; Program compilers","Conference paper","Final","","Scopus","2-s2.0-85096107918"
"Huang S.; Pan J.","Huang, Shaomang (36468288200); Pan, Jianfeng (57219600119)","36468288200; 57219600119","A Software MTD Technique of Multipath Execution Protection","2020","Proceedings of 2020 IEEE 3rd International Conference on Information Systems and Computer Aided Education, ICISCAE 2020","","","9236800","489","496","7","","10.1109/ICISCAE51034.2020.9236800","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096359752&doi=10.1109%2fICISCAE51034.2020.9236800&partnerID=40&md5=f994db1939ff52c51bc3332dfc6559b7","Attackers exploit vulnerabilities to attack target software relying on accurate mastery of its code implementation details and exact prediction of its runtime state transitions. Especially, when and where a sensitive pointer is allocated, what struct a sensitive object is and how to hijack the victim program's execution flow. Software randomization technique, based on the idea of moving target defense, introduces uncertainty into the target program and offers probabilistic protection similar to cryptography. In this paper we present the multipath execution, a software randomization technique based on LLVM framework. Multipath execution is designed and implemented to achieve not only randomness of memory layout and diversity of binary files, but also randomness of program code paths executed, which is unique in our work. The processed software shows uncertainty from static and dynamic aspects, which makes it difficult to analyze and attack. Both qualitative and quantitative assessments illustrate the effectiveness of our method. © 2020 IEEE.","mimic security defense; Moving target defense; multi-path execution; software diversity; software randomization","Codes (symbols); Computer aided instruction; Information systems; Information use; Network security; Random processes; Uncertainty analysis; Attack target; Dynamic aspects; Memory layout; Moving target defense; Multi-path execution; Qualitative and quantitative assessments; Randomization techniques; Run-time state; Computer software","Conference paper","Final","","Scopus","2-s2.0-85096359752"
"Elbaz C.; Rilling L.; Morin C.","Elbaz, Clément (57205883148); Rilling, Louis (15127766300); Morin, Christine (7202218434)","57205883148; 15127766300; 7202218434","Fighting N-day vulnerabilities with automated CVSS vector prediction at disclosure","2020","ACM International Conference Proceeding Series","","","","","","","","10.1145/3407023.3407038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117542230&doi=10.1145%2f3407023.3407038&partnerID=40&md5=b64da5b7ad3fc7886b5fa39a2d978549","The Common Vulnerability Scoring System (CVSS) is the industry standard for describing the characteristics of a software vulnerability and measuring its severity. However, during the first days after a vulnerability disclosure, the initial human readable description of the vulnerability is not available as a machine readable CVSS vector yet. This situation creates a period of time when only expensive manual analysis can be used to react to new vulnerabilities because no data is available for cheaper automated analysis yet. We present a new technique based on linear regression to automatically predict the CVSS vector of newly disclosed vulnerabilities using only their human readable descriptions, with a strong emphasis on decision explicability. Our experimental results suggest real world applicability. © 2020 ACM.","CVE; CVSS; Linear regression; Machine learning; Security","Computer applications; Computer programming; Automated analysis; Common vulnerability scoring systems; Human-readable; Industry standards; Manual analysis; Real-world; Software vulnerabilities; Vulnerability disclosure; Turing machines","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85117542230"
"Ozols M.; Eckersley A.; Platt C.I.; Stewart-Mcguinness C.; Hibbert S.A.; Revote J.; Li F.; Griffiths C.E.M.; Watson R.E.B.; Song J.; Bell M.; Sherratt M.J.","Ozols, Matiss (57200243340); Eckersley, Alexander (57201479346); Platt, Christopher I. (57086247400); Stewart-Mcguinness, Callum (57222382225); Hibbert, Sarah A. (57023692000); Revote, Jerico (56133068000); Li, Fuyi (57198720015); Griffiths, Christopher E. M. (7201983002); Watson, Rachel E. B. (12242442800); Song, Jiangning (56023619300); Bell, Mike (7401466716); Sherratt, Michael J. (6701449297)","57200243340; 57201479346; 57086247400; 57222382225; 57023692000; 56133068000; 57198720015; 7201983002; 12242442800; 56023619300; 7401466716; 6701449297","Predicting proteolysis in complex proteomes using deep learning","2021","International Journal of Molecular Sciences","22","6","3071","1","20","19","","10.3390/ijms22063071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102568371&doi=10.3390%2fijms22063071&partnerID=40&md5=46c775065a628207079f752d03fd2088","Both protease-and reactive oxygen species (ROS)-mediated proteolysis are thought to be key effectors of tissue remodeling. We have previously shown that comparison of amino acid composition can predict the differential susceptibilities of proteins to photo-oxidation. However, predicting protein susceptibility to endogenous proteases remains challenging. Here, we aim to develop bioin-formatics tools to (i) predict cleavage site locations (and hence putative protein susceptibilities) and (ii) compare the predicted vulnerabilities of skin proteins to protease-and ROS-mediated proteolysis. The first goal of this study was to experimentally evaluate the ability of existing protease cleavage site prediction models (PROSPER and DeepCleave) to identify experimentally determined MMP9 cleavage sites in two purified proteins and in a complex human dermal fibroblast-derived extracellular matrix (ECM) proteome. We subsequently developed deep bidirectional recurrent neural network (BRNN) models to predict cleavage sites for 14 tissue proteases. The predictions of the new models were tested against experimental datasets and combined with amino acid composition analysis (to predict ultraviolet radiation (UVR)/ROS susceptibility) in a new web app: the Manchester proteome susceptibility calculator (MPSC). The BRNN models performed better in predicting cleavage sites in native dermal ECM proteins than existing models (DeepCleave and PROSPER), and application of MPSC to the skin proteome suggests that: compared with the elastic fiber network, fibrillar collagens may be susceptible primarily to protease-mediated proteolysis. We also identify additional putative targets of oxidative damage (dermatopontin, fibulins and defensins) and protease action (laminins and nidogen). MPSC has the potential to identify potential targets of proteolysis in disparate tissues and disease states. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Aging; Biomark-ers; Deep-learning; Degradomics; Extracellular matrix; Machine learning; Protease; Skin","Amino Acids; Deep Learning; Extracellular Matrix Proteins; Humans; Neural Networks, Computer; Peptide Hydrolases; Proteolysis; Proteome; Reactive Oxygen Species; Reproducibility of Results; Software; Ultraviolet Rays; cell protein; decorin; defensin; dermatopontin; entactin; fibrillar collagen; fibulin; gelatinase B; laminin; reactive oxygen metabolite; recombinant enzyme; recombinant protein; unclassified drug; vitronectin; amino acid; peptide hydrolase; proteome; reactive oxygen metabolite; scleroprotein; amino acid composition; Article; bioinformatics; controlled study; deep bidirectional recurrent neural network; deep learning; degradomics; elastic fiber; enzyme activity; enzyme degradation; extracellular matrix; fibroblast culture; human; human cell; human tissue; in vitro study; oxidative stress; protein cleavage; protein purification; recurrent neural network; skin fibroblast; ultraviolet radiation; metabolism; protein degradation; radiation response; reproducibility; software","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85102568371"
"Ma L.; Ma L.","Ma, Lei (57753855000); Ma, Lei (58451818800)","57753855000; 58451818800","Research on Vulnerability Exploitation and Detection Technology Based on Big Data Analysis","2021","2021 IEEE International Conference on Industrial Application of Artificial Intelligence, IAAI 2021","","","","429","435","6","","10.1109/IAAI54625.2021.9699917","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125964822&doi=10.1109%2fIAAI54625.2021.9699917&partnerID=40&md5=f829d9cb1fccda6a27d047d459ec8763","Because the traditional methods do not extract the attack characteristics in the process of vulnerability detection, the hit rate of vulnerability detection is low, and the false positive rate of unknown non defective software is high. In order to solve the above problems, a new vulnerability utilization and detection technology is studied based on big data analysis. Set up a Spark-based big data security analysis platform and network security equipment processing module, analyze the real-time data stream of the network through the big data security analysis platform, extract attack characteristics in the process of exploiting vulnerabilities, and use big data to efficiently process large amounts of data. To improve the detection efficiency of exploits. The platform integrates the Flume collection module, Kafka sorting module and Spark-streaming computing module. It extracts the characteristics of some scanning attacks and DoS attacks, uses the CART decision tree algorithm to build an attack prediction model, realizes the detection of exploits, and then detects the received abnormal data stream and normal data stream, which are sent to the corresponding network security device for processing by the network security device processing module to prevent the vulnerability from being successfully exploited. Through experiments, the good performance of the big data security analysis platform is verified.  © 2021 IEEE.","Big Data Analysis; Detection Technology; Technical Research; Vulnerability Exploitation","Data mining; Decision trees; Denial-of-service attack; Network security; Security systems; Big data analyse; Data stream; Detection technology; Networks security; Processing modules; Security analysis; Security devices; Technical research; Vulnerability detection; Vulnerability exploitation; Big data","Conference paper","Final","","Scopus","2-s2.0-85125964822"
"Luckow K.; Kersten R.; Pasareanu C.","Luckow, Kasper (54383756000); Kersten, Rody (36632338300); Pasareanu, Corina (57204255037)","54383756000; 36632338300; 57204255037","Complexity vulnerability analysis using symbolic execution","2020","Software Testing Verification and Reliability","30","7-8","e1716","","","","","10.1002/stvr.1716","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079051231&doi=10.1002%2fstvr.1716&partnerID=40&md5=cfe00d5515843bff9a6ac3ebff2839eb","We describe techniques based on symbolic execution for finding software vulnerabilities that are due to algorithmic complexity. Such vulnerabilities allow an attacker to mount denial-of-service attacks to deny service to benign users or to otherwise disable a software system. The techniques use an efficient guided symbolic execution of a program to compute bounds on the worst-case complexity (for increasing input sizes) and to generate test values that trigger the worst-case behaviours. The resulting bounds are fitted to a function to obtain a prediction of the worst-case program behaviour at any input size. Scalability is achieved by using path policies that guide the symbolic execution towards worst-case paths. The policies are learned from the worst-case results obtained with exhaustive exploration at small input sizes and are applied to guide exploration at larger input sizes, where unguided exhaustive exploration is not possible. To achieve precision in the analysis, the path policies take into account the history of choices made along the path when deciding which branch to execute next. Furthermore, the computation is contextpreserving, meaning that the decision for each branch depends on the history computed with respect to the enclosing method. We further report preliminary results on a complementary technique that uses machine learning for building the path policies that guide the search. The techniques are implemented in open-source projects that build on the Symbolic Pathfinder tool for analysing Java programs. Experimental evaluation shows that the techniques can find vulnerabilities in complex Java programs and can outperform previous symbolic approaches. Copyright © 2020 John Wiley & Sons, Ltd.","complexity analysis; guided exploration; machine learning; symbolic execution","Denial-of-service attack; Java programming language; Learning systems; Machine learning; Model checking; Open source software; Parallel processing systems; Algorithmic complexity; Complementary techniques; Complexity analysis; Experimental evaluation; Software vulnerabilities; Symbolic execution; Vulnerability analysis; Worst-case complexity; Computational complexity","Conference paper","Final","","Scopus","2-s2.0-85079051231"
"Anand A.; Bhatt N.; Aggrawal D.","Anand, Adarsh (56647624300); Bhatt, Navneet (57203618455); Aggrawal, Deepti (55967578900)","56647624300; 57203618455; 55967578900","Modeling Software Patch Management Based on Vulnerabilities Discovered","2020","International Journal of Reliability, Quality and Safety Engineering","27","2","2040003","","","","","10.1142/S0218539320400033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072293142&doi=10.1142%2fS0218539320400033&partnerID=40&md5=a068b9adb681879d5e7388710e78446c","A software system deals with various security implications after its release in the market. Correspondingly, firm releases security patches to counter those flaws discovered in the software system. A vendor releases a patch only if a vulnerability has been discovered in a software. It is an important aspect that encompasses the prediction of potential number of patches to be released to maintain the stability of a software. Vulnerability Discovery Models (VDMs) help a software vendor to acknowledge the security trends, forecast security investments and to plan patches, but very few attempts have been made to model the Vulnerability Patch Modeling (VPM) based on the impact of vulnerabilities discovered over the time period. In this proposal, we deduce a novel approach that addresses trend in the sequential development of patches based on the vendor or reporters fetching out the vulnerabilities in a software. The vulnerability trends in a software significantly affect the discovery process and later trigger a patch deployment to suppress the possible likelihood of a breach. The integrative approach underlines the association of vulnerability patch modeling with the vulnerability discovery phenomenon. To exemplify the proposed systematic structure, a statistical analysis has been conducted using real life vulnerability and patch datasets. © 2020 World Scientific Publishing Company.","Patch; security; updates; vulnerability; vulnerability discovery models","Computer software; Patch; security; updates; vulnerability; Vulnerability discovery; Investments","Article","Final","","Scopus","2-s2.0-85072293142"
"Jabeen G.; Yang X.; Luo P.","Jabeen, Gul (54791058900); Yang, Xi (57202029754); Luo, Ping (55705278900)","54791058900; 57202029754; 55705278900","Vulnerability severity prediction model for software based on Markov chain","2021","International Journal of Information and Computer Security","15","2-3","","109","140","31","","10.1504/IJICS.2021.116302","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111134084&doi=10.1504%2fIJICS.2021.116302&partnerID=40&md5=50566be0f0819d263795ef147798d4d8","Software vulnerabilities primarily constitute security risks. Commonalities between faults and vulnerabilities prompt developers to utilise traditional fault prediction models and metrics for vulnerability prediction. Although traditional models can predict the number of vulnerabilities and their occurrence time, they fail to accurately determine the seriousness of vulnerabilities, impacts, and severity level. To address these deficits, we propose a method for predicting software vulnerabilities based on a Markov chain model, which offers a more comprehensive descriptive model with the potential to accurately predict vulnerability type, i.e., the seriousness of the vulnerabilities. The experiments are performed using real vulnerability data of three types of popular software: Windows 10, Adobe Flash Player and Firefox. Our model is shown to produce accurate predictive results. © 2021 Inderscience Enterprises Ltd.","Markov chain; Prediction model; Severity/seriousness; Software security; Software vulnerability; VL","Forecasting; Markov chains; Descriptive Model; Fault prediction models; Firefox; Markov chain models; Prediction model; Security risks; Software vulnerabilities; Traditional models; Predictive analytics","Article","Final","","Scopus","2-s2.0-85111134084"
"Koruyeh E.M.; Haji Amin Shirazi S.; Khasawneh K.N.; Song C.; Abu-Ghazaleh N.","Koruyeh, Esmaeil Mohammadian (57209458375); Haji Amin Shirazi, Shirin (57219115155); Khasawneh, Khaled N. (57009157500); Song, Chengyu (41562187300); Abu-Ghazaleh, Nael (6701804109)","57209458375; 57219115155; 57009157500; 41562187300; 6701804109","SpecCFI: Mitigating spectre attacks using CFI informed speculation","2020","Proceedings - IEEE Symposium on Security and Privacy","2020-May","","9152786","39","53","14","","10.1109/SP40000.2020.00033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091592948&doi=10.1109%2fSP40000.2020.00033&partnerID=40&md5=820d6fbd2e6cc4461203eb5ba65ece08","Spectre attacks and their many subsequent variants are a new vulnerability class affecting modern CPUs. The attacks rely on the ability to misguide speculative execution, generally by exploiting the branch prediction structures, to execute a vulnerable code sequence speculatively. In this paper, we propose to use Control-Flow Integrity (CFI), a security technique used to stop control-flow hijacking attacks, on the committed path, to prevent speculative control-flow from being hijacked to launch the most dangerous variants of the Spectre attacks (Spectre-BTB and Spectre-RSB). Specifically, CFI attempts to constrain the possible targets of an indirect branch to a set of legal targets defined by a pre-calculated control-flow graph (CFG). As CFI is being adopted by commodity software (e.g., Windows and Android) and commodity hardware (e.g., Intel's CET and ARM's BTI), the CFI information becomes readily available through the hardware CFI extensions. With the CFI information, we apply CFI principles to also constrain illegal control-flow during speculative execution. Specifically, our proposed defense, SpecCFI, ensures that control flow instructions target legal destinations to constrain dangerous speculation on forward control-flow paths (indirect calls and branches). We augment this protection with a precise speculation-aware hardware stack to constrain speculation on backward control-flow edges (returns). We combine this solution with existing solutions against branch target predictor attacks (Spectre-PHT) to close all known non-vendor-specific Spectre vulnerabilities. We show that SpecCFI results in small overheads both in terms of performance and additional hardware complexity. © 2020 IEEE.","","Data flow analysis; Program processors; Branch prediction; Commodity hardware; Commodity software; Control flow graphs; Control-flow integrities; Hardware complexity; Speculative control; Speculative execution; Flow graphs","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85091592948"
"Ahakonye L.A.C.; Nwakanma C.I.; Lee J.-M.; Kim D.-S.","Ahakonye, Love Allen Chijioke (57362087800); Nwakanma, Cosmas Ifeanyi (57211502137); Lee, Jae-Min (57194451001); Kim, Dong-Seong (55586876300)","57362087800; 57211502137; 57194451001; 55586876300","Efficient Classification of Enciphered SCADA Network Traffic in Smart Factory Using Decision Tree Algorithm","2021","IEEE Access","9","","","154892","154901","9","","10.1109/ACCESS.2021.3127560","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119452276&doi=10.1109%2fACCESS.2021.3127560&partnerID=40&md5=c56705e0493d4ffdf34e0f1f9df69f32","Vulnerability detection in Supervisory Control and Data Acquisition (SCADA) network of a Smart Factory (SF) is a high-priority research area in the cyber-security domain. Choosing an efficient Machine Learning (ML) algorithm for intrusion detection is a huge challenge. This study performed an investigative analysis into the classification ability of various ML models leveraging public cyber-security datasets to determine the best model. Based on the performance evaluation, all adaptions of Decision Tree (DT) and KNN in terms of accuracy, training time, MCE, and prediction speed are the most suitable ML for resolving security issues in the SCADA system. © 2013 IEEE.","Algorithms; artificial intelligence; machine learning; SCADA systems","Artificial intelligence; Classification (of information); Data mining; Decision trees; Intrusion detection; Learning algorithms; Learning systems; Network security; Software testing; Computational modelling; Cyber security; Data acquisition networks; Decision-tree algorithm; Machine-learning; Network traffic; Security; Smart manufacturing; Software algorithms; Supervisory control and data acquisition; SCADA systems","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85119452276"
"Sultana K.Z.; Codabux Z.; Williams B.","Sultana, Kazi Zakia (23494078600); Codabux, Zadia (55893535500); Williams, Byron (24478168500)","23494078600; 55893535500; 24478168500","Examining the relationship of code and architectural smells with software vulnerabilities","2020","Proceedings - Asia-Pacific Software Engineering Conference, APSEC","2020-December","","9359268","31","40","9","","10.1109/APSEC51365.2020.00011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102366441&doi=10.1109%2fAPSEC51365.2020.00011&partnerID=40&md5=7af878cd7cf68052f6a24a9872206c15","Context: Security is vital to software developed for commercial or personal use. Although more organizations are realizing the importance of applying secure coding practices, in many of them, security concerns are not known or addressed until a security failure occurs. The root cause of security failures is vulnerable code. While metrics have been used to predict software vulnerabilities, we explore the relationship between code and architectural smells with security weaknesses. As smells are surface indicators of a deeper problem in software, determining the relationship between smells and software vulnerabilities can play a significant role in vulnerability prediction models. Objective: This study explores the relationship between smells and software vulnerabilities to identify the smells. Method: We extracted the class, method, file, and package level smells for three systems: Apache Tomcat, Apache CXF, and Android. We then compared their occurrences in the vulnerable classes which were reported to contain vulnerable code and in the neutral classes (non-vulnerable classes where no vulnerability had yet been reported). Results: We found that a vulnerable class is more likely to have certain smells compared to a non-vulnerable class. God Class, Complex Class, Large Class, Data Class, Feature Envy, Brain Class have a statistically significant relationship with software vulnerabilities. We found no significant relationship between architectural smells and software vulnerabilities. Conclusion: We can conclude that for all the systems examined, there is a statistically significant correlation between software vulnerabilities and some smells.  © 2020 IEEE.","Architectural Smell; Code Smell; Software Security; Vulnerability","Network security; Predictive analytics; Software engineering; Apache tomcats; Complex class; Package levels; Prediction model; Secure coding; Security failure; Security weakness; Software vulnerabilities; Odors","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85102366441"
"Del Vecchio C.; Molitierno C.; Di Ludovico M.; Verderame G.M.; Prota A.; Manfredi G.","Del Vecchio, C. (56278059800); Molitierno, C. (57226495106); Di Ludovico, M. (23469290900); Verderame, G.M. (6506827751); Prota, A. (6603458266); Manfredi, G. (7005193798)","56278059800; 57226495106; 23469290900; 6506827751; 6603458266; 7005193798","Experimental response and numerical modelling of two-storey infilled RC frame","2021","COMPDYN Proceedings","2021-June","","","","","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120787703&partnerID=40&md5=e1afd5e9d12e80234b411293c0aa46ff","Recent devastating earthquakes pointed out the high vulnerability of existing reinforced concrete (RC) buildings and the critical role of infills. The presence of stiff infills may significantly modify the lateral response of RC moment resisting frame buildings and cause severe damage on the surrounding RC members due to seismic actions transmitted at level of beam-column joints. Furthermore, due to their brittle response, they commonly exhibited severe damage often leading to high economic losses. In this context, effective seismic retrofit strategies should aim at both increasing the shear strength of RC members and reducing the damage to infills. However only few tests are available in literature addressing the seismic strengthening of existing RC frames accounting for the infill-to-structure interaction. To fill this gap a comprehensive experimental program of pseudo-dynamic tests on full-scale two-storey infilled RC frames has been designed and is currently ongoing at the full-scale laboratory testing of the University of Napoli Federico II. This paper reports the preliminary experimental results and numerical analyses carried out by using available non-linear models accounting for the infills contribution. The comparison between theoretical predictions and experimental results provides useful insights to improve the numerical models to reproduce the infill-to-strut interaction. Finally, a retrofit strategy to improve the seismic performance of the structural system and reducing the expected damage to infills is herein outlined. © 2021 COMPDYN Proceedings.","FRP; Full-scale; Pseudo-dynamic; Shear failure","Computational methods; Earthquake engineering; Engineering geology; Failure (mechanical); Infill drilling; Losses; Numerical models; Retrofitting; Seismology; Software testing; Existing reinforced concrete; FRP; Full-scale; Lateral response; Pseudo-dynamics; Reinforced concrete buildings; Reinforced concrete frames; Reinforced concrete member; Retrofit strategies; Shear failure; Reinforced concrete","Conference paper","Final","","Scopus","2-s2.0-85120787703"
"Theisen C.; Williams L.","Theisen, Christopher (57216482869); Williams, Laurie (35565101900)","57216482869; 35565101900","Better together: Comparing vulnerability prediction models","2020","Information and Software Technology","119","","106204","","","","","10.1016/j.infsof.2019.106204","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075264352&doi=10.1016%2fj.infsof.2019.106204&partnerID=40&md5=f58cf86f8225738505bd83d9c3716e75","Context: Vulnerability Prediction Models (VPMs) are an approach for prioritizing security inspection and testing to find and fix vulnerabilities. VPMs have been created based on a variety of metrics and approaches, yet widespread adoption of VPM usage in practice has not occurred. Knowing which VPMs have strong prediction and which VPMs have low data requirements and resources usage would be useful for practitioners to match VPMs to their project's needs. The low density of vulnerabilities compared to defects is also an obstacle for practical VPMs. Objective: The goal of the paper is to help security practitioners and researchers choose appropriate features for vulnerability prediction through a comparison of Vulnerability Prediction Models. Method: We performed replications of VPMs on Mozilla Firefox with 28,750 source code files featuring 271 vulnerabilities using software metrics, text mining, and crash data. We then combined features from each VPM and reran our classifiers. Results: We improved the F-score of the best VPM (.20 to 0.28) by combining features from three types of VPMs and using Naive Bayes as the classifier. The strongest features in the combined model were the number of times a file was involved in a crash, the number of outgoing calls from a file, and the string “nullptr”. Conclusion: Our results indicate that further work is needed to develop new features for input into classifiers. In addition, new analytic approaches for VPMs are needed for VPMs to be useful in practical situations, due to the low density of vulnerabilities in software (less than 1% for our dataset). © 2019","Prediction model; Security; Software engineering; Vulnerabilities","Forecasting; Security of data; Software engineering; Analytic approach; Combined features; Data requirements; Prediction model; Security; Security practitioners; Software metrics; Vulnerabilities; Classification (of information)","Article","Final","","Scopus","2-s2.0-85075264352"
"Meng D.; Guerriero M.; MacHiry A.; Aghakhani H.; Bose P.; Continella A.; Kruegel C.; Vigna G.","Meng, Dongyu (57200512870); Guerriero, Michele (57162975400); MacHiry, Aravind (55848928300); Aghakhani, Hojjat (57203548141); Bose, Priyanka (56963503200); Continella, Andrea (56524430700); Kruegel, Christopher (14017971800); Vigna, Giovanni (7003452317)","57200512870; 57162975400; 55848928300; 57203548141; 56963503200; 56524430700; 14017971800; 7003452317","Bran: Reduce Vulnerability Search Space in Large Open Source Repositories by Learning Bug Symptoms","2021","ASIA CCS 2021 - Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security","","","","731","743","12","","10.1145/3433210.3453115","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108074399&doi=10.1145%2f3433210.3453115&partnerID=40&md5=bea42dcec6f01612153c0297b64e8349","Software is continually increasing in size and complexity, and therefore, vulnerability discovery would benefit from techniques that identify potentially vulnerable regions within large code bases, as this allows for easing vulnerability detection by reducing the search space. Previous work has explored the use of conventional code-quality and complexity metrics in highlighting suspicious sections of (source) code. Recently, researchers also proposed to reduce the vulnerability search space by studying code properties with neural networks. However, previous work generally failed in leveraging the rich metadata that is available for long-running, large code repositories. In this paper, we present an approach, named Bran, to reduce the vulnerability search space by combining conventional code metrics with fine-grained repository metadata. Bran locates code sections that are more likely to contain vulnerabilities in large code bases, potentially improving the efficiency of both manual and automatic code audits. In our experiments on four large code bases, Bran successfully highlights potentially vulnerable functions, outperforming several baselines, including state-of-art vulnerability prediction tools. We also assess Bran's effectiveness in assisting automated testing tools. We use Bran to guide syzkaller, a known kernel fuzzer, in fuzzing a recent version of the Linux kernel. The guided fuzzer identifies 26 bugs (10 are zero-day flaws), including arbitrary writes and reads. © 2021 Owner/Author.","machine learning; static analysis; vulnerabilities","Complex networks; Linux; Metadata; Open source software; Automated testing tools; Complexity metrics; Large code basis; Open source repositories; Prediction tools; Vulnerability detection; Vulnerability discovery; Vulnerable regions; Codes (symbols)","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85108074399"
"Filus K.; Siavvas M.; Domańska J.; Gelenbe E.","Filus, Katarzyna (57217079434); Siavvas, Miltiadis (57194500913); Domańska, Joanna (16038785400); Gelenbe, Erol (7006026729)","57217079434; 57194500913; 16038785400; 7006026729","The Random Neural Network as a Bonding Model for Software Vulnerability Prediction","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12527 LNCS","","","102","116","14","","10.1007/978-3-030-68110-4_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101843542&doi=10.1007%2f978-3-030-68110-4_7&partnerID=40&md5=c79a10d5921b16e3b6d9366c5c615995","Software vulnerability prediction is an important and active area of research where new methods are needed to build accurate and efficient tools that can identify security issues. Thus we propose an approach based on mixed features that combines text mining features and the features generated using a Static Code Analyzer. We use a Random Neural Network as a bonding model that combines the text analysis that is carried out on software using a Convolutional Neural Network, and the outputs of Static Code Analysis. The proposed approach was evaluated on commonly used datasets and led to 97% training accuracy, and 93%–94% testing accuracy, with a 1% reduction in false positives with respect to previously published results on similar data sets. © 2021, Springer Nature Switzerland AG.","Convolutional neural networks; Machine learning; Random neural networks; Software vulnerability prediction; Text mining","Computer software; Convolutional neural networks; Text mining; False positive; Random neural network; Security issues; Similar datum; Software vulnerabilities; Static code analysis; Testing accuracy; Training accuracy; Network security","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85101843542"
"Wang S.; Liu W.; Chang C.-H.","Wang, Si (57215695538); Liu, Wenye (57197792597); Chang, Chip-Hong (7407038192)","57215695538; 57197792597; 7407038192","A New Lightweight in Situ Adversarial Sample Detector for Edge Deep Neural Network","2021","IEEE Journal on Emerging and Selected Topics in Circuits and Systems","11","2","9416697","252","266","14","","10.1109/JETCAS.2021.3076101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105112486&doi=10.1109%2fJETCAS.2021.3076101&partnerID=40&md5=3bfcf0a8a31ccd24ddc7e163c5037173","The flourishing of Internet of Things (IoT) has rekindled on-premise computing to allow data to be analyzed closer to the source. To support edge Artificial Intelligence (AI), hardware accelerators, open-source AI model compilers and commercially available toolkits have evolved to facilitate the development and deployment of applications that use AI at its core. This paradigm shift in deep learning computations does not, however, reduce the vulnerability of deep neural networks (DNN) against adversarial attacks but introduces a difficult catch-up. This is because existing methodologies rely mainly on off-line analysis to detect adversarial inputs, assuming that the deep learning model is implemented on a 32-bit floating-point graphical processing unit (GPU) instance. In this paper, we propose a new hardware-oriented approach for in-situ detection of adversarial inputs feeding through a spatial DNN accelerator architecture or a third-party DNN Intellectual Property (IP) implemented on the edge. Our method exploits controlled glitch injection into the clock signal of the DNN accelerator to maximize the information gain for the discrimination of adversarial and benign inputs. A light gradient boosting machine (lightGBM) is constructed by analyzing the prediction probability of unmutated and mutated models and the label change inconsistency between the adversarial and benign samples in the training dataset. With negligibly small hardware overhead, the glitch injection circuit and the trained lightGBM detector can be easily implemented alongside with the deep learning model on a Xilinx ZU9EG chip. The effectiveness of the proposed detector is validated against four state-of-the-art adversarial attacks on two different types and scales of DNN models, VGG16 and ResNet50, for a thousand-class visual object recognition application. The results show a significant increase in true positive rate and a substantial reduction in false positive rate on the Fast Gradient Sign Method (FGSM), Iterative-FGSM (I-FGSM), CW and universal perturbation attacks compared with modern software-oriented adversarial sample detection methods. © 2011 IEEE.","adversarial examples; AI security; deep learning accelerator; Deep neural network; edge AI","Deep neural networks; Digital arithmetic; Graphics processing unit; Internet of things; Iterative methods; Learning systems; Neural networks; Object recognition; Open source software; Open systems; Accelerator architectures; False positive rates; Graphical processing units; Hardware accelerators; Internet of Things (IOT); Prediction probabilities; Substantial reduction; Visual object recognition; Deep learning","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85105112486"
"Huang S.-Y.; Ban T.","Huang, Shin-Ying (55842631300); Ban, Tao (7202924921)","55842631300; 7202924921","Monitoring social media for vulnerability-threat prediction and topic analysis","2020","Proceedings - 2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications, TrustCom 2020","","","9343128","1771","1776","5","","10.1109/TrustCom50675.2020.00243","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101224145&doi=10.1109%2fTrustCom50675.2020.00243&partnerID=40&md5=2ae210f17450d1e33ced40e4acf1da78","Publicly available software vulnerabilities and exploit code are often abused by malicious actors to launch cyberattacks to vulnerable targets. Organizations not only have to update their software to the latest versions, but do effective patch management and prioritize security-related patching as well. In addition to intelligence sources such as Computer Emergency Response Team (CERT) alerts, cybersecurity news, national vulnerability database (NBD), and commercial cybersecurity vendors, social media is another valuable source that facilitates early stage intelligence gathering. To early detect future cyber threats based on publicly available resources on the Internet, we propose a dynamic vulnerability-threat assessment model to predict the tendency to be exploited for vulnerability entries listed in Common Vulnerability Exposures, and also to analyze social media contents such as Twitter to extract meaningful information. The model takes multiple aspects of vulnerabilities gathered from different sources into consideration. Features range from profile information to contextual information about these vulnerabilities. For the social media data, this study leverages machine learning techniques specially for Twitter which helps to filter out non-cybersecurity-related tweets and also label the topic categories of each tweet. When applied to predict the vulnerabilities exploitation and analyzed the real-world social media discussion data, it showed promising prediction accuracy with purified social media intelligence. Moreover, the AI-enabling modules have been deployed into a threat intelligence platform for further applications. © 2020 IEEE.","Machine learning; Social media; Threat intelligence; Vulnerability exploit prediction","Forecasting; Learning systems; Privacy by design; Computer emergency response teams; Contextual information; Intelligence gathering; Intelligence sources; Machine learning techniques; National vulnerability database; Software vulnerabilities; Threat assessment models; Social networking (online)","Conference paper","Final","","Scopus","2-s2.0-85101224145"
"Zheng W.; Chen Z.; Wu X.; Fu W.; Sun B.; Cheng J.","Zheng, Wei (56650413300); Chen, Zheng (57771418300); Wu, Xiaoxue (56650282600); Fu, Weiqiang (57220740414); Sun, Bowen (57427246900); Cheng, Jingyuan (57426567000)","56650413300; 57771418300; 56650282600; 57220740414; 57427246900; 57426567000","A Domain Knowledge-Guided Lightweight Approach for Security Bug Reports Prediction","2021","Proceedings - 2021 8th International Conference on Dependable Systems and Their Applications, DSA 2021","","","","359","368","9","","10.1109/DSA52907.2021.00055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123492277&doi=10.1109%2fDSA52907.2021.00055&partnerID=40&md5=9b1bcf38e138c663e9b343d5a5e75879","Security bug report (SBR) prediction has been increasingly investigated for eliminating security attack risks of software products. However, there is still much room for improving the performance of automatic SBR prediction. This work is inspired by the work of two recent studies proposed by Peters et al. and Wu et al., which are focused on SBR prediction and both published on the top tier journal TSE (Transactions on Software Engineering). The goal of this work is to improve the effectiveness of supervised machine learning-based SBR prediction with the help of software security domain knowledge. It first extracts software security domain knowledge from CWE (Common Weakness Enumeration) and CVE (Common Vulnerabilities Exposure), which are authoritative sources of software vulnerability. After that, the matrix of bug reports is generated based on the roots of security domain keywords. Large-scale experiments are conducted on a set of trustworthy datasets cleaned by Wu et al. The results show our domain knowledge-guided approach could improve the effectiveness of SBR prediction by 25% in terms of F1-score on average. © 2021 IEEE.","bug report prediction; domain knowledge; keyBERT; keywords root; software security","Domain Knowledge; Large dataset; Supervised learning; Bug report prediction; Bug reports; Domain knowledge; KeyBERT; Keyword root; Security attacks; Security bugs; Security domains; Software products; Software security; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85123492277"
"Liu R.; Mubang F.; Hall L.O.","Liu, Renhao (57192674123); Mubang, Frederick (57212476338); Hall, Lawrence O. (57203365478)","57192674123; 57212476338; 57203365478","Simulating Temporal User Activity on Social Networks with Sequence to Sequence Neural Models","2020","Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics","2020-October","","9283257","1677","1684","7","","10.1109/SMC42975.2020.9283257","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098867303&doi=10.1109%2fSMC42975.2020.9283257&partnerID=40&md5=ed03f4d9e2e9947f13110d751d7bebb6","The prediction of long-term activities of groups of users and clusters of activities around a subject in social networks is a very challenging task. In this paper, we propose a novel temporal neural network framework that tracks user engagement and activity associated with particular subjects (e.g. CVE IDs) across online platforms. The framework is able to simulate which user will do what activity and at what time. Furthermore, this framework captures groups of users reacting to an event. It also captures responses to an event on a platform and the influence of the event on activity on other platforms over time. The proposed framework aims to predict future user activity related to specific subjects across platforms. The framework also illustrates the importance influence of activities that occur on other platforms when predicting user activity for particular events on a different platform. The learned model can do simulations in a timely manner. We evaluated our user group activity prediction method on the CVE (Common Vulnerabilities and Exposures) related user groups (software vulnerability) using 3 public online social network datasets: Github, Reddit, and Twitter. Groups of users who work on a particular CVE ID are identified. Each user group has information on all users' activities related to a CVE ID. The 3 datasets from Github, Reddit, and Twitter contain more than 490,000 cross platform activities related to over 20,000 user groups (CVE IDs) from more than 50,000 users. Compared to the proposed baseline, our simulation method is better in both predictions of total activity volume over time and activity associated with an individual CVE ID. © 2020 IEEE.","","Forecasting; Network security; Social sciences computing; Activity volumes; Common vulnerabilities and exposures; Cross-platform; On-line social networks; Online platforms; Software vulnerabilities; Temporal neural networks; User engagement; Social networking (online)","Conference paper","Final","","Scopus","2-s2.0-85098867303"
"Suneja S.; Zheng Y.; Zhuang Y.; Laredo J.A.; Morari A.","Suneja, Sahil (55813198500); Zheng, Yunhui (55901243300); Zhuang, Yufan (57219684931); Laredo, Jim A. (57197580620); Morari, Alessandro (51665633100)","55813198500; 55901243300; 57219684931; 57197580620; 51665633100","Probing model signal-awareness via prediction-preserving input minimization","2021","ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering","","","","945","955","10","","10.1145/3468264.3468545","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116225134&doi=10.1145%2f3468264.3468545&partnerID=40&md5=bc1ed2f8f1a094e2b96d809a4cc86774","This work explores the signal awareness of AI models for source code understanding. Using a software vulnerability detection use case, we evaluate the models' ability to capture the correct vulnerability signals to produce their predictions. Our prediction-preserving input minimization (P2IM) approach systematically reduces the original source code to a minimal snippet which a model needs to maintain its prediction. The model's reliance on incorrect signals is then uncovered when the vulnerability in the original code is missing in the minimal snippet, both of which the model however predicts as being vulnerable. We measure the signal awareness of models using a new metric we propose-Signal-aware Recall (SAR). We apply P2IM on three different neural network architectures across multiple datasets. The results show a sharp drop in the model's Recall from the high 90s to sub-60s with the new metric, highlighting that the models are presumably picking up a lot of noise or dataset nuances while learning their vulnerability detection logic. Although the drop in model performance may be perceived as an adversarial attack, but this isn't P2IM's objective. The idea is rather to uncover the signal-awareness of a black-box model in a data-driven manner via controlled queries. SAR's purpose is to measure the impact of task-agnostic model training, and not to suggest a shortcoming in the Recall metric. The expectation, in fact, is for SAR to match Recall in the ideal scenario where the model truly captures task-specific signals.  © 2021 ACM.","machine learning; model signal-awareness; signal-aware recall","Drops; Machine learning; Network architecture; Code understanding; Minimisation; Model signal-awareness; Model signals; Modeling abilities; Neural network architecture; Signal-aware recall; Software vulnerabilities; Source codes; Vulnerability detection; Forecasting","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85116225134"
"Jacobs J.; Romanosky S.; Edwards B.; Adjerid I.; Roytman M.","Jacobs, Jay (57193069657); Romanosky, Sasha (15521454700); Edwards, Benjamin (56431268400); Adjerid, Idris (55603956700); Roytman, Michael (55072531200)","57193069657; 15521454700; 56431268400; 55603956700; 55072531200","Exploit Prediction Scoring System (EPSS)","2021","Digital Threats: Research and Practice","2","3","3436242","","","","","10.1145/3436242","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123774865&doi=10.1145%2f3436242&partnerID=40&md5=1e73201625ff5fc1cd2bcc12fa0538d6","Despite the large investments in information security technologies and research over the past decades, the information security industry is still immature when it comes to vulnerability management. In particular, the prioritization of remediation efforts within vulnerability management programs predominantly relies on a mixture of subjective expert opinion and severity scores. Compounding the need for prioritization is the increase in the number of vulnerabilities the average enterprise has to remediate. This article describes the first open, data-driven framework for assessing vulnerability threat, that is, the probability that a vulnerability will be exploited in the wild within the first 12 months after public disclosure. This scoring system has been designed to be simple enough to be implemented by practitioners without specialized tools or software yet provides accurate estimates (ROC AUC = 0.838) of exploitation. Moreover, the implementation is flexible enough that it can be updated as more, and better, data becomes available. We call this system the Exploit Prediction Scoring System (EPSS). © 2021 Owner/Author.","EPSS; machine learning; vulnerability exploits; Vulnerability management","Security of data; Data driven; Expert opinion; Information security technologies; Prioritization; Public disclosures; Scoring systems; Specialized tools; Vulnerability management; Remediation","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85123774865"
"Bagheri A.; Hegedűs P.","Bagheri, Amirreza (57233413900); Hegedűs, Péter (25926433300)","57233413900; 25926433300","A Comparison of Different Source Code Representation Methods for Vulnerability Prediction in Python","2021","Communications in Computer and Information Science","1439 CCIS","","","267","281","14","","10.1007/978-3-030-85347-1_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115244647&doi=10.1007%2f978-3-030-85347-1_20&partnerID=40&md5=54e77c0bd13e0c12ef3476581d71f485","In the age of big data and machine learning, at a time when the techniques and methods of software development are evolving rapidly, a problem has arisen: programmers can no longer detect all the security flaws and vulnerabilities in their code manually. To overcome this problem, developers can now rely on automatic techniques, like machine learning based prediction models, to detect such issues. An inherent property of such approaches is that they work with numeric vectors (i.e., feature vectors) as inputs. Therefore, one needs to transform the source code into such feature vectors, often referred to as code embedding. A popular approach for code embedding is to adapt natural language processing techniques, like text representation, to automatically derive the necessary features from the source code. However, the suitability and comparison of different text representation techniques for solving Software Engineering (SE) problems is rarely studied systematically. In this paper, we present a comparative study on three popular text representation methods, word2vec, fastText, and BERT applied to the SE task of detecting vulnerabilities in Python code. Using a data mining approach, we collected a large volume of Python source code in both vulnerable and fixed forms that we embedded with word2vec, fastText, and BERT to vectors and used a Long Short-Term Memory network to train on them. Using the same LSTM architecture, we could compare the efficiency of the different embeddings in deriving meaningful feature vectors. Our findings show that all the text representation methods are suitable for code representation in this particular task, but the BERT model is the most promising as it is the least time consuming and the LSTM model based on it achieved the best overall accuracy (93.8%) in predicting Python source code vulnerabilities. © 2021, Springer Nature Switzerland AG.","Code embedding; Comparative study; Machine learning; Vulnerability prediction","Codes (symbols); Data mining; Embeddings; Forecasting; High level languages; Machine learning; Natural language processing systems; Predictive analytics; Software design; Vectors; Automatic technique; Code representation; Comparative studies; NAtural language processing; Overall accuracies; Short term memory; Source code representations; Text representation; Long short-term memory","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85115244647"
"Viszkok T.; Hegedus P.; Ferenc R.","Viszkok, Tamás (57217150009); Hegedus, Péter (25926433300); Ferenc, Rudolf (6603559878)","57217150009; 25926433300; 6603559878","Improving vulnerability prediction of javascript functions using process metrics","2021","Proceedings of the 16th International Conference on Software Technologies, ICSOFT 2021","","","","185","195","10","","10.5220/0010558501850195","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111746129&doi=10.5220%2f0010558501850195&partnerID=40&md5=f193611be1a15f3bd94765108205f29e","Due to the growing number of cyber attacks against computer systems, we need to pay special attention to the security of our software systems. In order to maximize the effectiveness, excluding the human component from this process would be a huge breakthrough. The first step towards this is to automatically recognize the vulnerable parts in our code. Researchers put a lot of effort into creating machine learning models that could determine if a given piece of code, or to be more precise, a selected function, contains any vulnerabilities or not. We aim at improving the existing models, building on previous results in predicting vulnerabilities at the level of functions in JavaScript code using the well-known static source code metrics. In this work, we propose to include several so-called process metrics (e.g., code churn, number of developers modifying a file, or the age of the changed source code) into the set of features, and examine how they affect the performance of the function-level JavaScript vulnerability prediction models. We can confirm that process metrics significantly improve the prediction power of such models. On average, we observed a 8.4% improvement in terms of F-measure (from 0.764 to 0.848), 3.5% improvement in terms of precision (from 0.953 to 0.988) and a 6.3% improvement in terms of recall (from 0.697 to 0.760). Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved","JavaScript Security; Process Metrics; Static Source Code Metrics; Vulnerability Prediction","Forecasting; High level languages; Network security; Cyber-attacks; Function levels; Human components; Machine learning models; Prediction model; Process metrics; Software systems; Static sources; Predictive analytics","Conference paper","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85111746129"
"Mosolygo B.; Vandor N.; Antal G.; Hegedus P.; Ferenc R.","Mosolygo, Balazs (57219332393); Vandor, Norbert (57219330885); Antal, Gabor (6701427737); Hegedus, Peter (25926433300); Ferenc, Rudolf (6603559878)","57219332393; 57219330885; 6701427737; 25926433300; 6603559878","Towards a prototype based explainable javascript vulnerability prediction model","2021","2021 International Conference on Code Quality, ICCQ 2021","","","9392984","15","25","10","","10.1109/ICCQ51190.2021.9392984","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104651668&doi=10.1109%2fICCQ51190.2021.9392984&partnerID=40&md5=28faf79ccbd38418f14e1e7a5832d48b","Security has become a central and unavoidable aspect of today's software development. Practitioners and researchers have proposed many code analysis tools and techniques to mitigate security risks. These tools apply static and dynamic analysis or, more recently, machine learning. Machine learning models can achieve impressive results in finding and forecasting possible security issues in programs. However, most of the current approaches fall short of developer demands in two areas at least: Explainability and granularity of predictions. In this paper, we propose a novel and simple yet, promising approach to identify potentially vulnerable source code in JavaScript programs. The model improves the state-of-the-art in terms of explainability and prediction granularity as it gives results at the level of individual source code lines, which is fine-grained enough for developers to take immediate actions. Additionally, the model explains each predicted line (i.e., provides the most similar vulnerable line from the training set) using a prototype-based approach. In a study of 186 real-world and confirmed JavaScript vulnerability fixes of 91 projects, the approach could flag 60% of the known vulnerable lines on average by marking only 10% of the code-base, but in particular cases, the model identified 100% of the vulnerable code lines while flagging only 8.72% of the code-base.  © 2021 IEEE.","CVE; data mining; explainable ML; software security; vulnerability prediction","Forecasting; High level languages; Machine learning; Risk assessment; Software design; JavaScript programs; Machine learning models; Prediction model; Security issues; Security risks; Source code lines; State of the art; Static and dynamic analysis; Predictive analytics","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85104651668"
"Das S.S.; Serra E.; Halappanavar M.; Pothen A.; Al-Shaer E.","Das, Siddhartha Shankar (57222359971); Serra, Edoardo (35812641000); Halappanavar, Mahantesh (49661237100); Pothen, Alex (6602851606); Al-Shaer, Ehab (6602187244)","57222359971; 35812641000; 49661237100; 6602851606; 6602187244","V2W-BERT: A Framework for Effective Hierarchical Multiclass Classification of Software Vulnerabilities","2021","2021 IEEE 8th International Conference on Data Science and Advanced Analytics, DSAA 2021","","","","","","","","10.1109/DSAA53316.2021.9564227","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126096845&doi=10.1109%2fDSAA53316.2021.9564227&partnerID=40&md5=4f29c21faf7f2bf4cb42508c0d07cdcd","We consider the problem of automating the mapping of observed vulnerabilities in software listed in Common Vulnerabilities and Exposures (CVE) reports to weaknesses listed in Common Weakness Enumerations (CWE) reports, a hierarchically designed dictionary of software weaknesses. Mapping of CVEs to CWEs provides a means to understand how they might be exploited for malicious purposes, and to mitigate their impact. Since manual mapping of CVEs to CWEs is not a viable approach due to their ever-increasing sizes, automated approaches need to be devised but obtaining highly accurate mapping is a challenging problem. We present a novel Transformer-based learning framework (V2W-BERT) in this paper to solve this problem by bringing together ideas from natural language processing, link prediction and transfer learning. Our method outperforms previous approaches not only for CWE instances with abundant data to train, but also for rare CWE classes with little or no data. Using vulnerability and weakness reports from MITRE and the National Vulnerability Database, we achieve up to 97% prediction accuracy for randomly partitioned data and up to 94% prediction accuracy in temporally partitioned data. We demonstrate significant improvements in using historical data to predict weaknesses for future instances of CVEs. We believe that our work will would influence the design of better automated mapping approaches, and also that this technology could be deployed for more effective cybersecurity. © 2021 IEEE.","Cyber-security; Link Prediction; Transformer","Classification (of information); Cybersecurity; Forecasting; Learning systems; Natural language processing systems; Automated approach; Common vulnerabilities and exposures; Cyber security; Highly accurate; Link prediction; Manual mapping; Multi-class classification; Prediction accuracy; Software vulnerabilities; Transformer; Mapping","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85126096845"
"Chakraborty S.; Krishna R.; Ding Y.; Ray B.","Chakraborty, Saikat (57193152367); Krishna, Rahul (57282246700); Ding, Yangruibo (57221142654); Ray, Baishakhi (24492560400)","57193152367; 57282246700; 57221142654; 24492560400","Deep Learning based Vulnerability Detection: Are We There Yet","2021","IEEE Transactions on Software Engineering","","","","","","","","10.1109/TSE.2021.3087402","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111011696&doi=10.1109%2fTSE.2021.3087402&partnerID=40&md5=7ed8e9b6182392aa420a50a1a516ac16","Automated detection of software vulnerabilities is a fundamental problem in software security. Existing program analysis techniques either suffer from high false positives or false negatives. Recent progress in Deep Learning (DL) has resulted in a surge of interest in applying DL for automated vulnerability detection. Several recent studies have demonstrated promising results achieving an accuracy of up to 95% at detecting vulnerabilities. In this paper, we ask, ""how well do the state-of-the-art DL-based techniques perform in a real-world vulnerability prediction scenario"". To our surprise, we find that their performance drops by more than 50%. A systematic investigation of what causes such precipitous performance drop reveals that existing DL-based vulnerability prediction approaches suffer from challenges with the training data (e.g., data duplication, unrealistic distribution of vulnerable classes, etc.) and with the model choices (e.g., simple token-based models). As a result, these approaches often do not learn features related to the actual cause of the vulnerabilities. Instead, they learn unrelated artifacts from the dataset (e.g., specific variable/function names, etc.). Leveraging these empirical findings, we demonstrate how a more principled approach to data collection and model design, based on realistic settings of vulnerability prediction, can lead to better solutions. The resulting tools perform significantly better than the studied baseline up to 33.57% boost in precision and 128.38% boost in recall compared to the best performing model in the literature. Overall, this paper elucidates existing DL-based vulnerability prediction systems' potential issues and draws a roadmap for future DL-based vulnerability prediction research. In that spirit, we make available all the artifacts supporting our results: https://git.io/Jf6IA. IEEE","Data models; datasets; gaze detection; Neural networks; neural networks; Predictive models; Security; Testing; text tagging; Training; Training data","Drops; Forecasting; Automated detection; Data duplication; Empirical findings; Prediction research; Prediction systems; Software security; Software vulnerabilities; Vulnerability detection; Deep learning","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85111011696"
"Sultana K.Z.; Anu V.; Chong T.-Y.","Sultana, Kazi Zakia (23494078600); Anu, Vaibhav (57191251672); Chong, Tai-Yin (57215345965)","23494078600; 57191251672; 57215345965","Using software metrics for predicting vulnerable classes and methods in Java projects: A machine learning approach","2021","Journal of Software: Evolution and Process","33","3","e2303","","","","","10.1002/smr.2303","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089099524&doi=10.1002%2fsmr.2303&partnerID=40&md5=9424c1faa5e5138d2e672bcd8f28c1a2","[Context]A software vulnerability becomes harmful for software when an attacker successfully exploits the insecure code and reveals the vulnerability. A single vulnerability in code can put the entire software at risk. Therefore, maintaining software security throughout the software life cycle is an important and at the same time challenging task for development teams. This can also leave the door open for vulnerable code being evolved during successive releases. In recent years, researchers have used software metrics-based vulnerability prediction approaches to detect vulnerable code early and ensure secure code releases. Software metrics have been employed to predict vulnerability specifically in C/C++ and Java-based systems. However, the prediction performance of metrics at different granularity levels (class level or method level) has not been analyzed. In this paper, we focused on metrics that are specific to lower granularity levels (Java classes and methods). Based on statistical analysis, we first identified a set of class-level metrics and a set of method-level metrics and then employed them as features in machine learning techniques to predict vulnerable classes and methods, respectively. This paper describes a comparative study on how our selected metrics perform at different granularity levels. Such a comparative study can help the developers in choosing the appropriate metrics (at the desired level of granularity). [Objective] The goal of this research is to propose a set of metrics at two lower granularity levels and provide evidence for their usefulness during vulnerability prediction (which will help in maintaining secure code and ensure secure software evolution). [Method] For four Java-based open source systems (including two releases of Apache Tomcat), we designed and conducted experiments based on statistical tests to propose a set of software metrics that can be used for predicting vulnerable code components (i.e., vulnerable classes and methods). Next, we used our identified metrics as features to train supervised machine learning algorithms to classify Java code as vulnerable or non-vulnerable. [Result] Our study has successfully identified a set of class-level metrics and a second set of method-level metrics that can be useful from a vulnerability prediction standpoint. We achieved recall higher than 70% and precision higher than 75% in vulnerability prediction using our identified class-level metrics as features of machine learning. Furthermore, method-level metrics showed recall higher than 65% and precision higher than 80%. © 2020 John Wiley & Sons, Ltd.","software evolution; software maintenance; software metrics; software security; vulnerability prediction","Forecasting; Java programming language; Learning algorithms; Learning systems; Life cycle; Open source software; Open systems; Software testing; Supervised learning; Different granularities; Machine learning approaches; Machine learning techniques; Prediction performance; Single vulnerabilities; Software life cycles; Software vulnerabilities; Supervised machine learning; C++ (programming language)","Article","Final","","Scopus","2-s2.0-85089099524"
"Wood P.; Furman J.; Monreal R.","Wood, Paul (57206810962); Furman, Judith (8322993500); Monreal, Roberto (54397495900)","57206810962; 8322993500; 54397495900","Predicted vs. Actual Bit Error Rates for MMS HPCA Memories and Their Impact on Software Reliability","2021","IEEE Aerospace Conference Proceedings","2021-March","","9438146","","","","","10.1109/AERO50100.2021.9438146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111380012&doi=10.1109%2fAERO50100.2021.9438146&partnerID=40&md5=58d3803d2d30a875be6101da4414249c","Magnetospheric Multiscale (MMS) is a constellation of four observatories examining Earth's magnetosphere. The observatories have an elliptical orbit that currently takes them out to 30 RE. Each observatory includes a Hot Plasma Composition Analyzer (HPCA) instrument to measure particle flux for key particles in the magnetosphere and solar wind. During design, HPCA was analyzed to determine its potential vulnerability to energetic protons and heavy ions sourced from the trapped radiation belts, Galactic Cosmic Rays (GCR) or Solar Events. These agents are responsible for the Single Event Effects (SEE), manifested as single and multi-bit errors in its memories. Now, with five years of experience across four observatories, we examine the predictions vs. experience with these memories to determine the accuracy of those predictions and the implications for the models we use for those bit error rate predictions. In addition, we examine the impact of these events on system operation and software reliability. © 2021 IEEE.","","Bit error rate; Cosmology; Errors; Forecasting; Heavy ions; Observatories; Orbits; Radiation belts; Trapped ions; Earth's magnetosphere; Elliptical orbits; Energetic protons; Galactic cosmic rays; Magnetospheric multi scale; Multi-bit error; Single event effects; System operation; Software reliability","Conference paper","Final","","Scopus","2-s2.0-85111380012"
"Chen X.; Zhao Y.; Cui Z.; Meng G.; Liu Y.; Wang Z.","Chen, Xiang (57189091783); Zhao, Yingquan (57195352686); Cui, Zhanqi (35172605800); Meng, Guozhu (56747189200); Liu, Yang (56911879800); Wang, Zan (35216436800)","57189091783; 57195352686; 35172605800; 56747189200; 56911879800; 35216436800","Large-Scale Empirical Studies on Effort-Aware Security Vulnerability Prediction Methods","2020","IEEE Transactions on Reliability","69","1","8809906","70","87","17","","10.1109/TR.2019.2924932","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081969937&doi=10.1109%2fTR.2019.2924932&partnerID=40&md5=a978a89038d6adc574db64a5954c2a2c","Security vulnerability prediction (SVP) can identify potential vulnerable modules in advance and then help developers to allocate most of the test resources to these modules. To evaluate the performance of different SVP methods, we should take the security audit and code inspection into account and then consider effort-aware performance measures (such as $ACC$ and $P_{\rm opt}$). However, to the best of our knowledge, the effectiveness of different SVP methods has not been thoroughly investigated in terms of effort-aware performance measures. In this article, we consider 48 different SVP methods, of which 36 are supervised methods and 12 are unsupervised methods. For the supervised methods, we consider 34 software-metric-based methods and two text-mining-based methods. For the software-metric-based methods, in addition to a large number of classification methods, we also consider four state-of-the-art methods (i.e., EALR, OneWay, CBS, and MULTI) proposed in recent effort-aware just-in-time defect prediction studies. For text-mining-based methods, we consider the Bag-of-Word model and the term-frequency-inverse-document-frequency model. For the unsupervised methods, all the modules are ranked in the ascendent order based on a specific metric. Since 12 software metrics are considered when measuring extracted modules, there are 12 different unsupervised methods. To the best of our knowledge, over 40 SVP methods have not been considered in previous SVP studies. In our large-scale empirical studies, we use three real open-source web applications written in PHP as benchmark. These three web applications include 3466 modules and 223 vulnerabilities in total. We evaluate these SVP methods both in the within-project SVP scenario and the cross-project SVP scenario. Empirical results show that two unsupervised methods [i.e., lines of code (LOC) and Halstead's volume (HV)] and four recently proposed state-of-the-art supervised methods (i.e., MULTI, OneWay, CBS, and EALR) can achieve better performance than the other methods in terms of effort-aware performance measures. Then, we analyze the reasons why these six methods can achieve better performance. For example, when using 20% of the entire efforts, we find that these six methods always require more modules to be inspected, especially for unsupervised methods LOC and HV. Finally, from the view of practical vulnerability localization, we find that all the unsupervised methods and the OneWay method have high false alarms before finding the first vulnerable module. This may have an impact on developers' confidence and tolerance, and supervised methods (especially MULTI and text-mining-based methods) are preferred. © 1963-2012 IEEE.","Effort-aware performance measures; security vulnerability prediction (SVP); software metric; supervised method; text mining; unsupervised method","Benchmarking; Electric circuit breakers; Forecasting; Open source software; Supervised learning; Text mining; Performance measure; Security vulnerabilities; Software metrices; Supervised methods; Unsupervised method; Inverse problems","Article","Final","","Scopus","2-s2.0-85081969937"
"Nguyen H.N.; Teerakanok S.; Inomata A.; Uehara T.","Nguyen, Hai Ngoc (57221013039); Teerakanok, Songpon (37015690500); Inomata, Atsuo (8950100000); Uehara, Tetsutaro (8215150700)","57221013039; 37015690500; 8950100000; 8215150700","The comparison of word embedding techniques in RNNS for vulnerability detection","2021","ICISSP 2021 - Proceedings of the 7th International Conference on Information Systems Security and Privacy","","","","109","120","11","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103065550&partnerID=40&md5=82e72453308e8ed994f46dfa2128fa6e","Many studies have combined Deep Learning and Natural Language Processing (NLP) techniques in security systems in performing tasks such as bug detection, vulnerability prediction, or classification. Most of these works relied on NLP embedding methods to generate input vectors for the deep learning models. However, there are many existing embedding methods to encode software text files into vectors, and the structures of neural networks are immense and heuristic. This leads to a challenge for the researcher to choose the appropriate combination of embedding techniques and the model structure for training the vulnerability detection classifiers. For this task, we propose a system to investigate the use of four popular word embedding techniques combined with four different recurrent neural networks (RNNs), including both bidirectional RNNs (BRNNs) and unidirectional RNNs. We trained and evaluated the models by using two types of vulnerable function datasets written in C code. Our results showed that the FastText embedding technique combined with BRNNs produced the most efficient detection rate, compared to other combinations, on a real-world but not on an artificially-produced dataset. Further experiments on other datasets are necessary to confirm this result. Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved model also requires distinguished amounts of time consumed for training and testing on different types of representations. To select the suitable embedding method can be a critical task since it can affect the performance of the models and the time complexity of training and detection. Thus, we aim to determine the most viable combination between certain sequence models and available embedding methods for generating semantic vectors.","Deep Learning; RNNs; Vulnerability Detection; Word Embeddings","Embeddings; Heuristic methods; Information systems; Information use; Learning systems; Privacy by design; Recurrent neural networks; Bug detection; Efficient detection; Embedding method; Embedding technique; Learning models; NAtural language processing; Recurrent neural network (RNNs); Vulnerability detection; Natural language processing systems","Conference paper","Final","","Scopus","2-s2.0-85103065550"
"Nikoloudakis Y.; Kefaloukos I.; Klados S.; Panagiotakis S.; Pallis E.; Skianis C.; Markakis E.K.","Nikoloudakis, Yannis (57191261491); Kefaloukos, Ioannis (57211409220); Klados, Stylianos (57226142551); Panagiotakis, Spyros (57131428900); Pallis, Evangelos (6508217585); Skianis, Charalabos (15124619200); Markakis, Evangelos K. (7006033121)","57191261491; 57211409220; 57226142551; 57131428900; 6508217585; 15124619200; 7006033121","Towards a machine learning based situational awareness framework for cybersecurity: An SDN implementation","2021","Sensors","21","14","4939","","","","","10.3390/s21144939","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110630899&doi=10.3390%2fs21144939&partnerID=40&md5=090ecd5c9c6657f422ad69f82f34c7d3","The ever-increasing number of internet-connected devices, along with the continuous evolution of cyber-attacks, in terms of volume and ingenuity, has led to a widened cyber-threat landscape, rendering infrastructures prone to malicious attacks. Towards addressing systems’ vulnerabilities and alleviating the impact of these threats, this paper presents a machine learning based situational awareness framework that detects existing and newly introduced network-enabled entities, utilizing the real-time awareness feature provided by the SDN paradigm, assesses them against known vulnerabilities, and assigns them to a connectivity-appropriate network slice. The assessed entities are continuously monitored by an ML-based IDS, which is trained with an enhanced dataset. Our endeavor aims to demonstrate that a neural network, trained with heterogeneous data stemming from the operational environment (common vulnerability enumeration IDs that correlate attacks with existing vulnerabilities), can achieve more accurate prediction rates than a conventional one, thus addressing some aspects of the situational awareness paradigm. The proposed framework was evaluated within a real-life environment and the results revealed an increase of more than 4% in the overall prediction accuracy. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Intrusion detection systems; Machine learning; SDN; Situational awareness; Software defined networking; Vulnerability assessment","Awareness; Computer Security; Machine Learning; Neural Networks, Computer; Machine learning; Real time systems; Turing machines; Accurate prediction; Cyber security; Heterogeneous data; Malicious attack; Operational environments; Prediction accuracy; Rendering infrastructure; Situational awareness; awareness; computer security; machine learning; Network security","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85110630899"
"Yang H.; Park S.; Yim K.; Lee M.","Yang, Heedong (57209618143); Park, Seungsoo (57208879439); Yim, Kangbin (7005983616); Lee, Manhee (56143648600)","57209618143; 57208879439; 7005983616; 56143648600","Better not to use vulnerability's reference for exploitability prediction","2020","Applied Sciences (Switzerland)","10","7","2555","","","","","10.3390/app10072555","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083466499&doi=10.3390%2fapp10072555&partnerID=40&md5=1597bf3977a9abdc0d83261d3da8544e","About half of all exploit codes will become available within about two weeks of the release date of its vulnerability. However, 80% of the released vulnerabilities are never exploited. Since putting the same effort to eliminate all vulnerabilities can be somewhat wasteful, software companies usually use different methods to assess which vulnerability is more serious and needs an immediate patch. Recently, there have been some attempts to usemachine learning techniques to predict a vulnerability's exploitability. In doing so, a vulnerability's related URL, called its reference, is commonly used as a machine learning algorithm's feature. However, we found that some references contained proof-of-concept codes. In this paper, we analyzed all references in the National Vulnerability Database and found that 46,202 of them contained such codes. We compared prediction performances between feature matrix with and without reference information. Experimental results showed that test sets that used references containing proof-of-concept codes had better prediction performance than ones that used references without such codes. Even though the difference is not huge, it is clear that references having answer information contributed to the prediction performance, which is not desirable. Thus, it is better not to use reference information to predict vulnerability exploitation. © 2020 by the authors.","CVE; Exploit; Machine learning; PoC; Prediction","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85083466499"
"Passe U.; Dorneich M.; Krejci C.; Koupaei D.M.; Marmur B.; Shenk L.; Stonewall J.; Thompson J.; Zhou Y.","Passe, Ulrike (36996216000); Dorneich, Michael (6701777807); Krejci, Caroline (36903627300); Koupaei, Diba Malekpour (57212458653); Marmur, Breanna (57144445800); Shenk, Linda (56174053300); Stonewall, Jacklin (57194727963); Thompson, Janette (56576750600); Zhou, Yuyu (7405365321)","36996216000; 6701777807; 36903627300; 57212458653; 57144445800; 56174053300; 57194727963; 56576750600; 7405365321","An urban modelling framework for climate resilience in low-resource neighbourhoods","2020","Buildings and Cities","1","1","","453","474","21","","10.5334/bc.17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118718395&doi=10.5334%2fbc.17&partnerID=40&md5=2c545a75396af8999812725eba3c6077","Climate predictions indicate a strong likelihood of more frequent, intense heat events. Resource-vulnerable, low-income neighbourhood populations are likely to be strongly impacted by future climate change, especially with respect to an energy burden. In order to identify existing and new vulnerabilities to climate change, local authorities need to understand the dynamics of extreme heat events at the neighbourhood level, particularly to identify those people who are adversely affected. A new comprehensive framework is presented that integrates human and biophysical data: occupancy/behaviour, building energy use, future climate scenarios and near-building microclimate projections. The framework is used to create an urban energy model for a low-resource neighbourhood in Des Moines, Iowa, US. Data were integrated into urban modelling interface (umi) software simulations, based on detailed surveys of residents’ practices, their buildings and near-building microclimates (tree canopy effects, etc.). The simulations predict annual and seasonal building energy use in response to different climate scenarios. Preliminary results, based on 50 simulation runs with different variable combinations, indicate the importance of using locally derived building occupant schedules and point toward increased summer cooling demand and increased vulnerability for parts of the population. Practice relevance To support planning responses to increased heat, local authorities need to ascertain which neighbourhoods will be negatively impacted in order to develop appropriate strategies. Localised data can provide good insights into the impacts of human decisions and climate variability in low-resource, vulnerable urban neighbourhoods. A new detailed modelling framework synthesises data on occupant–building interactions with present and future urban climate characteristics. This identifies the areas most vulnerable to extreme heat using future climate projections and community demographics. Cities can use this framework to support decisions and climate-adaptation responses, especially for low-resource neighbourhoods. Fine-grained and locally collected data influence the outcome of combined urban energy simulations that integrate human–building interactions and occupancy schedules as well as microclimate characteristics influenced by nearby vegetation. © 2020 The Author(s).","cities; heat stress; microclimate; neighbourhood; occupancy data; overheating; urban modelling; vulnerability","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85118718395"
"Aumpansub A.; Huang Z.","Aumpansub, Amy (57225126144); Huang, Zhen (57226054108)","57225126144; 57226054108","Detecting Software Vulnerabilities Using Neural Networks","2021","ACM International Conference Proceeding Series","","","3457707","166","171","5","","10.1145/3457682.3457707","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109216384&doi=10.1145%2f3457682.3457707&partnerID=40&md5=859b2e29813e7cca0bf7623f38f29ad1","As software vulnerabilities remain prevalent, automatically detecting software vulnerabilities is crucial for software security. Recently neural networks have been shown to be a promising tool in detecting software vulnerabilities. In this paper, we use neural networks trained with program slices, which extract the syntax and semantic characteristics of the source code of programs, to detect software vulnerabilities in C/C++ programs. To achieve a strong prediction model, we combine different types of program slices and optimize different types of neural networks. Our result shows that combining different types of characteristics of source code and using a balanced ratio of vulnerable program slices and non-vulnerable program slices a balanced accuracy in predicting both vulnerable code and non-vulnerable code. Among different neural networks, BGRU performs the best in detecting software vulnerabilities with an accuracy of 94.89%. © 2021 ACM.","deep learning; neural network; software vulnerability; vulnerability detection","C++ (programming language); Machine learning; Predictive analytics; Semantics; C/C++ programs; Prediction model; Program slice; Software security; Software vulnerabilities; Source codes; Neural networks","Conference paper","Final","","Scopus","2-s2.0-85109216384"
"Williams M.A.; Barranco R.C.; Naim S.M.; Dey S.; Shahriar Hossain M.; Akbar M.","Williams, Mark A. (57207577231); Barranco, Roberto Camacho (57193631043); Naim, Sheikh Motahar (6603067408); Dey, Sumi (57207570029); Shahriar Hossain, M. (57194772843); Akbar, Monika (23979147700)","57207577231; 57193631043; 6603067408; 57207570029; 57194772843; 23979147700","A vulnerability analysis and prediction framework","2020","Computers and Security","92","","101751","","","","","10.1016/j.cose.2020.101751","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080037848&doi=10.1016%2fj.cose.2020.101751&partnerID=40&md5=aa33ff3f15728a4926b16a34446a7ab8","As the world approaches a state of greater dependence on technology, many products face increasing threats from malicious attackers who are attempting to take advantage of vulnerabilities in software design. Most of the known vulnerability information is already aggregated, stored in text format, and readily accessible to the public, making such an aggregated database a prime corpus for analysis using data mining methods. Multiple research efforts have been launched in which individual aspects of such cyber-security corpora were analyzed to create taxonomies, assess vulnerability impact, and improve vulnerability detection. However, minimal effort has been committed to analyze cyber-security corpora to explore correlations between vulnerabilities, to study the evolution of a vulnerability from its genesis, and to predict vulnerabilities using multi-faceted algorithms. In this paper, we propose an integrated data mining framework to automatically describe how vulnerabilities develop over time and detect the evolution of a specific vulnerability. Additionally, our framework has a predictive functionality that can be used to predict specific vulnerabilities or to estimate future appearance probabilities of vulnerability groups. In our framework, we use (1) a Topically Supervised Evolution Model (TSEM) that can discover temporal themes from a text corpus, (2) a diffusion-based storytelling technique that sifts through past vulnerability reports to describe how a current vulnerability threat evolved, and (3) several prediction models that use features from a cyber-security corpus to predict vulnerabilities. A series of experiments demonstrate that the proposed framework can not only discover evolutionary patterns in today's most pressing vulnerabilities with a high degree of precision, but it can also predict vulnerabilities with impressive accuracy. As case studies, we also explore the development of vulnerabilities in certain products, providing a unique insight into the correspondence between seemingly unrelated vulnerabilities and the impact of that correspondence on overall software security. © 2020 Elsevier Ltd","Cyber-security; Deep neural networks; Regression; Storytelling; Temporal topic modeling; Vulnerabilities","Deep neural networks; Forecasting; Product design; Software design; Text mining; Cyber security; Regression; Storytelling; Topic Modeling; Vulnerabilities; Security of data","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85080037848"
"Zheng W.; Zhang M.; Tang H.; Cai Y.; Chen X.; Wu X.; Semasaba A.O.A.","Zheng, Wei (56650413300); Zhang, Manqing (57210928814); Tang, Hui (57705920300); Cai, Yuanfang (23003221700); Chen, Xiang (57189091783); Wu, Xiaoxue (56650282600); Semasaba, Abubakar Omari Abdallah (57220545690)","56650413300; 57210928814; 57705920300; 23003221700; 57189091783; 56650282600; 57220545690","Automatically Identifying Bug Reports with Tactical Vulnerabilities by Deep Feature Learning","2021","Proceedings - International Symposium on Software Reliability Engineering, ISSRE","2021-October","","","333","344","11","","10.1109/ISSRE52982.2021.00043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126391221&doi=10.1109%2fISSRE52982.2021.00043&partnerID=40&md5=cefdc9e905b9f7a5fc911195398c3838","Identifying and fixing bug reports with tactical vul-nerabilities in a timely and accurate manner is essential to ensure the security of the software architecture. Manually identifying the bug reports with tactical vulnerabilities is labor-intensive and challenging. This paper presents Itactivul, an approach to automatically identify bug reports with tactical vulnerabilities and recommend their tactical categories to guide the fix. Unlike the existing security bug report prediction approach, we are the first attempt to use deep learning to mine discriminative tactical text features only from the vulnerability descriptions of the National Vulnerability Database (NVD) and apply them to identify bug reports with tactical vulnerabilities. We evaluate Itactivul on three bug reports datasets gathered from three large-scale open-source projects, including Chromium, PHP, and Thunderbird. The experimental results show that Itactivul outperforms baselines by an average of 8.88 %, 13.58 %, and 6.61 % in the F1-score of three datasets, respectively. To improve the explainability of the features mined by Itactivul, we manually analyze the high-weight phrases extracted by using attention backtracking. The results show that Itactivul can mine key and potential tactical vulnerabilities text features.  © 2021 IEEE.","Bug Report; Model explainability; Tactical Vulnerability; Text mining","Large dataset; Open source software; Bug reports; Deep feature learning; Labour-intensive; Model explainability; National vulnerability database; Security bugs; Tactical vulnerability; Tacticals; Text feature; Vulnerability description; Deep learning","Conference paper","Final","","Scopus","2-s2.0-85126391221"
"Pekaric I.; Steinmüller P.; Felderer M.","Pekaric, Irdin (57194088382); Steinmüller, Philipp (57224827409); Felderer, Michael (24832720900)","57194088382; 57224827409; 24832720900","Vulnerlizer: Cross-analysis between vulnerabilities and software libraries","2021","Proceedings of the Annual Hawaii International Conference on System Sciences","2020-January","","","7015","7024","9","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108305171&partnerID=40&md5=e3229b5dd5f083109e6933cd7426764c","The identification of vulnerabilities is a continuous challenge in software projects. This is due to the evolution of methods that attackers employ as well as the constant updates to the software, which reveal additional issues. As a result, new and innovative approaches for the identification of vulnerable software are needed. In this paper, we present VULNERLIZER, which is a novel framework for cross-analysis between vulnerabilities and software libraries. It uses CVE (Common Vulnerabilities and Exposures) and software library data together with clustering algorithms to generate links between vulnerabilities and libraries. In addition, the training of the model is conducted in order to reevaluate the generated associations. This is achieved by updating the assigned weights. Finally, the approach is evaluated by making the predictions using the CVE data from the test set. The results show a great potential of the approach in predicting future vulnerable libraries based on an initial input CVE entry or a software library. The trained model reaches a prediction accuracy of 75% or higher. © 2021 IEEE Computer Society. All rights reserved.","","Clustering algorithms; Common vulnerabilities and exposures; Innovative approaches; Prediction accuracy; Software libraries; Software project; Test sets; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85108305171"
"Jafarian T.; Masdari M.; Ghaffari A.; Majidzadeh K.","Jafarian, Tohid (57218209148); Masdari, Mohammad (55206798900); Ghaffari, Ali (57197223215); Majidzadeh, Kambiz (54880938600)","57218209148; 55206798900; 57197223215; 54880938600","Security anomaly detection in software-defined networking based on a prediction technique","2020","International Journal of Communication Systems","33","14","e4524","","","","","10.1002/dac.4524","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088293630&doi=10.1002%2fdac.4524&partnerID=40&md5=34a682e172039541f71a9437956f44d0","Nowadays, software-defined networking (SDN) is regarded as the best solution for the centralized handling and monitoring of large networks. However, it should be noted that SDN architecture suffers from the same security issues, which are the case with common networks. As a case in point, one of the shortcomings of SDNs is related to its high vulnerability to distributed denial of service (DDoS) attacks and other similar ones. Indeed, anomaly detection systems have been considered to deal with these attacks. The challenges are related to designing these systems including gathering data, extracting effective features, and selecting the best model for anomaly detection. In this paper, a novel combined approach is proposed; this method uses NetFlow protocol for gathering information and generating dataset, information gain ratio (IGR), in order to select the effective and relevant features and ensemble learning scheme (Stacking) for developing a structure with desirable performance and efficiency for detecting anomaly in SDN environment. The results obtained from the experiments revealed that the proposed method performs better than other methods in terms of enhancing accuracy (AC) and detection rate (DR) and reducing classification error (CE) and false alarm rate (FAR). The AC, DR, CE, and FAR of the proposed model were measured as 99.92%, 99.83%, 0.08%, and 0.03%, respectively. Furthermore, the proposed method prevents the occurrence of excessive overload on the controller and OpenFlow. © 2020 John Wiley & Sons, Ltd.","anomaly detection; ensemble learning; floodlight; machine learning (ML); NetFlow; SDN; stacking","Denial-of-service attack; Feature extraction; Network security; Software defined networking; Anomaly detection systems; Classification errors; Distributed denial of service attack; Ensemble learning; Information gain ratio; Prediction techniques; Relevant features; Software defined networking (SDN); Anomaly detection","Article","Final","","Scopus","2-s2.0-85088293630"
"Ganesh S.; Ohlsson T.; Palma F.","Ganesh, Sundarakrishnan (57535400300); Ohlsson, Tobias (57435024300); Palma, Francis (55329588400)","57535400300; 57435024300; 55329588400","Predicting Security Vulnerabilities using Source Code Metrics","2021","Proceedings of the 2021 Swedish Workshop on Data Science, SweDS 2021","","","","","","","","10.1109/SweDS53855.2021.9638301","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123869704&doi=10.1109%2fSweDS53855.2021.9638301&partnerID=40&md5=f96fd6c775e323aaf92a95471ad219ae","Large open-source systems generate and operate on a plethora of sensitive enterprise data. Thus, security threats or vulnerabilities must not be present in open-source systems and must be resolved as early as possible in the development phases to avoid catastrophic consequences. One way to recognize security vulnerabilities is to predict them while developers write code to minimize costs and resources. This study examines the effectiveness of machine learning algorithms to predict potential security vulnerabilities by analyzing the source code of a system. We obtained the security vulnerabilities dataset from Apache Tomcat security reports for version 4.x to 10.x. We also collected the source code of Apache Tomcat 4.x to 10.x to compute 43 object-oriented metrics. We assessed four traditional supervised learning algorithms, i.e., Naive Bayes (NB), Decision Tree (DT), K-Nearest Neighbors (KNN), and Logistic Regression (LR), to understand their efficacy in predicting security vulnerabilities. We obtained the highest accuracy of 80.6% using the KNN. Thus, the KNN classifier was demonstrated to be the most effective of all the models we built. The DT classifier also performed well but under-performed when it came to multi-class classification.  © 2021 IEEE.","Machine Learning; Prediction; Security Vulnerabilities; Software Metrics; Source Code","Classifiers; Codes (symbols); Decision trees; Learning algorithms; Logistic regression; Machine learning; Nearest neighbor search; Object oriented programming; Open Data; Open source software; Open systems; Apache tomcats; Enterprise data; Machine-learning; Nearest-neighbour; Open source system; Security threats; Security vulnerabilities; Software metrics; Source code metrics; Source codes; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85123869704"
"Babalau I.; Corlatescu D.; Grigorescu O.; Sandescu C.; Dascalu M.","Babalau, Ion (57549253400); Corlatescu, Dragos (57200423109); Grigorescu, Octavian (57192821634); Sandescu, Cristian (36537934000); Dascalu, Mihai (24831962300)","57549253400; 57200423109; 57192821634; 36537934000; 24831962300","Severity Prediction of Software Vulnerabilities based on their Text Description","2021","Proceedings - 2021 23rd International Symposium on Symbolic and Numeric Algorithms for Scientific Computing, SYNASC 2021","","","","171","177","6","","10.1109/SYNASC54541.2021.00037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127053496&doi=10.1109%2fSYNASC54541.2021.00037&partnerID=40&md5=23201a09eecb62e4b31060ba26d71dd5","Software vulnerabilities represent a real challenge nowadays, often resulting in disruption of vital systems and data loss. Due to the multitude of software applications used within a company, system administrators often end up in the situation of facing multiple vulnerabilities at the same time, having no choice but to prioritize the most critical ones. Administrators commonly use vulnerability databases and metric systems to rank vulnerabilities; however, it usually takes from days to weeks for the metrics to be published since these metrics are established by human security analysts and the number of daily discovered exploits is constantly increasing. Therefore, newly discovered vulnerabilities, especially those without an available patch, represent the largest problem. In this paper, we propose a deep learning approach to predict the severity score and other metrics of a vulnerability using only its text description, which is available on discovery. We use a Multi-Task Learning architecture with a pre-trained BERT model for computing vector-space representations of words. Our best configuration achieves a mean absolute error of 0.86 for the severity score and an accuracy of 71.55% for the severity level.  © 2021 IEEE.","BERT-based language model; CVE; CVSS; Multi-Task Learning; Natural Language Processing; Software vulnerability","Application programs; Deep learning; Vector spaces; BERT-based language model; CVE; CVSS; Data loss; Language model; Multitask learning; Software applications; Software vulnerabilities; System loss; Vital systems; Natural language processing systems","Conference paper","Final","","Scopus","2-s2.0-85127053496"
"Wu X.; Zheng W.; Chen X.; Wang F.; Mu D.","Wu, Xiaoxue (56650282600); Zheng, Wei (56650413300); Chen, Xiang (57189091783); Wang, Fang (57195438328); Mu, Dejun (56383464900)","56650282600; 56650413300; 57189091783; 57195438328; 56383464900","CVE-assisted large-scale security bug report dataset construction method","2020","Journal of Systems and Software","160","","110456","","","","","10.1016/j.jss.2019.110456","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074980613&doi=10.1016%2fj.jss.2019.110456&partnerID=40&md5=23e143e2efb99dffae256ae090910f7c","Identifying SBRs (security bug reports) is crucial for eliminating security issues during software development. Machine learning are promising ways for SBR prediction. However, the effectiveness of the state-of-the-art machine learning models depend on high-quality datasets, while gathering large-scale datasets are expensive and tedious. To solve this issue, we propose an automated data labeling approach based on iterative voting classification. It starts with a small group of ground-truth traing samples, which can be labeled with the help of authoritative vulnerability records hosted in CVE (Common Vulnerabilities and Exposures). The accuracy of the prediction model is improved with an iterative voting strategy. By using this approach, we label over 80k bug reports from OpenStack and 40k bug reports from Chromium. The correctness of these labels are then manually reviewed by three experienced security testing members. Finally, we construct a large-scale SBR dataset with 191 SBRs and 88,472 NSBRs (non-security bug reports) from OpenStack; and improve the quality of existing SBR dataset Chromium by identifying 64 new SBRs from previously labeled NSBRs and filtering out 173 noise bug reports from this dataset. These share datasets as well as the proposed dataset construction method help to promote research progress in SBR prediction research domain. © 2019 Elsevier Inc.","Common vulnerabilities and exposures; Dataset construction; Security bug report prediction; Voting classification","Chromium; Classification (of information); Forecasting; Iterative methods; Machine learning; Platform as a Service (PaaS); Software design; Bug reports; Common vulnerabilities and exposures; Construction method; Large-scale datasets; Machine learning models; Prediction research; Security testing; Voting strategies; Large dataset","Article","Final","","Scopus","2-s2.0-85074980613"
"Gunasekaran J.R.; Subhra Mishra C.","Gunasekaran, Jashwant Raj (55440506600); Subhra Mishra, Cyan (57419574800)","55440506600; 57419574800","MLPP: Exploring Transfer Learning and Model Distillation for Predicting Application Performance","2021","2021 IEEE International Conference on Networking, Architecture and Storage, NAS 2021 - Proceedings","","","","","","","","10.1109/NAS51552.2021.9605431","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123199701&doi=10.1109%2fNAS51552.2021.9605431&partnerID=40&md5=bd63f4004928607465e7b71ecabcd97c","Performance prediction for applications is quintessential towards detecting malicious hardware and software vulnerabilities. Typically application performance is predicted using the profiling data generated from hardware tools such as linux perf. By leveraging the data, prediction models, both machine learning (ML) based and non ML-based have been proposed. However a majority of these models suffer from either loss in prediction accuracy, very large model sizes, and/or lack of general applicability to different hardware types such as wearables, handhelds, desktops etc. To address the aforementioned inefficiencies, in this paper we proposed MLPP, a machine learning based performance prediction model which can accurately predict application performance, and at the same time be easily transferable to a wide both mobile and desktop hardware platforms by leveraging transfer learning technique. Furthermore, MLPP incorporates model distillation techniques to significantly reduce the model size. Through our extensive experimentation and evaluation we show that MLPP can achieve up to 92.5% prediction accuracy while reducing the model size by up to 3.5 ×.  © 2021 IEEE.","deep learning; fault detection; performance counter; transfer learning","Application programs; Computer operating systems; Deep learning; Distillation; Forecasting; Application performance; Deep learning; Faults detection; Hardware and software; Model size; Performance counters; Performance prediction; Prediction accuracy; Transfer learning; Transfer models; Fault detection","Conference paper","Final","","Scopus","2-s2.0-85123199701"
"Dam H.K.; Tran T.; Pham T.; Ng S.W.; Grundy J.; Ghose A.","Dam, Hoa Khanh (36605907400); Tran, Truyen (56948264600); Pham, Trang (57191227924); Ng, Shien Wee (57201157502); Grundy, John (7102156137); Ghose, Aditya (7103191960)","36605907400; 56948264600; 57191227924; 57201157502; 7102156137; 7103191960","Automatic Feature Learning for Predicting Vulnerable Software Components","2021","IEEE Transactions on Software Engineering","47","1","8540022","67","85","18","","10.1109/TSE.2018.2881961","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056740888&doi=10.1109%2fTSE.2018.2881961&partnerID=40&md5=14503f1ab61cc464c87cf150bafa5bb1","Code flaws or vulnerabilities are prevalent in software systems and can potentially cause a variety of problems including deadlock, hacking, information loss and system failure. A variety of approaches have been developed to try and detect the most likely locations of such code vulnerabilities in large code bases. Most of them rely on manually designing code features (e.g., complexity metrics or frequencies of code tokens) that represent the characteristics of the potentially problematic code to locate. However, all suffer from challenges in sufficiently capturing both semantic and syntactic representation of source code, an important capability for building accurate prediction models. In this paper, we describe a new approach, built upon the powerful deep learning Long Short Term Memory model, to automatically learn both semantic and syntactic features of code. Our evaluation on 18 Android applications and the Firefox application demonstrates that the prediction power obtained from our learned features is better than what is achieved by state of the art vulnerability prediction models, for both within-project prediction and cross-project prediction. © 1976-2012 IEEE.","empirical software engineering; mining software engineering repositories; Software vulnerability prediction","Computer software; Deep learning; Feature extraction; Forecasting; Personal computing; Semantics; Syntactics; Empirical Software Engineering; Engineering repository; Predictive models; Security; Software systems; Software vulnerabilities; System recovery; Predictive analytics","Article","Final","","Scopus","2-s2.0-85056740888"
"Chen Y.; Santosa A.E.; Yi A.M.; Sharma A.; Sharma A.; Lo D.","Chen, Yang (57219401915); Santosa, Andrew E. (6506343800); Yi, Ang Ming (57219532844); Sharma, Abhishek (57202966237); Sharma, Asankhaya (56298660300); Lo, David (35269388000)","57219401915; 6506343800; 57219532844; 57202966237; 56298660300; 35269388000","A Machine Learning Approach for Vulnerability Curation","2020","Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020","","","","32","42","10","","10.1145/3379597.3387461","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093688307&doi=10.1145%2f3379597.3387461&partnerID=40&md5=69ea6acf785954d248b95ec09c1add64","Software composition analysis depends on database of open-source library vulerabilities, curated by security researchers using various sources, such as bug tracking systems, commits, and mailing lists. We report the design and implementation of a machine learning system to help the curation by by automatically predicting the vulnerability-relatedness of each data item. It supports a complete pipeline from data collection, model training and prediction, to the validation of new models before deployment. It is executed iteratively to generate better models as new input data become available. We use self-training to significantly and automatically increase the size of the training dataset, opportunistically maximizing the improvement in the models' quality at each iteration. We devised new deployment stability metric to evaluate the quality of the new models before deployment into production, which helped to discover an error. We experimentally evaluate the improvement in the performance of the models in one iteration, with 27.59% maximum PR AUC improvements. Ours is the first of such study across a variety of data sources. We discover that the addition of the features of the corresponding commits to the features of issues/pull requests improve the precision for the recall values that matter. We demonstrate the effectiveness of self-training alone, with 10.50% PR AUC improvement, and we discover that there is no uniform ordering of word2vec parameters sensitivity across data sources.  © 2020 ACM.","application security; classifiers ensemble; machine learning; open-source software; self-training","Machine learning; Open source software; Open systems; Turing machines; Bug tracking system; Data collection; Design and implementations; Machine learning approaches; Model training; Open-source libraries; Software composition; Training dataset; Quality control","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85093688307"
"Anium S.","Anium, Shahid (57203936169)","57203936169","DoS Attacks, Triad and Privacy: Software Exposures in Microsoft, Apple and Google","2020","Proceedings - 2020 1st International Conference of Smart Systems and Emerging Technologies, SMART-TECH 2020","","","9283766","53","58","5","","10.1109/SMART-TECH49988.2020.00028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099279477&doi=10.1109%2fSMART-TECH49988.2020.00028&partnerID=40&md5=e90b4bfc4819e368a7f82302ec25a0dd","Internet age thrives on connectivity features and accessibility options, while denial of services attacks are detrimental to accessibility. In this context our research is making use of software exposures datasets from National Institute of Standards and Technology for analysis of denial of services (DoS) exposure of various products of three major vendors named as Microsoft (MS), Apple (AP) and Google (GO). A non-linear logit regression model has been used to force binary predictions for variety of DoS related vulnerabilities, given the values of independent variables established through the odd ratios of dependent variables including accessibility type, complexity and scores of attacks, authenticity level as well as confidentiality, accessibility and/or integrity violations. Interesting results compares and contrasts three vendors. Availability has bigger effects for two vendors (MS and AP) but not for GO in case of dependent variable DoS as well for all three vendors for dependent variable of (DoS +Info) vulnerability type which is also heavily influenced by Confidentiality in all three vendors, although impact on MS for both these independent variables on both of these dependent variables are too high as compared to other two vendors. Other heavy affects have been noted on (DoS +Priv) from Access and Integrity for MS and confidentiality for GO, on DoS Mem. Corr. from availability and on DoS Exec Code Overflow Mem. Corr. from complexity both for AP.  © 2020 IEEE.","Denial of Service; Logistic Regression; National Institute of Standards and Technology; Software Vendors; Triad Exposures","Fruits; Logistic regression; Privacy by design; Denial of Service; Denial-of-services attack; Dependent variables; Independent variables; Logit regression model; National Institute of Standards and Technology; Non linear; Odd ratios; Denial-of-service attack","Conference paper","Final","","Scopus","2-s2.0-85099279477"
"Song S.; Song C.; Jang Y.; Lee B.","Song, Suhwan (57220180816); Song, Chengyu (41562187300); Jang, Yeongjin (56417036900); Lee, Byoungyoung (56434753000)","57220180816; 41562187300; 56417036900; 56434753000","CrFuzz: Fuzzing multi-purpose programs through input validation","2020","ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering","","","","690","700","10","","10.1145/3368089.3409769","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097154742&doi=10.1145%2f3368089.3409769&partnerID=40&md5=6992078fbc2fbcc1fe395463644f95e1","Fuzz testing has been proved its effectiveness in discovering software vulnerabilities. Empowered its randomness nature along with a coverage-guiding feature, fuzzing has been identified a vast number of vulnerabilities in real-world programs. This paper begins with an observation that the design of the current state-of-the-art fuzzers is not well suited for a particular (but yet important) set of software programs. Specifically, current fuzzers have limitations in fuzzing programs serving multiple purposes, where each purpose is controlled by extra options. This paper proposes CrFuzz, which overcomes this limitation. CrFuzz designs a clustering analysis to automatically predict if a newly given input would be accepted or not by a target program. Exploiting this prediction capability, CrFuzz is designed to efficiently explore the programs with multiple purposes. We employed CrFuzz for three state-of-the-art fuzzers, AFL, QSYM, and MOpt, and CrFuzz-augmented versions have shown 19.3% and 5.68% better path and edge coverage on average. More importantly, during two weeks of long-running experiments, CrFuzz discovered 277 previously unknown vulnerabilities where 212 of those are already confirmed and fixed by the respected vendors. We would like to emphasize that many of these vulnerabilities were discoverd from FFMpeg, ImageMagick, and Graphicsmagick, all of which are targets of Google's OSS-Fuzz project and thus heavily fuzzed for last three years by far. Nevertheless, CrFuzz identified a remarkable number of vulnerabilities, demonstrating its effectiveness of vulnerability finding capability. © 2020 ACM.","Coverage-guided fuzzing; Fuzz testing","Engineering; Industrial engineering; Clustering analysis; Input validation; Prediction capability; Real world projects; Software program; Software vulnerabilities; State of the art; Vulnerability finding; Software testing","Conference paper","Final","","Scopus","2-s2.0-85097154742"
"Alenezi M.; Zagane M.; Javed Y.","Alenezi, Mamdouh (55854089000); Zagane, Mohammed (57216807396); Javed, Yasir (56983138200)","55854089000; 57216807396; 56983138200","Efficient deep features learning for vulnerability detection using character n-gram embedding","2021","Jordanian Journal of Computers and Information Technology","7","1","","25","38","13","","10.5455/jjcit.71-1597824949","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102940705&doi=10.5455%2fjjcit.71-1597824949&partnerID=40&md5=a9b000a17b5f066389647f4551b5d7bf","Deep Learning (DL) techniques were successfully applied to solve challenging problems in the field of Natural Language Processing (NLP). Since source code and natural text share several similarities, it was possible to adopt text classification techniques, such as word embedding, to propose DL-based Automatic Vulnerabilities Prediction (AVP) approaches. Although the obtained results were interesting, they were not good enough compared to those obtained in NLP. In this paper, we propose an improved DL-based AVP approach based on the technique of character n-gram embedding. We evaluate the proposed approach for 4 types of vulnerabilities using a large c/c++ open-source codebase. The results show that our approach can yield a very excellent performance which outperforms the performances obtained by previous approaches. © 2021, Scientific Research Support Fund of Jordan. All rights reserved.","Character N-gram embedding; Deep features learning; Software security; Vulnerability detection","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85102940705"
"Imtiaz S.; Amin M.R.; Do A.Q.; Iannucci S.; Bhowmik T.","Imtiaz, Sayem (57205018935); Amin, Md Rayhan (57420185400); Do, Anh Quoc (57197766556); Iannucci, Stefano (36518726700); Bhowmik, Tanmay (57679627000)","57205018935; 57420185400; 57197766556; 36518726700; 57679627000","Predicting Vulnerability for Requirements","2021","Proceedings - 2021 IEEE 22nd International Conference on Information Reuse and Integration for Data Science, IRI 2021","","","","160","167","7","","10.1109/IRI51335.2021.00028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123457371&doi=10.1109%2fIRI51335.2021.00028&partnerID=40&md5=7d6e95f898e71a6ada7662c44df0c258","Software security being one of the primary concerns in the software engineering community, researchers are coming up with many preemptive approaches aiming to minimize the vulnerabilities in the software. These approaches, dominated by static and dynamic analysis of the code often with machine learning (ML) techniques, are designed to detect vulnerabilities in the post-implementation stage of the software development life-cycle (SDLC). While they are found to be effective in detecting vulnerabilities, the consequences are often expensive. Accommodating changes after detecting a vulnerability in the system in later stages of the SDLC is very costly, sometimes even infeasible as it may involve changes in design or architecture. Moreover, the root of a vulnerability can often be traced back to the requirements specification. On that account, Imtiaz and Bhowmik have advocated a novel framework to provide an additional measure of predicting vulnerabilities at earlier stages of the SDLC. In this study, we build upon their proposed framework and leverage state-of-the-art ML algorithms to predict vulnerabilities for new requirements. We also present a case study on a large open-source-software (OSS) system, Firefox, evaluating the effectiveness of the extended prediction module. The results demonstrate that the framework could be a viable complement to the traditional yulnerability-fighting approaches.  © 2021 IEEE.","machine learning; multilabel classification; software security; vulnerability prediction for requirements","Classification (of information); Life cycle; Machine learning; Open source software; Open systems; Software design; Engineering community; Late stage; Machine learning techniques; Multi-label classifications; Post-implementation; Requirements specifications; Software development life-cycle; Software security; Static and dynamic analysis; Vulnerability prediction for requirement; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85123457371"
"Gagliardo R.; Terracciano G.; Cascini L.; Portioli F.; Landolfo R.","Gagliardo, R. (57203814786); Terracciano, G. (55246192200); Cascini, L. (49960952400); Portioli, F. (24076852400); Landolfo, R. (6701407971)","57203814786; 55246192200; 49960952400; 24076852400; 6701407971","The prediction of collapse mechanisms for masonry structures affected by ground movements using Rigid Block Limit Analysis","2020","Procedia Structural Integrity","29","","","48","54","6","","10.1016/j.prostr.2020.11.138","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102505512&doi=10.1016%2fj.prostr.2020.11.138&partnerID=40&md5=d1929a90c4cac3b88f7fbb7bc43e0f62","Masonry structures belonged to the Cultural Heritage suffered severe damages in the last decays due to the action of the settlementinduced ground movements. The researchers have been developing numerical tools for the vulnerability analysis and assessment of masonry structures subjected to settlements. Continuous, discrete and rigid block mod els were proposed in literature. The analysis of both local or global failure modes due to settlement is a still debated topic, involving several questions relate d to the modelling techniques and to the investigation of the parameters which affect the masonry behaviour against foundation movements. In this framework, the paper focuses on a numerical approach for the settlement analysis based on the rigid block limit analysis. The Italian Code (NTC 2018) also suggests linear kinematic approach for the seismic-induced collapse mechanisms analysis. In such a formulation, the structure is modelled as a collection of polyhedral rigid blocks assuming frictional contact interfac es with infinite compressive strength and zero tensile strength and neglecting the mortar contribution. Originally formulated for the inplane and out-of-plane mechanisms analysis, the numerical formulation was recently improved in order to analy ze blockystructures subjected to uniform settlement. Numerical case study of a monumental masonry church façade subjected to uniform settlement at the base was presented in this paper aiming at testing the numerical procedure. The results were discussed to evaluate the software capability and accuracy in the settlement-induced collapse mechanisms prediction.  © 2020 The Authors.","Collapse mechanism analysis; Masonry monumental structures; Mathematical programming; Rigid block limit analysis; Settlement vulnerability","","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85102505512"
"Zagane M.; Abdi M.K.; Alenezi M.","Zagane, Mohammed (57216807396); Abdi, Mustapha Kamel (56016662800); Alenezi, Mamdouh (55854089000)","57216807396; 56016662800; 55854089000","A New Approach to Locate Software Vulnerabilities Using Code Metrics","2020","International Journal of Software Innovation","8","3","","82","95","13","","10.4018/IJSI.2020070106","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087740212&doi=10.4018%2fIJSI.2020070106&partnerID=40&md5=c0a6200a0c4931f0ab00755f33e5ed09","Automatic vulnerabilities prediction assists developers and minimizes resources allocated to fix software security issues. These costs can be minimized even more if the exact location of vulnerability is correctly indicated. In this study, the authors propose a new approach to using code metrics in vulnerability detection. The strength part of the proposed approach lies in using code metrics not to simply quantify characteristics of software components at a coarse granularity (package, file, class, function) such as complexity, coupling, etc., which is the approach commonly used in previous studies, but to quantify extracted pieces of code that hint presence of vulnerabilities at a fine granularity (few lines of code). Obtained results show that code metrics can be used with a machine learning technique not only to indicate vulnerable components wish was the aim of previous approaches but also to detect and locate vulnerabilities with very good accuracy. © 2020 International Journal of Software Innovation.","Code Metrics; Machine Learning; Vulnerability Detection; Vulnerability Prediction Models","","Article","Final","","Scopus","2-s2.0-85087740212"
"Stokkenes S.; Strand R.D.; Kristensen L.M.; Log T.","Stokkenes, S. (57223997461); Strand, R.D. (57224007820); Kristensen, L.M. (56214742900); Log, T. (6701908841)","57223997461; 57224007820; 56214742900; 6701908841","Validation of a predictive fire risk indication model using cloud-based weather data services","2021","Procedia Computer Science","184","","","186","193","7","","10.1016/j.procs.2021.03.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106738354&doi=10.1016%2fj.procs.2021.03.029&partnerID=40&md5=1e0fb70d952f1617e5cfa6861804ef53","The high and dense representation of wooden homes in Norway, combined with periods of dry and cold climate during the winter season resulting in very dry indoor conditions, have historically resulted in severe fires. Thus, it is important to have an accurate estimate of the current and near future fire risk to take proper planning precautions. Cloud computing services providing access to weather data in the form of measurements and forecasts combined with recent developments in fire risk modelling may enable smart and fine-grained fire risk predication services. The main contribution of this study is implementation and experimental validation of a predictive fire risk indication model, which exploits cloud-provided measurements from weather stations and weather forecasts to predict the current and future fire risk for wooden homes at a given geographical location. The basic idea of the model is to estimate the indoor climate using measured and forecasted outdoor climate for computing indoor wooden fuel moisture content and an estimated time to flashover as indication of the fire risk. The model implementation was integrated into a micro-service based software system and experimentally validated during one winter at selected geographical locations, relying on weather data provided by the RESTful API of the Norwegian Meteorological Institute. Additionally, weather data from several historical fires were considered to relate our predictions to known fire incidents. Our evaluation demonstrates the ability to provide trustworthy and accurate fire risk indications using a combination of weather data measurements and forecast data. Furthermore, our cloud- and micro-service based software system implementation is efficient with respect to data storage and computation time. © 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)","Climate related risks; Cloud data services; Mitigating urban fire risk; Reducing vulnerability in cities; Smart city","Climate models; Computer software; Digital storage; Risk assessment; Risk perception; Smart city; Weather forecasting; 'current; 'Dry' [; Climate related risks; Cloud data service; Fire risks; Geographical locations; Indication models; Mitigating urban fire risk; Reducing vulnerability in city; Weather data; Fires","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85106738354"
"Zou D.; Zhu Y.; Xu S.; Li Z.; Jin H.; Ye H.","Zou, Deqing (8935128200); Zhu, Yawei (57219648674); Xu, Shouhuai (12241233400); Li, Zhen (57188851187); Jin, Hai (56434989100); Ye, Hengkai (57222494608)","8935128200; 57219648674; 12241233400; 57188851187; 56434989100; 57222494608","Interpreting Deep Learning-based Vulnerability Detector Predictions Based on Heuristic Searching","2021","ACM Transactions on Software Engineering and Methodology","30","2","23","","","","","10.1145/3429444","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102875804&doi=10.1145%2f3429444&partnerID=40&md5=cda88485b7efdce571aeacd20d4734a3","Detecting software vulnerabilities is an important problem and a recent development in tackling the problem is the use of deep learning models to detect software vulnerabilities. While effective, it is hard to explain why a deep learning model predicts a piece of code as vulnerable or not because of the black-box nature of deep learning models. Indeed, the interpretability of deep learning models is a daunting open problem. In this article, we make a significant step toward tackling the interpretability of deep learning model in vulnerability detection. Specifically, we introduce a high-fidelity explanation framework, which aims to identify a small number of tokens that make significant contributions to a detector's prediction with respect to an example. Systematic experiments show that the framework indeed has a higher fidelity than existing methods, especially when features are not independent of each other (which often occurs in the real world). In particular, the framework can produce some vulnerability rules that can be understood by domain experts for accepting a detector's outputs (i.e., true positives) or rejecting a detector's outputs (i.e., false-positives and false-negatives). We also discuss limitations of the present study, which indicate interesting open problems for future research. © 2021 ACM.","deep learning; Explainable AI; sensitivity analysis; vulnerability detection","Heuristic algorithms; Knowledge acquisition; Learning systems; Domain experts; False negatives; False positive; Interpretability; Learning models; Software vulnerabilities; Systematic experiment; Vulnerability detection; Deep learning","Article","Final","","Scopus","2-s2.0-85102875804"
"Chen X.; Yuan Z.; Cui Z.; Zhang D.; Ju X.","Chen, Xiang (57189091783); Yuan, Zhidan (57195352186); Cui, Zhanqi (35172605800); Zhang, Dun (57204128096); Ju, Xiaolin (55883850700)","57189091783; 57195352186; 35172605800; 57204128096; 55883850700","Empirical studies on the impact of filter-based ranking feature selection on security vulnerability prediction","2021","IET Software","15","1","","75","89","14","","10.1049/sfw2.12006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108911852&doi=10.1049%2fsfw2.12006&partnerID=40&md5=80735dce296dbd08db6c55964efa6e12","Security vulnerability prediction (SVP) can construct models to identify potentially vulnerable program modules via machine learning. Two kinds of features from different points of view are used to measure the extracted modules in previous studies. One kind considers traditional software metrics as features, and the other kind uses text mining to extract term vectors as features. Therefore, gathered SVP data sets often have numerous features and result in the curse of dimensionality. In this article, we mainly investigate the impact of filter-based ranking feature selection (FRFS) methods on SVP, since other types of feature selection methods have too much computational cost. In empirical studies, we first consider three real-world large-scale web applications. Then we consider seven methods from three FRFS categories for FRFS and use a random forest classifier to construct SVP models. Final results show that given the similar code inspection cost, using FRFS can improve the performance of SVP when compared with state-of-the-art baselines. Moreover, we use McNemar's test to perform diversity analysis on identified vulnerable modules by using different FRFS methods, and we are surprised to find that almost all the FRFS methods can identify similar vulnerable modules via diversity analysis. © 2020 The Authors. IET Software published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.","","Decision trees; Filtration; Text mining; Computational costs; Curse of dimensionality; Diversity analysis; Empirical studies; Feature selection methods; Random forest classifier; Security vulnerabilities; State of the art; Feature extraction","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85108911852"
"Liu M.; Li K.; Chen T.","Liu, Muyang (57203099581); Li, Ke (57145231600); Chen, Tao (55687687800)","57203099581; 57145231600; 55687687800","DeepSQLi: Deep semantic learning for testing SQL injection","2020","ISSTA 2020 - Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis","","","","286","297","11","","10.1145/3395363.3397375","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088916703&doi=10.1145%2f3395363.3397375&partnerID=40&md5=f1929e57a3f0596ef7ebc6c0cfda4f45","Security is unarguably the most serious concern for Web applications, to which SQL injection (SQLi) attack is one of the most devastating attacks. Automatically testing SQLi vulnerabilities is of ultimate importance, yet is unfortunately far from trivial to implement. This is because the existence of a huge, or potentially infinite, number of variants and semantic possibilities of SQL leading to SQLi attacks on various Web applications. In this paper, we propose a deep natural language processing based tool, dubbed DeepSQLi, to generate test cases for detecting SQLi vulnerabilities. Through adopting deep learning based neural language model and sequence of words prediction, DeepSQLi is equipped with the ability to learn the semantic knowledge embedded in SQLi attacks, allowing it to translate user inputs (or a test case) into a new test case, which is se- mantically related and potentially more sophisticated. Experiments are conducted to compare DeepSQLi with SQLmap, a state-of-the-art SQLi testing automation tool, on six real-world Web applications that are of different scales, characteristics and domains. Empirical results demonstrate the effectiveness and the remarkable superiority of DeepSQLi over SQLmap, such that more SQLi vulnerabilities can be identified by using a less number of test cases, whilst running much faster. © 2020 ACM.","deep learning; natural language processing; SQL injection; test case generation; Web security","Natural language processing systems; Software testing; Language model; NAtural language processing; Real world web; Semantic knowledge; Semantic learning; State of the art; Testing automation; WEB application; Deep learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85088916703"
"Halim Z.; Usman Ahmed Khan R.; Waqas M.; Tu S.","Halim, Zahid (24437954200); Usman Ahmed Khan, Raja (57223221005); Waqas, Muhammad (57215416666); Tu, Shanshan (36642846800)","24437954200; 57223221005; 57215416666; 36642846800","Utilizing 3D joints data extracted through depth camera to train classifiers for identifying suicide bomber","2021","Expert Systems with Applications","179","","115081","","","","","10.1016/j.eswa.2021.115081","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105277236&doi=10.1016%2fj.eswa.2021.115081&partnerID=40&md5=179db3dac02cfd97a3101d14bedb1f21","Safety and security of humans is an important concern in every aspect. With the advancement in engineering, sciences, and technology (unfortunately) new methods to harm humans have also been introduced. At the same time, scientists are paying attention to the security aspects by developing new software and hardware gadgets. In comparison to the system level security, the safety/security of human beings is more important. Suicide bombing is one such nuisance that is still an open challenge for the world to detect before it is triggered. This work deals with the identification of a suicide bomber using a 3D depth camera and machine learning techniques. This work utilizes the skeletal data provided by the 3D depth camera to identify a bomber wearing a suicide jacket. The prediction is based on real-time 3D posture data of the body joints obtained through the depth camera. Using a comprehensive experimental design, a dataset is created consisting of 20 joints information obtained from 120 participants. The dataset records this for each of the participants with and without wearing a suicide jacket. Experiments are performed with the suicide jacket bearing 10- to 20-kg weight. Simulations are performed using 3D spatial features of the participants' body in four ways: full body joints (20 joints), upper-half of the body (above the spine base of the skeleton), 20 joints with 15 frames, and 20 joints with 20 frames. It is observed that 15 to 20 frames are sufficient to identify a suspected suicide bomber. The proposed framework utilize four classifiers to identify vulnerability of a subject to be a suicide bomber. Results show that the proposed framework is capable of identifying a suicide bomber with an average accuracy of 92.30%. © 2021 Elsevier Ltd","Bomber identification; Data models; Predictive models; Task analysis","Bombers; Cameras; Classification (of information); Job analysis; Learning systems; Safety engineering; Bomber identification; Depth camera; Engineering science; Predictive models; Safety and securities; Science and Technology; Security aspects; Software and hardwares; Suicide bomber; Task analysis; Data structures","Article","Final","","Scopus","2-s2.0-85105277236"
"Jeon S.; Kim H.K.","Jeon, Sanghoon (57220838204); Kim, Huy Kang (7410133266)","57220838204; 7410133266","AutoVAS: An automated vulnerability analysis system with a deep learning approach","2021","Computers and Security","106","","102308","","","","","10.1016/j.cose.2021.102308","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106225546&doi=10.1016%2fj.cose.2021.102308&partnerID=40&md5=892d0b5b71d5c0e518a099e230f3518b","Owing to the advances in automated hacking and analysis technologies in recent years, numerous software security vulnerabilities have been announced. Software vulnerabilities are increasing rapidly, whereas methods to analyze and cope with them depend on manual analyses, which result in a slow response. In recent years, studies concerning the prediction of vulnerabilities or the detection of patterns of previous vulnerabilities have been conducted by applying deep learning algorithms in an automated vulnerability search based on source code. However, existing methods target only certain security vulnerabilities or make limited use of source code to compile information. Few studies have been conducted on methods that represent source code as an embedding vector. Thus, this study proposes a deep learning-based automated vulnerability analysis system (AutoVAS) that effectively represents source code as embedding vectors by using datasets from various projects in the National Vulnerability Database (NVD) and Software Assurance Reference Database (SARD). To evaluate AutoVAS, we present and share a dataset for deep learning models. Experimental results show that AutoVAS achieves a false negative rate (FNR) of 3.62%, a false positive rate (FPR) of 1.88%, and an F1-score of 96.11%, which represent lower FNR and FPR values than those achieved by other approaches. We further apply AutoVAS to nine open-source projects and detect eleven vulnerabilities, most of which are missed by the other approaches we experimented with. Notably, we discovered three zero-day vulnerabilities, two of which were patched after being informed by AutoVAS. The other vulnerability received the Common Vulnerabilities and Exposures (CVE) ID after being detected by AutoVAS. © 2021 Elsevier Ltd","Cybersecurity; Data-driven security; Program slicing; Static analysis; Vulnerability detection","Automation; Codes (symbols); Computer programming languages; Deep learning; Embeddings; Open source software; Pattern recognition; Personal computing; Security of data; Analysis system; Cyber security; Data-driven security; Embeddings; False negative rate; Program slicing; Security vulnerabilities; Source codes; Vulnerability analysis; Vulnerability detection; Static analysis","Article","Final","","Scopus","2-s2.0-85106225546"
"Liu B.; Meng G.; Zou W.; Gong Q.; Li F.; Lin M.; Sun D.; Huo W.; Zhang C.; Zhang C.","Liu, Bingchang (57204643589); Meng, Guozhu (56747189200); Zou, Wei (57209290415); Gong, Qi (57219626690); Li, Feng (55624942700); Lin, Min (57219625246); Sun, Dandan (57213520621); Huo, Wei (57202860764); Zhang, Chao (57221301333); Zhang, Chao (56101982700)","57204643589; 56747189200; 57209290415; 57219626690; 55624942700; 57219625246; 57213520621; 57202860764; 57221301333; 56101982700","A large-scale empirical study on vulnerability distribution within projects and the lessons learned","2020","Proceedings - International Conference on Software Engineering","","","3380923","1547","1559","12","","10.1145/3377811.3380923","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094316913&doi=10.1145%2f3377811.3380923&partnerID=40&md5=c56d2687cd1c83668eb88676165b4be9","The number of vulnerabilities increases rapidly in recent years, due to advances in vulnerability discovery solutions. It enables a thorough analysis on the vulnerability distribution and provides support for correlation analysis and prediction of vulnerabilities. Previous research either focuses on analyzing bugs rather than vulnerabilities, or only studies general vulnerability distribution among projects rather than the distribution within each project. In this paper, we collected a large vulnerability dataset, consisting of all known vulnerabilities associated with five representative open source projects, by utilizing automated crawlers and spending months of manual efforts. We then analyzed the vulnerability distribution within each project over four dimensions, including files, functions, vulnerability types and responsible developers. Based on the results analysis, we presented 12 practical insights on the distribution of vulnerabilities. Finally, we applied such insights on several vulnerability discovery solutions (including static analysis and dynamic fuzzing), and helped them find 10 zero-day vulnerabilities in target projects, showing that our insights are useful.  © 2020 Association for Computing Machinery.","Empirical study; Vulnerability distribution","Open source software; Static analysis; Correlation analysis; Empirical studies; Four dimensions; Open source projects; Vulnerability discovery; Zero day vulnerabilities; Large dataset","Conference paper","Final","","Scopus","2-s2.0-85094316913"
"Cui J.; Wang L.; Zhao X.; Zhang H.","Cui, Jianfeng (36671150300); Wang, Lixin (57215857663); Zhao, Xin (57215855941); Zhang, Hongyi (57203318760)","36671150300; 57215857663; 57215855941; 57203318760","Towards predictive analysis of android vulnerability using statistical codes and machine learning for IoT applications","2020","Computer Communications","155","","","125","131","6","","10.1016/j.comcom.2020.02.078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082130194&doi=10.1016%2fj.comcom.2020.02.078&partnerID=40&md5=5d6f51c0d1f6a340c1212546522d5b72","Recently, the Internet of Things (IoT) technology is used for several applications for exchanging information among various devices. The intelligent IoT based system utilizes an Android operating system because it is also primarily used in mobile devices. One of the main problems for different IoT applications is associated with android vulnerability is its complicated and large size. To overcome the main issue of IoT, the existing studies have proposed several effective prediction models using machine learning algorithms and software metrics. In this paper, we are focused on conducting android vulnerability prediction analysis using machine learning for intelligent IoT applications. We conducted an empirical investigation for examining security risk prediction of 1406 Android applications with varying levels of risk using a metric set of 21 static code metrics and 6 machine learning (ML) techniques. It is observed from results that ML algorithms have different performances for predicting security risks. RF algorithm performs better for Android applications of all risk levels. By analyzing the findings of the conducted empirical study, it is suggested that developers may consider object-oriented metrics and RF algorithm in the software development process for android based intelligent IoT systems. © 2020","Android vulnerability; IoT applications; Machine learning; Prediction; Software metrics","Android (operating system); Application programs; Forecasting; Internet of things; Learning algorithms; Learning systems; Machine learning; Mobile telecommunication systems; Object oriented programming; Risks; Software design; Android applications; Android vulnerability; Empirical investigation; Internet of thing (IOT); IOT applications; Object oriented metrics; Software development process; Software metrics; Mobile security","Article","Final","","Scopus","2-s2.0-85082130194"
"Medeiros N.; Ivaki N.; Costa P.; Vieira M.","Medeiros, Nadia (57196222319); Ivaki, Naghmeh (55556863000); Costa, Pedro (55934302200); Vieira, Marco (7202140748)","57196222319; 55556863000; 55934302200; 7202140748","An Empirical Study on Software Metrics and Machine Learning to Identify Untrustworthy Code","2021","Proceedings - 2021 17th European Dependable Computing Conference, EDCC 2021","","","","87","94","7","","10.1109/EDCC53658.2021.00020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123467578&doi=10.1109%2fEDCC53658.2021.00020&partnerID=40&md5=0ebd8aa8fb4378a0c2c4d213ebb85f06","The increasingly intensive use of software systems in diverse sectors, especially in business, government, healthcare, and critical infrastructures, makes it essential to deliver code that is secure. In this work, we present two sets of experiments aiming at helping developers to improve software security from the early development stages. The first experiment is focused on using software metrics to build prediction models to distinguish vulnerable from non-vulnerable code. The second experiment studies the hypothesis of developing a consensus-based decision-making approach on top of several machine learning-based prediction models, trained using software metrics data to categorize code units with respect to their security. Such categories suggest a priority (ranking) of software code units based on the potential existence of security vulnerabilities. Results show that software metrics do not constitute sufficient evidence of security issues and cannot effectively be used to build a prediction model to distinguish vulnerable from non-vulnerable code. However, with a consensus-based decision-making approach, it is possible to classify code units from a security perspective, which allows developers to decide (considering the criticality of the system under development and the available resources) which parts of the software should be the focal point for the detection and removal of security vulnerabilities. © 2021 IEEE.","Code Trustworthiness Assessment; Consensus-Based Decision-Making; Machine Learning; Software Metrics; Software Vulnerabilities","Codes (symbols); Forecasting; Machine learning; Network security; Code trustworthiness assessment; Consensus-based decision-making; Decisions makings; Empirical studies; Machine-learning; Metric learning; Prediction modelling; Security vulnerabilities; Software metrics; Software vulnerabilities; Decision making","Conference paper","Final","","Scopus","2-s2.0-85123467578"
"Alghayadh F.; Debnath D.","Alghayadh, Faisal (57211423991); Debnath, Debatosh (7003273877)","57211423991; 7003273877","Performance Evaluation of Machine Learning for Prediction of Network Traffic in a Smart Home","2020","2020 11th IEEE Annual Ubiquitous Computing, Electronics and Mobile Communication Conference, UEMCON 2020","","","9298134","0837","0842","5","","10.1109/UEMCON51285.2020.9298134","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099769884&doi=10.1109%2fUEMCON51285.2020.9298134&partnerID=40&md5=f0339d5ed5fe2a8f2ceaad302eebd291","The network system of smart homes using a Internet of Things (IoT) device is increasing in parallel with cybersecurity challenges as these loT devices have some vulnerabilities such as hardware and software limitations that leads to difficulties with time to fit security features to any IoT systems. Therefore, the Intrusion Detection Systems (IDS) is the suggested method to mitigate these cyberattacks and monitor the requests in smart homes. IDS has the capacity to protect the smart home network and detect real-time vulnerabilities and threats. In this paper, we applied and compared four types of machine learning algorithms which are random forest, xgboost, decision tree, and k-nearest neighbors on two sorts of datasets. We randomly selected three samples from each dataset. The results show that our models for each algorithm can effectively achieve a satisfying seemingly classification accuracy with the lowest false positive rate. © 2020 IEEE.","Anomaly detection; behavioral patterns; challenges; security; smart home systems; threats","Ambient intelligence; Automation; Decision trees; Intelligent buildings; Internet of things; Intrusion detection; Learning algorithms; Machine learning; Mobile telecommunication systems; Nearest neighbor search; Network security; Personal communication systems; Predictive analytics; Classification accuracy; False positive rates; Hardware and software; Internet of Things (IOT); Intrusion Detection Systems; K-nearest neighbors; Network systems; Security features; Home networks","Conference paper","Final","","Scopus","2-s2.0-85099769884"
"Zhou A.; Sultana K.Z.; Samanthula B.K.","Zhou, Andy (57468835500); Sultana, Kazi Zakia (23494078600); Samanthula, Bharath K. (42862340400)","57468835500; 23494078600; 42862340400","Investigating the Changes in Software Metrics after Vulnerability is Fixed","2021","Proceedings - 2021 IEEE International Conference on Big Data, Big Data 2021","","","","5658","5663","5","","10.1109/BigData52589.2021.9671334","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125307241&doi=10.1109%2fBigData52589.2021.9671334&partnerID=40&md5=956c23419bd51ec424a0fe7679ef14e4","Preventing software vulnerabilities while writing code is one of the most effective ways for avoiding cyber attacks on any developed system. Although developers follow some standard guiding principles for ensuring secure code, the code can still have security bottlenecks and be compromised by an attacker. Therefore, assessing software security while developing code can help developers in writing vulnerability free code. Researchers have already focused on metrics-based and text mining based software vulnerability prediction models. The metrics based models showed higher precision in predicting vulnerabilities although the recall rate is low. In addition, current research did not investigate the impact of individual software metric on the occurrences of vulnerabilities. The main objective of this paper is to track the changes in every software metric after the developer fixes a particular vulnerability. The results of our research will potentially motivate further research on building more accurate vulnerability prediction models based on the appropriate software metrics. In particular, we have compared a total of 250 files from Apache Tomcat and Apache CXF. These files were extracted from the Apache database and were chosen because Apache released these files as vulnerable in their publicly available security advisories. Using a static analysis tool, metrics of the targeted vulnerable files and relevant fixed files (files where vulnerable code is removed by the developers) were extracted and compared. We show that eight of the 40 metrics have an average increase of 2% from vulnerable to fixed files. These metrics include CountDeclClass, CountDeclClassMethod, CountDeclClassVariable, CountDeclInstanceVariable, CountDeclMethodDefault, CountLineCode, MaxCyclomaticStrict, MaxNesting. This study will help developers to assess software security through utilizing software metrics in secure coding practices. © 2021 IEEE.","software metric; software security; vulnerability","Cybersecurity; Forecasting; Network security; Guiding principles; High-precision; Prediction modelling; Secure codes; Software metrics; Software security; Software vulnerabilities; Text-mining; Vulnerability; Writing codes; Static analysis","Conference paper","Final","","Scopus","2-s2.0-85125307241"
"Siavvas M.; Kalouptsoglou I.; Tsoukalas D.; Kehagias D.","Siavvas, Miltiadis (57194500913); Kalouptsoglou, Ilias (57219327969); Tsoukalas, Dimitrios (57208865760); Kehagias, Dionysios (7003972544)","57194500913; 57219327969; 57208865760; 7003972544","A Self-adaptive Approach for Assessing the Criticality of Security-Related Static Analysis Alerts","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12955 LNCS","","","289","305","16","","10.1007/978-3-030-87007-2_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116048484&doi=10.1007%2f978-3-030-87007-2_21&partnerID=40&md5=b225afaeb5b88a5c4c97273fa2b2f240","Despite the acknowledged ability of automated static analysis to detect software vulnerabilities, its adoption in practice is limited, mainly due to the large number of false alerts (i.e., false positives) that it generates. Although several machine learning-based techniques for assessing the actionability of the produced alerts and for filtering out false positives have been proposed, none of them have demonstrated sufficient results, whereas limited attempts focus on assessing the criticality of the alerts from a security viewpoint. To this end, in the present paper we propose an approach for assessing the criticality of security-related static analysis alerts. In particular, we develop a machine learning-based technique for prioritizing and classifying security-related static analysis alerts based on their criticality, by considering information retrieved from the alerts themselves, vulnerability prediction models, and user feedback. The concept of retraining is also adopted to enable the model to correct itself and adapt to previously unknown software products. The technique has been evaluated through a case study, which revealed its capacity to effectively assess the criticality of alerts of previously unknown projects, as well as its ability to dynamically adapt to the characteristics of the new project and provide more accurate assessments through retraining. © 2021, Springer Nature Switzerland AG.","Automated static analysis; Self-adaptive systems; Software quality; Software security; Vulnerability prediction","Classification (of information); Computer software selection and evaluation; Criticality (nuclear fission); Machine learning; Quality control; Actionability; Automated static analyse; False positive; Model feedback; Prediction modelling; Self-adaptive approaches; Self-adaptive system; Software security; Software vulnerabilities; Vulnerability prediction; Static analysis","Conference paper","Final","","Scopus","2-s2.0-85116048484"
"Zhu L.; Cheng S.; Wang Q.","Zhu, Ling (55710404000); Cheng, Shuo (57222731123); Wang, Qi (57221577185)","55710404000; 57222731123; 57221577185","A Method For Predicting Reactive Voltage Of Photovoltaic Transmission System Based On Double-RF","2021","5th IEEE Conference on Energy Internet and Energy System Integration: Energy Internet for Carbon Neutrality, EI2 2021","","","","3015","3019","4","","10.1109/EI252483.2021.9713551","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128240661&doi=10.1109%2fEI252483.2021.9713551&partnerID=40&md5=91a0a053c9c46f9d660480237f12a479","Under the target of ""30 60"", the power system is characterized by a high proportion of new energy on the source side, and the uncertainty of new energy generation leads to a strong vulnerability of the power system. This causes a short circuit fault to occur during the transmission of electric energy in a large-scale HVDC transmission system, which causes the voltage of the AC bus at the end to decrease first and then increase. If the reactive power compensation support is not returned in time in the process of low voltage through, it will cause the risk of overvoltage and damage to every part of the power system. By predicting the severity of overvoltage in advance, the reactive power compensation device can be adjusted in time to avoid accidents. In this paper, the mechanism of AC bus overvoltage is analyzed, and the fault of power system is simulated by using PSCAD software. Then, the double-RF algorithm is built to predict the overvoltage level, and the results show that the identification accuracy of this method can reach 98.82%, which is conducive to the timely and rapid decision of reactive power compensation and the reduction of overvoltage level. © 2021 IEEE","DC commutation failure; double-RF; Overvoltage prediction; Photovoltaic power generation","Electric power transmission; Electric power utilization; Electrolysis; Reactive power; Solar cells; Solar energy; Transmissions; Commutation failure; DC commutation failure; Double-RF; New energies; Over-voltages; Overvoltage prediction; Photovoltaic power generation; Photovoltaics; Power; Reactive voltages; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85128240661"
"Yoon S.; Cho J.-H.; Kim D.S.; Moore T.J.; Free-Nelson F.; Lim H.","Yoon, Seunghyun (57212950009); Cho, Jin-Hee (55474140000); Kim, Dong Seong (55667184100); Moore, Terrence J. (57199292529); Free-Nelson, Frederica (57210286312); Lim, Hyuk (55654245700)","57212950009; 55474140000; 55667184100; 57199292529; 57210286312; 55654245700","Attack Graph-Based Moving Target Defense in Software-Defined Networks","2020","IEEE Transactions on Network and Service Management","17","3","9063635","1653","1668","15","","10.1109/TNSM.2020.2987085","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083422166&doi=10.1109%2fTNSM.2020.2987085&partnerID=40&md5=448e3242a92633c7efe5b716bbd51ce6","Moving target defense (MTD) has emerged as a proactive defense mechanism aiming to thwart a potential attacker. The key underlying idea of MTD is to increase uncertainty and confusion for attackers by changing the attack surface (i.e., system or network configurations) that can invalidate the intelligence collected by the attackers and interrupt attack execution; ultimately leading to attack failure. Recently, the significant advance of software-defined networking (SDN) technology has enabled several complex system operations to be highly flexible and robust; particularly in terms of programmability and controllability with the help of SDN controllers. Accordingly, many security operations have utilized this capability to be optimally deployed in a complex network using the SDN functionalities. In this paper, by leveraging the advanced SDN technology, we developed an attack graph-based MTD technique that shuffles a host's network configurations (e.g., MAC/IP/port addresses) based on its criticality, which is highly exploitable by attackers when the host is on the attack path(s). To this end, we developed a hierarchical attack graph model that provides a network's vulnerability and network topology, which can be utilized for the MTD shuffling decisions in selecting highly exploitable hosts in a given network, and determining the frequency of shuffling the hosts' network configurations. The MTD shuffling with a high priority on more exploitable, critical hosts contributes to providing adaptive, proactive, and affordable defense services aiming to minimize attack success probability with minimum MTD cost. We validated the out performance of the proposed MTD in attack success probability and MTD cost via both simulation and real SDN testbed experiments.  © 2004-2012 IEEE.","asset criticality; attack path prediction; hierarchical attack graph; Moving target defense; network address shuffling; proactive/adaptive defense; software-defined networking","Complex networks; Graph theory; Graphic methods; Software defined networking; Complex system operation; Defense services; Moving target defense; Network configuration; Network topology; Proactive defense; Security operations; Software defined networking (SDN); Network security","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85083422166"
"Chong T.-Y.; Anu V.; Sultana K.Z.","Chong, Tai-Yin (57215345965); Anu, Vaibhav (57191251672); Sultana, Kazi Zakia (23494078600)","57215345965; 57191251672; 23494078600","Using software metrics for predicting vulnerable code-components: A study on java and python open source projects","2019","Proceedings - 22nd IEEE International Conference on Computational Science and Engineering and 17th IEEE International Conference on Embedded and Ubiquitous Computing, CSE/EUC 2019","","","8919513","98","103","5","","10.1109/CSE/EUC.2019.00028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077044675&doi=10.1109%2fCSE%2fEUC.2019.00028&partnerID=40&md5=4332d78e39d88b162c2e2b6e87505ddf","Software vulnerabilities often remain hidden until an attacker exploits the weak/insecure code. Therefore, testing the software from a vulnerability discovery perspective becomes challenging for developers if they do not inspect their code thoroughly (which is time-consuming). We propose that vulnerability prediction using certain software metrics can support the testing process by identifying vulnerable code-components (e.g., functions, classes, etc.). Once a code-component is predicted as vulnerable, the developers can focus their testing efforts on it, thereby avoiding the time/effort required for testing the entire application. The current paper presents a study that compares how software metrics perform as vulnerability predictors for software projects developed in two different languages (Java vs Python). The goal of this research is to analyze the vulnerability prediction performance of software metrics for different programming languages. We designed and conducted experiments on security vulnerabilities reported for three Java projects (Apache Tomcat 6, Tomcat 7, Apache CXF) and two Python projects (Django and Keystone). In this paper, we focus on a specific type of code component: Functions. We apply Machine Learning models for predicting vulnerable functions. Overall results show that software metrics-based vulnerability prediction is more useful for Java projects than Python projects (i.e., software metrics when used as features were able to predict Java vulnerable functions with a higher recall and precision compared to Python vulnerable functions prediction). © 2019 IEEE.","Machine learning; Software metrics; Software reliability; Software security; Vulnerability prediction","Forecasting; High level languages; Learning systems; Machine learning; Object oriented programming; Open source software; Open systems; Software reliability; Software testing; Ubiquitous computing; Machine learning models; Open source projects; Prediction performance; Security vulnerabilities; Software metrics; Software security; Software vulnerabilities; Vulnerability discovery; Java programming language","Conference paper","Final","","Scopus","2-s2.0-85077044675"
"Kudjo P.K.; Aformaley S.B.; Mensah S.; Chen J.","Kudjo, Patrick Kwaku (57195678643); Aformaley, Selasie Brown (57211425046); Mensah, Solomon (57191254462); Chen, Jinfu (56506180800)","57195678643; 57211425046; 57191254462; 56506180800","The Significant Effect of Parameter Tuning on Software Vulnerability Prediction Models","2019","Proceedings - Companion of the 19th IEEE International Conference on Software Quality, Reliability and Security, QRS-C 2019","","","8859413","526","527","1","","10.1109/QRS-C.2019.00107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073873565&doi=10.1109%2fQRS-C.2019.00107&partnerID=40&md5=8d29a1ec1e135d527739bbeb5bb13aad","Vulnerability prediction is one of the critical issues for researchers in the software industry. Technically, a vulnerability predictor is a machine learning model trained to identify vulnerable and non-vulnerable modules. Recent studies have shown that the performance of these models can be affected when the default parameter settings are used. Unfortunately, most studies in literature present their results using the default parameter settings. This study investigates the extent to which parameter optimization affect the performance of vulnerability prediction models. To evaluate our procedure, we conducted an empirical study on three open-source vulnerability datasets, namely Drupal, Moodle and PHPMyAdmin using five machine learning algorithms. Surprisingly, we found that in all cases of the 3 datasets studied, our models provided a significant increase in precision and accuracy against the benchmark study. In conclusion, software engineers can use the results obtained from this study when building data miners for identifying vulnerable modules. © 2019 IEEE.","machine learning algorithms; parameter optimization; Software vulnerability","C (programming language); Computer software selection and evaluation; Forecasting; Learning systems; Machine learning; Open source software; Software reliability; Default parameters; Effect of parameters; Empirical studies; Machine learning models; Parameter optimization; Prediction model; Software industry; Software vulnerabilities; Learning algorithms","Conference paper","Final","","Scopus","2-s2.0-85073873565"
"Krishnan P.; Duttagupta S.; Achuthan K.","Krishnan, Prabhakar (57194605863); Duttagupta, Subhasri (16549830000); Achuthan, Krishnashree (35483077800)","57194605863; 16549830000; 35483077800","VARMAN: Multi-plane security framework for software defined networks","2019","Computer Communications","148","","","215","239","24","","10.1016/j.comcom.2019.09.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072873993&doi=10.1016%2fj.comcom.2019.09.014&partnerID=40&md5=4d5ed863a69f000009d1108c6f90439b","In the context of future networking technologies, Software-Defined paradigm offers compelling solutions and advantages for traffic orchestration and shaping, flexible and dynamic routing, programmable control and smart application-driven resource management. But the SDN operation has to confront critical issues and technical vulnerabilities, security problems and threats in the enabling technical architecture itself. To address the critical security problems in SDN enabled data centers, we propose a collaborative “Network Security and Intrusion Detection System(NIDS)” scheme called ‘VARMAN: adVanced multi-plAne secuRity fraMework for softwAre defined Networks’. The SDN security scheme comprises of coarse-grained flow monitoring algorithms on the dataplane for rapid anomaly detection and prediction of network-centric DDoS/botnet attacks. In addition, this is combined with a fine-grained hybrid deep-learning based classifier pipeline on the control plane. It is observed that existing ML-based classifiers improve the accuracy of NIDS, however, at the cost of higher processing power and memory requirement, thus unrealistic for real-time solutions. To address these problems and still achieve accuracy and speed, we designed a hybrid model, combining both deep and shallow learning techniques, that are implemented in an improved SDN stack. The data plane deploys attack prediction and behavioral trigger mechanisms, efficient data filtering, feature selection, and data reduction techniques. To demonstrate the practical feasibility of our security scheme in real modern datacenters, we utilized the popular NSL-KDD dataset, most recent CICIDS2017 dataset, and refined it to a balanced dataset containing a comparable number of normal traffic and malware samples. We further augmented the training by organically generating datasets from lab-simulated and public-network hosted hackathon websites. The results show that VARMAN framework is capable of detecting attacks in real-time with accuracy more than 98% under attack intensities up to 50k packets/second. In a multi-controller interconnected SDN domain, the flow setup time improves by 70% on an average, and controller response time reduces by 40%, without incurring additional latency due to security intelligence processing overhead in SDN stack. The comparisons of VARMAN under similar attack scenarios and test environment, with related recent works that utilized ML-based NIDS, demonstrate that our scheme offers higher accuracy, less than 5% false positive rate for various attack intensities and significant training space/time reduction. © 2019 Elsevier B.V.","Botnet; CICIDS2017; Cloud; DDoS; Deep learning; Edge networks; IDS; IoT; IPS; Machine learning; Malware; Network security; NFV; NIDS; SDN; SDNFV; Security; Threat analytics","Anomaly detection; Application programs; Botnet; Clouds; Controllers; Data reduction; Deep learning; Intrusion detection; Learning systems; Malware; Software defined networking; Uninterruptible power systems; CICIDS2017; DDoS; EDGE Networks; NIDS; SDNFV; Security; Threat analytics; Network security","Article","Final","","Scopus","2-s2.0-85072873993"
"Movahedi Y.; Cukier M.; Andongabo A.; Gashi I.","Movahedi, Yazdan (56582473200); Cukier, Michel (6603803353); Andongabo, Ambrose (55247416200); Gashi, Ilir (6505943089)","56582473200; 6603803353; 55247416200; 6505943089","Cluster-Based Vulnerability Assessment Applied to Operating Systems","2017","Proceedings - 2017 13th European Dependable Computing Conference, EDCC 2017","","","8123548","18","25","7","","10.1109/EDCC.2017.27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041169071&doi=10.1109%2fEDCC.2017.27&partnerID=40&md5=6af03c63a066a2975418bf6fc5a57f1a","Organizations face the issue of how to best allocate their security resources. Thus, they need an accurate method for assessing how many new vulnerabilities will be reported for the operating systems (OSs) they use in a given time period. Our approach consists of clustering vulnerabilities by leveraging the text information within vulnerability records, and then simulating the mean value function of vulnerabilities by relaxing the monotonic intensity function assumption, which is prevalent among the studies that use software reliability models (SRMs) and nonhomogeneous Poisson process (NHPP) in modeling. We applied our approach to the vulnerabilities of four OSs: Windows, Mac, IOS, and Linux. For the OSs analyzed in terms of curve fitting and prediction capability, our results, compared to a power-law model without clustering issued from a family of SRMs, are more accurate in all cases we analyzed. © 2017 IEEE.","Clustering; Nonhomogeneous Poisson process; Software reliability models; Vulnerability assessment","Computer operating systems; Curve fitting; Poisson distribution; Clustering; Intensity functions; Mean value function; Non-homogeneous Poisson process; Prediction capability; Software reliability models; Text information; Vulnerability assessments; Software reliability","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85041169071"
"Sultana K.Z.; Williams B.J.; Bosu A.","Sultana, Kazi Zakia (23494078600); Williams, Byron J. (24478168500); Bosu, Amiangshu (55258553900)","23494078600; 24478168500; 55258553900","A Comparison of Nano-Patterns vs. Software Metrics in Vulnerability Prediction","2018","Proceedings - Asia-Pacific Software Engineering Conference, APSEC","2018-December","","8719530","355","364","9","","10.1109/APSEC.2018.00050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066783110&doi=10.1109%2fAPSEC.2018.00050&partnerID=40&md5=2569f38613751eb53543fe9a7c11721d","Context: Software security is an imperative aspect of software quality. Early detection of vulnerable code during development can better ensure the security of the codebase and minimize testing efforts. Although traditional software metrics are used for early detection of vulnerabilities, they do not clearly address the granularity level of the issue to precisely pinpoint vulnerabilities. The goal of this study is to employ method-level traceable patterns (nano-patterns) in vulnerability prediction and empirically compare their performance with traditional software metrics. The concept of nano-patterns is similar to design patterns, but these constructs can be automatically recognized and extracted from source code. If nano-patterns can better predict vulnerable methods compared to software metrics, they can be used in developing vulnerability prediction models with better accuracy. Aims: This study explores the performance of method-level patterns in vulnerability prediction. We also compare them with method-level software metrics. Method: We studied vulnerabilities reported for two major releases of Apache Tomcat (6 and 7), Apache CXF, and two stand-alone Java web applications. We used three machine learning techniques to predict vulnerabilities using nano-patterns as features. We applied the same techniques using method-level software metrics as features and compared their performance with nano-patterns. Results: We found that nano-patterns show lower false negative rates for classifying vulnerable methods (for Tomcat 6, 21% vs 34.7%) and therefore, have higher recall in predicting vulnerable code than the software metrics used. On the other hand, software metrics show higher precision than nano-patterns (79.4% vs 76.6%). Conclusion: In summary, we suggest developers use nano-patterns as features for vulnerability prediction to augment existing approaches as these code constructs outperform standard metrics in terms of prediction recall. © 2018 IEEE.","nano-pattern; software metrics; software security; Vulnerability","Codes (symbols); Computer software selection and evaluation; Learning systems; False negative rate; Granularity levels; Machine learning techniques; Nano pattern; Software metrics; Software Quality; Software security; Vulnerability; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85066783110"
"Alenezi M.; Almomani I.","Alenezi, Mamdouh (55854089000); Almomani, Iman (11439592000)","55854089000; 11439592000","Empirical analysis of static code metrics for predicting risk scores in android applications","2018","Advances in Intelligent Systems and Computing","753","","","84","94","10","","10.1007/978-3-319-78753-4_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045278314&doi=10.1007%2f978-3-319-78753-4_8&partnerID=40&md5=f06d2aac15e8454ca147ad2651e12734","Recently, with the purpose of helping developers reduce the needed effort to build highly secure software, researchers have proposed a number of vulnerable source code prediction models that are built on different kinds of features. Identifying security vulnerabilities along with differentiating non-vulnerable from a vulnerable code is not an easy task. Commonly, security vulnerabilities remain dormant until they are exploited. Software metrics have been widely used to predict and indicate several quality characteristics about software, but the question at hand is whether they can recognize vulnerable code from non-vulnerable ones. In this work, we conduct a study on static code metrics, their interdependency, and their relationship with security vulnerabilities in Android applications. The aim of the study is to understand: (i) the correlation between static software metrics; (ii) the ability of these metrics to predict security vulnerabilities, and (iii) which are the most informative and discriminative metrics that allow identifying vulnerable units of code. © Springer International Publishing AG, part of Springer Nature 2018.","Android; Risk scores; Security prediction models; Static code metrics","Android (operating system); Codes (symbols); Data mining; Forecasting; Risk assessment; Security of data; Android; Android applications; Empirical analysis; Prediction model; Quality characteristic; Risk score; Security vulnerabilities; Static code metrics; Mobile security","Conference paper","Final","","Scopus","2-s2.0-85045278314"
"Kudjo P.K.; Chen J.; Brown S.A.; Mensah S.","Kudjo, Patrick Kwaku (57195678643); Chen, Jinfu (56485257600); Brown, Selasie Aformaley (57215127506); Mensah, Solomon (57191254462)","57195678643; 56485257600; 57215127506; 57191254462","The effect of weighted moving windows on security vulnerability prediction","2019","Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering Workshops, ASEW 2019","","","8967408","65","68","3","","10.1109/ASEW.2019.00031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079271451&doi=10.1109%2fASEW.2019.00031&partnerID=40&md5=1ac349686599038d14fe75949f838322","Vulnerability prediction models strive to identify vulnerable modules in large software systems. Consequently, several vulnerability prediction approaches have been proposed to identify such susceptible units by using software metrics, historical data, and machine learning techniques. However, in spite of the key role seasonal trends of vulnerabilities play in estimating the resources needed for developing corrective measures, most of the proffered models fail to examine the trend, level, and seasonality of security vulnerability. To address this lacuna, this paper examines the statistical significance of the annual seasonal patterns and trends in vulnerability discovery using the weighted moving window. Our approach takes into account the chronological order within vulnerability data and assigns different weights of importance to projects in a window to effectively portray current security practices. Specifically, we used three regression-based models as vulnerability predictors for historical vulnerability data mined from five open-source applications offered by the Common Vulnerability Exposures and the National Vulnerability Database (CVE-NVD). In addition, we evaluate the performance and reliability of the models with symmetric mean absolute percent error (SMAPE). The preliminary results suggest that weighting the moving window based on Gaussian function yields improved accuracy and the recommended forecasting model is the robust regression. © 2019 IEEE.","Forecasting; Seasonality; Software vulnerabilities; Weighted moving window","Forecasting; Open source software; Technical presentations; Machine learning techniques; Moving window; National vulnerability database; Performance and reliabilities; Seasonality; Security vulnerabilities; Software vulnerabilities; Statistical significance; Learning systems","Conference paper","Final","","Scopus","2-s2.0-85079271451"
"Alohaly M.; Takabi H.","Alohaly, Manar (57193347849); Takabi, Hassan (23006829300)","57193347849; 23006829300","When Do Changes Induce Software Vulnerabilities?","2017","Proceedings - 2017 IEEE 3rd International Conference on Collaboration and Internet Computing, CIC 2017","2017-January","","","59","66","7","","10.1109/CIC.2017.00020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046635942&doi=10.1109%2fCIC.2017.00020&partnerID=40&md5=49b3e5ad2a32ac3b5d365958347b9aea","Version control systems (VCSs) have almost become the de facto standard for the management of open-source projects and the development of their source code. In VCSs, source code which can potentially be vulnerable is introduced to a system through what are so called commits. Vulnerable commits force the system into an insecure state. The farreaching impact of vulnerabilities attests to the importance of identifying and understanding the characteristics of prior vulnerable changes (or commits), in order to detect future similar ones. The concept of change classification was previously studied in the literature of bug detection to identify commits with defects. In this paper, we borrow the notion of change classification from the literature of defect detection to further investigate its applicability to vulnerability detection problem using semi-supervised learning. In addition, we also experiment with new vulnerability predictors, and compare the predictive power of our proposed features with vulnerability prediction techniques based on text mining. The experimental results show that our semi-supervised approach holds promise in improving change classification effectiveness by leveraging unlabeled data. © 2017 IEEE.","Semi supervised Learning; Software Security; Software Vulnerabilities; Source Code; Vulnerability Prediction","Codes (symbols); Computer programming languages; Data mining; Defects; Open source software; Program debugging; Supervised learning; Open source projects; Prediction techniques; Semi- supervised learning; Software security; Software vulnerabilities; Source codes; Version control system; Vulnerability detection; Open systems","Conference paper","Final","","Scopus","2-s2.0-85046635942"
"Agea-Medina N.; Molina-Palacios S.; Lang D.H.; Ferreiro-Prieto I.; Huesca J.A.; Galiana-Merino J.J.; Soler-Llorens J.L.","Agea-Medina, N. (57196956212); Molina-Palacios, S. (55649570355); Lang, D.H. (24471816900); Ferreiro-Prieto, I. (55599972500); Huesca, J.A. (57190336085); Galiana-Merino, J.J. (7801475177); Soler-Llorens, J.L. (57189582081)","57196956212; 55649570355; 24471816900; 55599972500; 57190336085; 7801475177; 57189582081","Sensitivity of structural damage to earthquake ground motion scenarios. the Torrevieja earthquake case study","2018","International Journal of Computational Methods and Experimental Measurements","6","5","","921","932","11","","10.2495/CMEM-V6-N5-921-932","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055989700&doi=10.2495%2fCMEM-V6-N5-921-932&partnerID=40&md5=5936209bb53cb00e63734ef611090174","Structural damage computation using analytical methods requires the knowledge of the ground motion distribution in the urban area caused by a given earthquake. In this manuscript, the ground motion estimates (i.e. PGA and spectral acceleration values) are obtained through simulation of the 1829 Torrevieja earthquake using the NGA ground motion prediction equations (GMPE). The building stock under consideration has been classified according to the methodology presented in RISK-UE. The computations have been done using the last version of the software SELENA. The epistemic uncertainties of the analysis are accounted for by means of a logic tree computation scheme. The logic tree has two branches for the uncertainty in the earthquake scenario, two branches for the GMPEs and three branches to consider the uncertainties in average shear wave velocity Vs30 (soil conditions). Results indicate large differences derived for the different earthquake loss scenarios (ELE) obtained following each branch of the logic tree. The greatest structural damages and losses are obtained when the earthquake is located in the Bajo Segura fault zone, using Campbell and Bozorgnia GMPE and for soft soil conditions. This article has allowed us to see how the different possible input parameters for ELE should be carefully analyzed for each case study and the importance of providing ELE results in terms of mean values with corresponding uncertainty ranges. © 2018 WIT Press, www.witpress.com.","Analytical method; Earthquake damage; Epistemic uncertainty; Seismic risk; Seismic vulnerability","","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85055989700"
"Ciapessoni E.; Cirio D.; Pitto A.; Sforna M.","Ciapessoni, E. (6505804291); Cirio, D. (6504625684); Pitto, A. (35068505700); Sforna, M. (55903400800)","6505804291; 6504625684; 35068505700; 55903400800","A probabilistic risk-based security assessment tool allowing contingency forecasting","2018","2018 International Conference on Probabilistic Methods Applied to Power Systems, PMAPS 2018 - Proceedings","","","8440526","","","","","10.1109/PMAPS.2018.8440526","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053160678&doi=10.1109%2fPMAPS.2018.8440526&partnerID=40&md5=83c86e7c1fb13f878cac8d64312af6b8","Extreme weather events are getting more and more frequent due to climate changes and often determine large load disruptions on power systems; this calls for the analysis of the vulnerabilities to natural threats which may cause multiple, dependent contingencies. In this context, exploiting the data coming from forecasting systems in a risk-based security assessment environment can help anticipate the most risky contingencies provoked by the weather event itself. The paper proposes an in-depth risk-based security assessment methodology, based on an extended definition of risk and aimed to predict the most risky contingencies which will affect the power system (contingency forecasting) on the basis of the k-hour ahead forecasts of the weather events. Big data analytics can be useful to get an accurate model for weather-related threats. The relevant software platform integrates the security assessment methodology with weather prediction systems. The application to a realistic wet snow threat scenario in the Italian transmission grid shows the added value of the proposed approach with respect to conventional security analyses, by defining a set of single and multiple contingencies evolving with the weather disturbance, thus complementing the conventional N-l security criterion. © 2018 IEEE.","Contingency; Data analytics; Power systems; Risk; Security; Threats; Vulnerability","Big data; Climate change; Risk assessment; Risks; Standby power systems; Weather information services; Contingency; Data analytics; Security; Threats; Vulnerability; Weather forecasting","Conference paper","Final","","Scopus","2-s2.0-85053160678"
"Siavvas M.; Gelenbe E.; Kehagias D.; Tzovaras D.","Siavvas, Miltiadis (57194500913); Gelenbe, Erol (7006026729); Kehagias, Dionysios (7003972544); Tzovaras, Dimitrios (13105681700)","57194500913; 7006026729; 7003972544; 13105681700","Static analysis-based approaches for secure software development","2018","Communications in Computer and Information Science","821","","","142","157","15","","10.1007/978-3-319-95189-8_13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050267517&doi=10.1007%2f978-3-319-95189-8_13&partnerID=40&md5=d766be56e7dcbda57526fc31529db228","Software security is a matter of major concern for software development enterprises that wish to deliver highly secure software products to their customers. Static analysis is considered one of the most effective mechanisms for adding security to software products. The multitude of static analysis tools that are available provide a large number of raw results that may contain security-relevant information, which may be useful for the production of secure software. Several mechanisms that can facilitate the production of both secure and reliable software applications have been proposed over the years. In this paper, two such mechanisms, particularly the vulnerability prediction models (VPMs) and the optimum checkpoint recommendation (OCR) mechanisms, are theoretically examined, while their potential improvement by using static analysis is also investigated. In particular, we review the most significant contributions regarding these mechanisms, identify their most important open issues, and propose directions for future research, emphasizing on the potential adoption of static analysis for addressing the identified open issues. Hence, this paper can act as a reference for researchers that wish to contribute in these subfields, in order to gain solid understanding of the existing solutions and their open issues that require further research. © 2018, The Author(s).","Checkpoint and Restart; Reliability; Software security; Static analysis; Vulnerability prediction","Application programs; Reliability; Reliability analysis; Software design; Software reliability; Analysis-based approaches; Checkpoint-and-restart; Effective mechanisms; Potential adoption; Secure software development; Software applications; Software products; Software security; Static analysis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85050267517"
"Ciapessoni E.; Cirio D.; Pitto A.; Marcacci P.; Lacavalla M.; Massucco S.; Silvestro F.; Sforna M.","Ciapessoni, Emanuele (6505804291); Cirio, Diego (6504625684); Pitto, Andrea (35068505700); Marcacci, Pietro (38061748900); Lacavalla, Matteo (52063487400); Massucco, Stefano (6603357712); Silvestro, Federico (23026343700); Sforna, Marino (55903400800)","6505804291; 6504625684; 35068505700; 38061748900; 52063487400; 6603357712; 23026343700; 55903400800","A risk-based methodology and tool combining threat analysis and power system security assessment","2018","Energies","11","1","83","","","","","10.3390/en11010083","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040460076&doi=10.3390%2fen11010083&partnerID=40&md5=6cdfce32c072fd12b7fdf8e70e342bf5","A thorough investigation of power system security requires the analysis of the vulnerabilities to natural and man-related threats which potentially trigger multiple contingencies. In particular, extreme weather events are becoming more and more frequent due to climate changes and often cause large load disruptions on the system, thus the support for security enhancement gets tricky. Exploiting data coming from forecasting systems in a security assessment environment can help assess the risk of operating power systems subject to the disturbances provoked by the weather event itself. In this context, the paper proposes a security assessment methodology, based on an updated definition of risk suitable for power system risk evaluations. Big data analytics can be useful to get an accurate model for weather-related threats. The relevant software (SW) platform integrates the security assessment methodology with prediction systems which provide short term forecasts of the threats affecting the system. The application results on a real wet snow threat scenario in the Italian High Voltage grid demonstrate the effectiveness of the proposed approach with respect to conventional security approaches, by complementing the conventional ""N - 1"" security criterion and exploiting big data to link the security assessment phase to the analysis of incumbent threats.","Blackouts; Contingency; Extreme events; Power systems; Probability; Risk; Security; Threats; Vulnerability","Big data; Climate change; Electric power system security; Electric power systems; Forecasting; Probability; Risks; Standby power systems; Weather information services; Blackouts; Contingency; Extreme events; Security; Threats; Vulnerability; Risk assessment","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85040460076"
"Ruohonen J.","Ruohonen, Jukka (56743033000)","56743033000","An empirical analysis of vulnerabilities in python packages for web applications","2018","Proceedings - 2018 9th International Workshop on Empirical Software Engineering in Practice, IWESEP 2018","","","8661215","25","30","5","","10.1109/IWESEP.2018.00013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063961075&doi=10.1109%2fIWESEP.2018.00013&partnerID=40&md5=be38cfe936d9fe6657d4b3b4ba575fa0","This paper examines software vulnerabilities in common Python packages used particularly for web development. The empirical dataset is based on the PyPI package repository and the so-called Safety DB used to track vulnerabilities in selected packages within the repository. The methodological approach builds on a release-based time series analysis of the conditional probabilities for the releases of the packages to be vulnerable. According to the results, many of the Python vulnerabilities observed seem to be only modestly severe; input validation and cross-site scripting have been the most typical vulnerabilities. In terms of the time series analysis based on the release histories, only the recent past is observed to be relevant for statistical predictions; the classical Markov property holds. © 2018 IEEE.","Software-vulnerability,-software-evolution,-software-release,-time-series,-autologistic,-Safety-DB,-PyPI,-pip","Application programs; Harmonic analysis; High level languages; Time series analysis; Empirical analysis; Software Evolution; Software release time; Software vulnerabilities; Software-vulnerability,-software-evolution,-software-release,-time-series,-autologistic,-safety-DB,-PyPI,-pip; Time-series analysis; Times series; WEB application; Web applications; Web development; Python","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85063961075"
"Ferenc R.; Hegedus P.; Gyimesi P.; Antal G.; Ban D.; Gyimothy T.","Ferenc, Rudolf (6603559878); Hegedus, Peter (25926433300); Gyimesi, Peter (56989871000); Antal, Gabor (6701427737); Ban, Denes (55496456800); Gyimothy, Tibor (6603266536)","6603559878; 25926433300; 56989871000; 6701427737; 55496456800; 6603266536","Challenging machine learning algorithms in predicting vulnerable javascript functions","2019","Proceedings - 2019 IEEE/ACM 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, RAISE 2019","","","8823747","8","14","6","","10.1109/RAISE.2019.00010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072922335&doi=10.1109%2fRAISE.2019.00010&partnerID=40&md5=344ec1bc077603e38956a2b786e35c67","The rapid rise of cyber-crime activities and the growing number of devices threatened by them place software security issues in the spotlight. As around 90% of all attacks exploit known types of security issues, finding vulnerable components and applying existing mitigation techniques is a viable practical approach for fighting against cyber-crime. In this paper, we investigate how the state-of-the-art machine learning techniques, including a popular deep learning algorithm, perform in predicting functions with possible security vulnerabilities in JavaScript programs. We applied 8 machine learning algorithms to build prediction models using a new dataset constructed for this research from the vulnerability information in public databases of the Node Security Project and the Snyk platform, and code fixing patches from GitHub. We used static source code metrics as predictors and an extensive grid-search algorithm to find the best performing models. We also examined the effect of various re-sampling strategies to handle the imbalanced nature of the dataset. The best performing algorithm was KNN, which created a model for the prediction of vulnerable functions with an F-measure of 0.76 (0.91 precision and 0.66 recall). Moreover, deep learning, tree and forest based classifiers, and SVM were competitive with F-measures over 0.70. Although the F-measures did not vary significantly with the re-sampling strategies, the distribution of precision and recall did change. No re-sampling seemed to produce models preferring high precision, while re-sampling strategies balanced the IR measures. © 2019 IEEE.","Code metrics; Dataset; Deep learnin; JavaScript; Machie learning; Vulnerability","Codes (symbols); Crime; Deep learning; Fixed platforms; Forecasting; High level languages; Machine learning; Software engineering; Support vector machines; Code metrics; Dataset; Deep learnin; Javascript; Machie learning; Vulnerability; Learning algorithms","Conference paper","Final","","Scopus","2-s2.0-85072922335"
"Sahin S.E.; Tosun A.","Sahin, Sefa Eren (57208426892); Tosun, Ayse (26649652100)","57208426892; 26649652100","A conceptual replication on predicting the severity of software vulnerabilities","2019","ACM International Conference Proceeding Series","","","","244","250","6","","10.1145/3319008.3319033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064757691&doi=10.1145%2f3319008.3319033&partnerID=40&md5=a69f4be694c9b0467539389df6b59ab8","Software vulnerabilities may lead to crucial security risks in software systems. Thus, prioritization of the vulnerabilities is an important task for security teams, and assessing how severe the vulnerabilities are would help teams during fixing and maintenance activities. We replicated a prior work which aims to predict the severity of software vulnerabilities by grouping vulnerabilities into different severity levels. We follow their approach on feature extraction using word embeddings, and on prediction model using Convolutional Neural Networks (CNNs). In addition, Long Short Term Memory (LSTM) and Extreme Gradient Boosting (XGBoost) models are used. We also extend the replicated work by aiming to predict severity scores rather than levels. We carried out two experiments for predicting severity levels and severity scores of 82,974 vulnerabilities. On predicting the severity levels, our LSTM and CNN models perform similarly with an F1 score of 0.756 F1 score and 0.752, respectively. On predicting the severity scores, LSTM, CNN and XGBoost models perform 16.14%, 17.03%, 18.91% MAPE values, respectively. © 2019 Association for Computing Machinery.","Multi-class classification; Regression; Vulnerability severity prediction; Word embeddings","Convolutional neural networks; Embeddings; Forecasting; Long short-term memory; Predictive analytics; Software engineering; Gradient boosting; Maintenance activity; Multi-class classification; Prediction model; Prioritization; Regression; Software systems; Software vulnerabilities; Security of data","Conference paper","Final","","Scopus","2-s2.0-85064757691"
"Yin J.; Tang M.J.; Cao J.; Wang H.; You M.; Lin Y.","Yin, Jiao (54884588500); Tang, MingJian (57215896761); Cao, Jinli (7403353999); Wang, Hua (57215111932); You, Mingshan (57215898907); Lin, Yongzheng (18042423600)","54884588500; 57215896761; 7403353999; 57215111932; 57215898907; 18042423600","Adaptive Online Learning for Vulnerability Exploitation Time Prediction","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12343 LNCS","","","252","266","14","","10.1007/978-3-030-62008-0_18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096620723&doi=10.1007%2f978-3-030-62008-0_18&partnerID=40&md5=aae473ef995851426410801af19ef3df","Exploitation analysis is vital in evaluating the severity of software vulnerabilities and thus prioritizing the order of patching. Although a few methods have been proposed to predict the exploitability of vulnerabilities, most of them treat this problem as an offline binary classification problem. To suit the real-world data stream applications and provide more fine-grained results for vulnerability evaluation, we believe that it is better to treat the exploitation time prediction problem as a multiclass online learning problem. In this paper, we propose an adaptive online learning framework for exploitation time prediction to tackle the combined challenges posed by online learning, multiclass learning and dynamic class imbalance. Within this framework, we design a Sliding Window Imbalance Factor Technique (SWIFT) to capture the real-time imbalanced statuses and thus to handle the dynamic imbalanced problem. Experimental results on real-world data demonstrate that the proposed framework can improve the predictive performance for both the minority class and the majority class. © 2020, Springer Nature Switzerland AG.","Exploitation time prediction; Multiclass imbalance; Online learning","Data streams; Forecasting; Information systems; Information use; Systems engineering; Adaptive online learning; Binary classification problems; Multi-class learning; Predictive performance; Software vulnerabilities; Stream application; Time predictions; Vulnerability evaluations; E-learning","Conference paper","Final","","Scopus","2-s2.0-85096620723"
"Sheehan B.; Murphy F.; Mullins M.; Ryan C.","Sheehan, Barry (57192010626); Murphy, Finbarr (56363662600); Mullins, Martin (23670699800); Ryan, Cian (57195201959)","57192010626; 56363662600; 23670699800; 57195201959","Connected and autonomous vehicles: A cyber-risk classification framework","2019","Transportation Research Part A: Policy and Practice","124","","","523","536","13","","10.1016/j.tra.2018.06.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056248785&doi=10.1016%2fj.tra.2018.06.033&partnerID=40&md5=1985bac4110ae6b70d842b69b3825042","The proliferation of technologies embedded in connected and autonomous vehicles (CAVs) increases the potential of cyber-attacks. The communication systems between vehicles and infrastructure present remote attack access for malicious hackers to exploit system vulnerabilities. Increased connectivity combined with autonomous driving functions pose a considerable threat to the vast socioeconomic benefits promised by CAVs. However, the absence of historical information on cyber-attacks mean that traditional risk assessment methods are rendered ineffective. This paper proposes a proactive CAV cyber-risk classification model which overcomes this issue by incorporating known software vulnerabilities contained within the US National Vulnerability Database into model building and testing phases. This method uses a Bayesian Network (BN) model, premised on the variables and causal relationships derived from the Common Vulnerability Scoring Scheme (CVSS), to represent the probabilistic structure and parameterisation of CAV cyber-risk. The resulting BN model is validated with an out-of-sample test demonstrating nearly 100% prediction accuracy of the quantitative risk score and qualitative risk level. The model is then applied to the use-case of GPS systems of a CAV with and without cryptographic authentication. In the use case, we demonstrate how the model can be used to predict the effect of risk reduction measures. © 2018 The Authors","Auto insurance; Bayesian networks; Connected and autonomous vehicles; Cyber liability; Cyber-risk; Intelligent transport systems; Risk assessment","United States; Chicken anemia virus; Bayesian networks; Classification (of information); Computer crime; Crime; Intelligent systems; Network security; Personal computing; Risk assessment; Software testing; Traffic control; Historical information; Intelligent transport systems; National vulnerability database; Probabilistic structures; Risk assessment methods; Risk reduction measures; Socio-economic benefits; Software vulnerabilities; automation; Bayesian analysis; database; insurance system; intelligent transportation system; liability; model test; model validation; risk assessment; transportation infrastructure; transportation technology; vulnerability; Autonomous vehicles","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85056248785"
"Liu Y.; Li J.; Zhuang Y.","Liu, Yunfei (57205366217); Li, Jing (57203887438); Zhuang, Yi (7202294580)","57205366217; 57203887438; 7202294580","Instruction SDC Vulnerability prediction using long short-term memory neural network","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11323 LNAI","","","140","149","9","","10.1007/978-3-030-05090-0_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059736530&doi=10.1007%2f978-3-030-05090-0_12&partnerID=40&md5=0bba75ff2f683f8886b91018f8011cb1","Silent Data Corruption (SDC) is one of the serious issues in soft errors and it is difficult to detect because it can cause erroneous results without any indication. In order to solve this problem, a new SDC vulnerability prediction method based on deep learning model is proposed. Our method predicts the SDC vulnerability of each instruction in the program based on the inherent and dependent features of each instruction in the Lower Level Virtual Machine (LLVM) intermediate. Firstly, the features are extracted from benchmarks by LLVM passes and feature selection is performed. Then, LLVM Based Fault Injection Tool (LLFI) is used to get SDC vulnerability labels to obtain the SDC prediction data set. Long Short-Term Memory (LSTM) neural network is applied to classification of SDC vulnerability. Finally, compared with the model based on SVM and Decision Tree, the experiment results show that the average accuracy of LSTM in classification of SDC vulnerability is 11.73% higher than SVM, and 10.74% higher than Decision Tree. © 2018, Springer Nature Switzerland AG.","Fault injection; LSTM; Prediction; Silent data corruption","Brain; Crime; Data mining; Decision trees; Deep learning; Forecasting; Network security; Radiation hardening; Software testing; Fault injection; Fault-injection tools; Learning models; LSTM; Model-based OPC; Prediction methods; Silent data corruption (SDC); Silent data corruptions; Long short-term memory","Conference paper","Final","","Scopus","2-s2.0-85059736530"
"Subroto A.; Apriyana A.","Subroto, Athor (57207848140); Apriyana, Andri (57209232205)","57207848140; 57209232205","Cyber risk prediction through social media big data analytics and statistical machine learning","2019","Journal of Big Data","6","1","50","","","","","10.1186/s40537-019-0216-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066949681&doi=10.1186%2fs40537-019-0216-1&partnerID=40&md5=f412b0165973b00113a9715b9170c31c","As a natural outcome of achieving equilibrium, digital economic progress will most likely be subject to increased cyber risks. Therefore, the purpose of this study is to present an algorithmic model that utilizes social media big data analytics and statistical machine learning to predict cyber risks. The data for this study consisted of 83,015 instances from the common vulnerabilities and exposures (CVE) database (early 1999 to March 2017) and 25,599 cases of cyber risks from Twitter (early 2016 to March 2017), after which 1000 instances from both platforms were selected. The predictions were made by analyzing the software vulnerabilities to threats, based on social media conversations, while prediction accuracy was measured by comparing the cyber risk data from Twitter with that from the CVE database. Utilizing confusion matrix, we can achieve the best prediction by using Rweka package to carry out machine learning (ML) experimentation and artificial neural network (ANN) with the accuracy rate of 96.73%. Thus, in this paper, we offer new insights into cyber risks and how such vulnerabilities can be adequately understood and predicted. The findings of this study can be used by managers of public and private companies to formulate effective strategies for reducing cyber risks to critical infrastructures. © 2019, The Author(s).","Big data; Cyber risks; Machine learning; Non-traditional actuary; Predictive analytics; Social media","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85066949681"
"Ha D.T.; Retiere N.; Caputo J.-G.","Ha, Dinh Truc (56196324600); Retiere, Nicolas (6602711549); Caputo, Jean-Guy (56036239300)","56196324600; 6602711549; 56036239300","A New Metric to Quantify the Vulnerability of Power Grids","2019","Proceedings of 2019 International Conference on System Science and Engineering, ICSSE 2019","","","8823374","206","213","7","","10.1109/ICSSE.2019.8823374","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072923894&doi=10.1109%2fICSSE.2019.8823374&partnerID=40&md5=45842472a3665f1f42a138982718dec1","Major blackouts are due to cascading failures in power systems. These failures usually occur at vulnerable links of the network. To identify these, indicators have already been defined using complex network theory. However, most of these indicators only depend on the topology of the grid; they fail to detect the weak links. We introduce a new metric to identify the vulnerable lines, based on the load-flow equations and the grid geometry. Contrary to the topological indicators, ours is built from the electrical equations and considers the location and magnitude of the loads and of the power generators. We apply this new metric to the IEEE 118-bus system and compare its prediction of weak links to the ones given by an industrial software. The agreement is very well and shows that using our indicator a simple examination of the network and its generator and load distribution suffices to find the weak lines. © 2019 IEEE.","blackout; complex network theory; DC load flow; Hybrid topological model; Network capacity reserve metric; power systems; Vulnerability","Circuit theory; Complex networks; Electric fault currents; Electric load flow; Electric power transmission networks; Network security; Network theory (graphs); Standby power systems; Topology; blackout; DC load flow; Network Capacity; Topological models; Vulnerability; Outages","Conference paper","Final","","Scopus","2-s2.0-85072923894"
"Zhang K.","Zhang, Kevin (57214680430)","57214680430","A machine learning based approach to identify SQL injection vulnerabilities","2019","Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019","","","8952467","1286","1288","2","","10.1109/ASE.2019.00164","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078929889&doi=10.1109%2fASE.2019.00164&partnerID=40&md5=67f0c7fb50edadcddab280cf4b551d4f","This paper presents a machine learning classifier designed to identify SQL injection vulnerabilities in PHP code. Both classical and deep learning based machine learning algorithms were used to train and evaluate classifier models using input validation and sanitization features extracted from source code files. On ten-fold cross validations a model trained using Convolutional Neural Network(CNN) achieved the highest precision (95.4%), while a model based on Multilayer Perceptron(MLP) achieved the highest recall (63.7%) and the highest f-measure (0.746). © 2019 IEEE.","Deep learning; Prediction model; SQL injection; Vulnerability","Deep learning; Learning algorithms; Multilayer neural networks; Software engineering; Classifier models; Convolutional neural network; Cross validation; Input validation; Multi layer perceptron; Prediction model; SQL injection; Vulnerability; Machine learning","Conference paper","Final","","Scopus","2-s2.0-85078929889"
"Jimenez M.; Rwemalika R.; Papadakis M.; Sarro F.; Le Traon Y.; Harman M.","Jimenez, Matthieu (57191959513); Rwemalika, Renaud (57208769382); Papadakis, Mike (57197295611); Sarro, Federica (36631133800); Le Traon, Yves (55884641800); Harman, Mark (7006379048)","57191959513; 57208769382; 57197295611; 36631133800; 55884641800; 7006379048","The importance of accounting for real-world labelling when predicting software vulnerabilities","2019","ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering","","","","695","705","10","","10.1145/3338906.3338941","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071933945&doi=10.1145%2f3338906.3338941&partnerID=40&md5=23b49d655bce00ce29b7863d5ecab93a","Previous work on vulnerability prediction assume that predictive models are trained with respect to perfect labelling information (includes labels from future, as yet undiscovered vulnerabilities). In this paper we present results from a comprehensive empirical study of 1,898 real-world vulnerabilities reported in 74 releases of three security-critical open source systems (Linux Kernel, OpenSSL and Wiresark). Our study investigates the effectiveness of three previously proposed vulnerability prediction approaches, in two settings: with and without the unrealistic labelling assumption. The results reveal that the unrealistic labelling assumption can profoundly mis- lead the scientific conclusions drawn; suggesting highly effective and deployable prediction results vanish when we fully account for realistically available labelling in the experimental methodology. More precisely, MCC mean values of predictive effectiveness drop from 0.77, 0.65 and 0.43 to 0.08, 0.22, 0.10 for Linux Kernel, OpenSSL and Wiresark, respectively. Similar results are also obtained for precision, recall and other assessments of predictive efficacy. The community therefore needs to upgrade experimental and empirical methodology for vulnerability prediction evaluation and development to ensure robust and actionable scientific findings. © 2019 ACM.","Machine Learning; Prediction Modelling; Software Vulnerabilities","Forecasting; Learning systems; Linux; Open source software; Empirical studies; Experimental methodology; Open source system; Prediction modelling; Predictive models; Scientific findings; Security-critical; Software vulnerabilities; Open systems","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85071933945"
"Munaiah N.; Meneely A.","Munaiah, Nuthan (57190685740); Meneely, Andrew (23135193600)","57190685740; 23135193600","Data-driven insights from vulnerability discovery metrics","2019","Proceedings - 2019 IEEE/ACM Joint 4th International Workshop on Rapid Continuous Software Engineering and 1st International Workshop on Data-Driven Decisions, Experimentation and Evolution, RCoSE/DDrEE 2019","","","8818181","1","7","6","","10.1109/RCoSE/DDrEE.2019.00008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072760935&doi=10.1109%2fRCoSE%2fDDrEE.2019.00008&partnerID=40&md5=46681bbb2d0dfd1a1936e6c109a2c540","Software metrics help developers discover and fix mistakes. However, despite promising empirical evidence, vulnerability discovery metrics are seldom relied upon in practice. In prior research, the effectiveness of these metrics has typically been expressed using precision and recall of a prediction model that uses the metrics as explanatory variables. These prediction models, being black boxes, may not be perceived as useful by developers. However, by systematically interpreting the models and metrics, we can provide developers with nuanced insights about factors that have led to security mistakes in the past. In this paper, we present a preliminary approach to using vulnerability discovery metrics to provide insightful feedback to developers as they engineer software. We collected ten metrics (churn, collaboration centrality, complexity, contribution centrality, nesting, known offender, source lines of code, # inputs, # outputs, and # paths) from six open-source projects. We assessed the generalizability of the metrics across two contextual dimensions (application domain and programming language) and between projects within a domain, computed thresholds for the metrics using an unsupervised approach from literature, and assessed the ability of these unsupervised thresholds to classify risk from historical vulnerabilities in the Chromium project. The observations from this study feeds into our ongoing research to automatically aggregate insights from the various analyses to generate natural language feedback on security. We hope that our approach to generate automated feedback will accelerate the adoption of research in vulnerability discovery metrics. © 2019 IEEE.","Interpretation; Metric; Security; Threshold; Vulnerability","Open source software; Risk assessment; Interpretation; Metric; Security; Threshold; Vulnerability; Open systems","Conference paper","Final","","Scopus","2-s2.0-85072760935"
"Tavabi N.; Goyal P.; Almukaynizi M.; Shakarian P.; Lerman K.","Tavabi, Nazgol (57192665367); Goyal, Palash (57191745928); Almukaynizi, Mohammed (57201734872); Shakarian, Paulo (36635055900); Lerman, Kristina (6603963324)","57192665367; 57191745928; 57201734872; 36635055900; 6603963324","Darkembed: Exploit prediction with neural language models","2018","32nd AAAI Conference on Artificial Intelligence, AAAI 2018","","","","7849","7854","5","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056753402&partnerID=40&md5=af02fb452a0434717cfb1b7857cb0c67","                             Software vulnerabilities can expose computer systems to attacks by malicious actors. With the number of vulnerabilities discovered in the recent years surging, creating timely patches for every vulnerability is not always feasible. At the same time, not every vulnerability will be exploited by attackers; hence, prioritizing vulnerabilities by assessing the likelihood they will be exploited has become an important research problem. Recent works used machine learning techniques to predict exploited vulnerabilities by analyzing discussions about vulnerabilities on social media. These methods relied on traditional text processing techniques, which represent statistical features of words, but fail to capture their context. To address this challenge, we propose DarkEmbed, a neural language modeling approach that learns low dimensional distributed representations, i.e., embeddings, of darkweb/deepweb discussions to predict whether vulnerabilities will be exploited. By capturing linguistic regularities of human language, such as syntactic, semantic similarity and logic analogy, the learned embeddings are better able to classify discussions about exploited vulnerabilities than traditional text analysis methods. Evaluations demonstrate the efficacy of learned embeddings on both structured text (such as security blog posts) and unstructured text (darkweb/deepweb posts). DarkEmbed outperforms state-of-the-art approaches on the exploit prediction task with an F                             1                             -score of 0.74.                          Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","","Artificial intelligence; Computational linguistics; Forecasting; Learning systems; Natural language processing systems; Network security; Semantics; Text processing; Distributed representation; Machine learning techniques; Semantic similarity; Software vulnerabilities; State-of-the-art approach; Statistical features; Text-analysis methods; Unstructured texts; Modeling languages","Conference paper","Final","","Scopus","2-s2.0-85056753402"
"Hegedűs P.","Hegedűs, Péter (25926433300)","25926433300","Inspecting JavaScript Vulnerability Mitigation Patches with Automated Fix Generation in Mind","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12252 LNCS","","","975","988","13","","10.1007/978-3-030-58811-3_69","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092236545&doi=10.1007%2f978-3-030-58811-3_69&partnerID=40&md5=c5b98260fa2f2a5b1460dc35b4e74b7f","Software security has become a primary concern for both the industry and academia in recent years. As dependency on critical services provided by software systems grows globally, a potential security threat in such systems poses higher and higher risks (e.g. economical damage, a threat to human life, criminal activity). Finding potential security vulnerabilities at the code level automatically is a very popular approach to aid security testing. However, most of the methods based on machine learning and statistical models stop at listing potentially vulnerable code parts and leave their validation and mitigation to the developers. Automatic program repair could fill this gap by automatically generating vulnerability mitigation code patches. Nonetheless, it is still immature, especially in targeting security-relevant fixes. In this work, we try to establish a path towards automatic vulnerability fix generation techniques in the context of JavaScript programs. We inspect 361 actual vulnerability mitigation patches collected from vulnerability databases and GitHub. We found that vulnerability mitigation patches are not short on average and in many cases affect not just program code but test code as well. These results point towards that a general automatic repair approach targeting all the different types of vulnerabilities is not feasible. The analysis of the code properties and fix patterns for different vulnerability types might help in setting up a more realistic goal in the area of automatic JavaScript vulnerability repair. © 2020, Springer Nature Switzerland AG.","Automatic repair; JavaScript; Prediction models; Security; Vulnerability","High level languages; Software testing; Automatic programs; Criminal activities; Generation techniques; JavaScript programs; Security vulnerabilities; Software security; Vulnerability database; Vulnerability mitigation; Codes (symbols)","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85092236545"
"Kim S.; Ryou J.","Kim, Sangwho (57188726436); Ryou, Jaecheol (7005512822)","57188726436; 7005512822","Source Code Analysis for Static Prediction of Dynamic Memory Usage","2019","2019 International Conference on Platform Technology and Service, PlatCon 2019 - Proceedings","","","8669417","","","","","10.1109/PlatCon.2019.8669417","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063904561&doi=10.1109%2fPlatCon.2019.8669417&partnerID=40&md5=bb99b15cd5393786e4628c78e8251edd","We studied source code analysis techniques to predict statically how real programs work and use memory. If we can recognize problems of memory usage in the source code, we are able to prevent them and improve security in the software development phase. The problem detection techniques which are already existed can detect whether the program includes weak code such as Common Vulnerabilities and Exposures, Common Weakness Enumeration. However, these methods are not useful for finding problems in programs that do not include OpenSource. because they use hash value or pattern of weak code contained in OpenSource. Therefore, we propose a static prediction technique for dynamic memory usage with source code analysis without using techniques such as similarity detection. Also, we present how to calculate the values used for static prediction from the source code. © 2019 IEEE.","dynamic memory ussage; source code analysis; static analysis; static prediction","Codes (symbols); Computer programming languages; Forecasting; Software design; Common vulnerabilities and exposures; Dynamic memory; Memory usage; Prediction techniques; Problem detection; Similarity detection; Source code analysis; Source codes; Static analysis","Conference paper","Final","","Scopus","2-s2.0-85063904561"
"Johnston R.; Sarkani S.; Mazzuchi T.; Holzer T.; Eveleigh T.","Johnston, Reuben (57201489189); Sarkani, Shahryar (7005731083); Mazzuchi, Thomas (7004222916); Holzer, Thomas (55224121400); Eveleigh, Timothy (6506128557)","57201489189; 7005731083; 7004222916; 55224121400; 6506128557","Multivariate models using MCMCBayes for web-browser vulnerability discovery","2018","Reliability Engineering and System Safety","176","","","52","61","9","","10.1016/j.ress.2018.03.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045109919&doi=10.1016%2fj.ress.2018.03.024&partnerID=40&md5=bb304746a3683140a4e47b4842ca8287","Vulnerabilities that enable well-known exploit techniques are preventable, but their public discovery continues in software. Vulnerability discovery modeling (VDM) techniques were proposed to assist managers with decisions, but do not include influential variables describing the software release (SR) (e.g., code size and complexity characteristics) and security assessment profile (SAP) (e.g., security team size or skill). Consequently, they have been limited to modeling discoveries over time for SR and SAP scenarios of unique products, whose results are not readily comparable without making assumptions that equate all SR and SAP combinations under study. This article introduces a groundbreaking capability that allows forecasting expected discoveries over time for arbitrary SR and SAP combinations, thus enabling managers to better understand the effects of influential variables they control on the phenomenon. To do this, we use variables that describe arbitrary SR and SAP combinations and construct VDM extensions that parametrically scale results from a defined baseline SR and SAP to the arbitrary SR and SAP of interest. Scaling parameters are estimated using expert judgment data gathered with a novel pairwise comparison approach. These data are then used to demonstrate predictions and how multivariate VDM techniques could be used by software-makers. © 2018 Elsevier Ltd","Growth curve; Parametric scaling; Poisson process; Regression","Managers; Complexity characteristics; Growth curves; Pair-wise comparison; Parametric scaling; Poisson process; Regression; Security assessment; Vulnerability discovery; Human resource management","Article","Final","","Scopus","2-s2.0-85045109919"
"Saccente N.; Dehlinger J.; Deng L.; Chakraborty S.; Xiong Y.","Saccente, Nicholas (57215141116); Dehlinger, Josh (10143022300); Deng, Lin (55142608500); Chakraborty, Suranjan (23666293600); Xiong, Yin (57214365580)","57215141116; 10143022300; 55142608500; 23666293600; 57214365580","Project achilles: A prototype tool for static method-level vulnerability detection of Java source code using a recurrent neural network","2019","Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering Workshops, ASEW 2019","","","8967427","114","121","7","","10.1109/ASEW.2019.00040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079276484&doi=10.1109%2fASEW.2019.00040&partnerID=40&md5=ce3b6dc8590da9710450470beb392500","Software has become an essential component of modern life, but when software vulnerabilities threaten the security of users, new ways of analyzing for software security must be explored. Using the National Institute of Standards and Technology's Juliet Java Suite, containing thousands of examples of defective Java methods for a variety of vulnerabilities, a prototype tool was developed implementing an array of Long-Short Term Memory Recurrent Neural Networks to detect vulnerabilities within source code. The tool employs various data preparation methods to be independent of coding style and to automate the process of extracting methods, labeling data, and partitioning the dataset. The result is a prototype command-line utility that generates an n-dimensional vulnerability prediction vector. The experimental evaluation using 44,495 test cases indicates that the tool can achieve an accuracy higher than 90% for 24 out of 29 different types of CWE vulnerabilities. © 2019 IEEE.","Machine learning; Security vulnerability analysis; Software security","Codes (symbols); Learning systems; Network coding; Network security; Recurrent neural networks; Software engineering; Technical presentations; Experimental evaluation; Java source codes; National Institute of Standards and Technology; Prediction vectors; Security vulnerability analysis; Software security; Software vulnerabilities; Vulnerability detection; Java programming language","Conference paper","Final","","Scopus","2-s2.0-85079276484"
"Ognawala S.; Amato R.N.; Pretschner A.; Kulkarni P.","Ognawala, Saahil (56380606900); Amato, Ricardo Nales (57204510131); Pretschner, Alexander (12645083400); Kulkarni, Pooja (57204512832)","56380606900; 57204510131; 12645083400; 57204512832","Automatically assessing vulnerabilities discovered by compositional analysis","2018","MASES 2018 - Proceedings of the 1st International Workshop on Machine Learning and Software Engineering in Symbiosis, co-located with ASE 2018","","","","16","25","9","","10.1145/3243127.3243130","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055871388&doi=10.1145%2f3243127.3243130&partnerID=40&md5=93289a9d102e4f302f15372163f06ed0","Testing is the most widely employed method to find vulnerabilities in real-world software programs. Compositional analysis, based on symbolic execution, is an automated testing method to find vulnerabilities in medium- To large-scale programs consisting of many interacting components. However, existing compositional analysis frameworks do not assess the severity of reported vulnerabilities. In this paper, we present a framework to analyze vulnerabilities discovered by an existing compositional analysis tool and assign CVSS3 (Common Vulnerability Scoring System v3.0) scores to them, based on various heuristics such as interaction with related components, ease of reachability, complexity of design and likelihood of accepting unsanitized input. By analyzing vulnerabilities reported with CVSS3 scores in the past, we train simple machine learning models. By presenting our interactive framework to developers of popular open-source software and other security experts, we gather feedback on our trained models and further improve the features to increase the accuracy of our predictions. By providing qualitative (based on community feedback) and quantitative (based on prediction accuracy) evidence from 21 open-source programs, we show that our severity prediction framework can effectively assist developers with assessing vulnerabilities. © 2018 Association for Computing Machinery.","Compositional analysis; Software testing; Symbolic execution; Vulnerability assessment","Artificial intelligence; Forecasting; Learning systems; Machinery; Model checking; Open systems; Software testing; Testing; Common vulnerability scoring systems; Compositional analysis; Large-scale programs; Machine learning models; Open source projects; Prediction accuracy; Symbolic execution; Vulnerability assessments; Open source software","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85055871388"
"Sedalor T.; Fleming D.C.","Sedalor, Teddy (54403858500); Fleming, David C. (7202941305)","54403858500; 7202941305","Predicting hydrodynamic ram damage in bonded composite tanks using progressive damage failure","2020","AIAA Scitech 2020 Forum","1 PartF","","","","","","","10.2514/6.2020-0970","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091946273&doi=10.2514%2f6.2020-0970&partnerID=40&md5=ae9303d05bcd5c1bd03144ec4c72a47e","The Live Fire Law mandates new vehicles to be realistically tested against perceived threats to assess vulnerability. One consequence of this requirement is Hydrodynamic Ram (HRAM) testing of projectile impact to fuel tanks. HRAM occurs due to energy transfer from a high velocity projectile going through a fluid medium in an enclosed structure. The result is usually catastrophic structural damage, which makes testing rather expensive. Therefore the need arises for numerical methods to supplement the testing and provide more test data during the article test. This research uses the building block approach to extrapolate failure data from single joint coupon level testing to predict failure of a fuel tank using progressive damage failure. The ALE fluid-structure interaction technique is used in concert with Cohesive Zone Modeling via the commercial explicit software LS-DYNA to achieve damage prediction in a fuel tank. Both numerical techniques require fine meshes with stringent minimum dimension requirements to ensure accurate results. These requirements are impractical for analyzing large articles like fuel tanks. Therefore a coarse mesh approach is developed to satisfy the requirement for both CZM and ALE FSI techniques while providing accurate damage prediction results. © 2020, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.","","Automobile fuel tanks; Aviation; Energy transfer; Fluid structure interaction; Forecasting; Fuels; Hydrodynamics; Numerical methods; Projectiles; Structural analysis; Tanks (military); Bonded composites; Building blockes; Cohesive zone model; Damage prediction; Numerical techniques; Progressive damage; Projectile impact; Structural damages; Failure (mechanical)","Conference paper","Final","","Scopus","2-s2.0-85091946273"
"Chen Q.; Bao L.; Li L.; Xia X.; Cai L.","Chen, Qiuyuan (57212535221); Bao, Lingfeng (56609745200); Li, Li (56438149900); Xia, Xin (54586248800); Cai, Liang (36724848100)","57212535221; 56609745200; 56438149900; 54586248800; 36724848100","Categorizing and Predicting Invalid Vulnerabilities on Common Vulnerabilities and Exposures","2018","Proceedings - Asia-Pacific Software Engineering Conference, APSEC","2018-December","","8719428","345","354","9","","10.1109/APSEC.2018.00049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066781873&doi=10.1109%2fAPSEC.2018.00049&partnerID=40&md5=84a850623f59387322cab3f4d7430dac","To share vulnerability information across separate databases, tools, and services, newly identified vulnerabilities are recurrently reported to Common Vulnerabilities and Exposures (CVE) database.Unfortunately, not all vulnerability reports will be accepted. Some of them might get rejected or be accepted with disputations.In this work, we refer to those rejected or disputed CVEs as invalid vulnerability reports. Invalid vulnerability reports not only cause unnecessary efforts to confirm the vulnerability but also impact the reputation of the software vendors. In this paper, we aim to understand the root causes of invalid vulnerability reports and build a prediction model to automatically identify them.To this end, we first leverage card sorting to categorize invalid vulnerability reports, from which six main reasons are observed for rejected and disputed CVEs, respectively.Then, we propose a text mining approach to predict the invalid vulnerability reports. Our experiments reveal that the proposed text mining approach can achieve an AUC score of 0.87 for predicting invalid vulnerabilities. We also discuss the implications of our study: our categorization can be used to guide new committer to avoid these traps; some root causes of invalid CVEs can be avoided by using automatic techniques or optimizing reviewing mechanism; invalid vulnerability reports data should not be neglected. © 2018 IEEE.","invalid CVE; prediction model; reason categorization","Data mining; Software engineering; Automatic technique; Card-sorting; Common vulnerabilities and exposures; invalid CVE; Prediction model; reason categorization; Software vendors; Text mining; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85066781873"
"Ariel Pinto C.; Zurasky M.","Ariel Pinto, C. (7102500911); Zurasky, Matthew (57216391706)","7102500911; 57216391706","Systemic methodology for cyber offense and defense","2020","Proceedings of the 15th International Conference on Cyber Warfare and Security, ICCWS 2020","","","","380","390","10","","10.34190/ICCWS.20.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083368485&doi=10.34190%2fICCWS.20.032&partnerID=40&md5=a238503762903d148ba219897bf90881","This paper describes a systemic method towards standardization of a cyber weapon effectiveness and effectiveness prediction process to promote consistency and improve cyber weapon system evaluation accuracy - for both offensive and defensive postures. The approach included theoretical examination of existing effectiveness prediction processes for kinetic and directed energy weapons, complemented with technical and social aspects of cyber realm. The examination highlighted several paradigm-shifts needed to transition from purely kinetic-based processes and transition into the realm of combined kinetic and cyber weapons. Components of the new method for cyber weapons are cyber payload assessment, effects identification, and target assessment. The ultimate outcome of method is the 'Probability of Kill' for a cyber weapon paired with a threat and within a given situation. This probability is a function of factors such as intelligence gathered on the latency of information, access points, hardware and software configurations, accuracy and completeness of network map, understanding of operations tempo; likelihood that vulnerabilities being exploited are patched and IT's ability to detect and respond to the delivery of the cyber payload; and probability that the payload will achieve the desired mission effects. Aside from the use of this method for offensive purposes, it can also be mirrored as cyber defense and can serve as basis for developing cyber defense strategies, such as focused counter intelligence on access points, hardware and software configurations, and network map and architecture, comprehensive patching to assure most current and complete patches are deployed, and well trained and equipped IT with ability to detect and respond to cyber payloads. © 2020. the authors. All Rights Reserved.","Defense; Effectiveness; Offense; Risk; Systematic; Systemic","Computer crime; Directed-energy weapons; Kinetics; Probability; Social aspects; Access points; Cyber defense; Hardware and software; Paradigm shifts; Prediction process; Probability of kill; Weapon effectiveness; Weapon system; Network security","Conference paper","Final","","Scopus","2-s2.0-85083368485"
"Wei S.; Zhong H.; Shan C.; Ye L.; Du X.; Guizani M.","Wei, Shengjun (8952204800); Zhong, Hao (36572693400); Shan, Chun (56102670900); Ye, Lin (35754159200); Du, Xiaojiang (8371278000); Guizani, Mohsen (7004750176)","8952204800; 36572693400; 56102670900; 35754159200; 8371278000; 7004750176","Vulnerability Prediction Based on Weighted Software Network for Secure Software Building","2018","2018 IEEE Global Communications Conference, GLOBECOM 2018 - Proceedings","","","8647583","","","","","10.1109/GLOCOM.2018.8647583","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063477400&doi=10.1109%2fGLOCOM.2018.8647583&partnerID=40&md5=3aac9a65100d390af59d74a9d2a0d7dc","To build a secure communications software, Vulnerability Prediction Models (VPMs) are used to predict vulnerable software modules in the software system before software security testing. At present many software security metrics have been proposed to design a VPM. In this paper, we predict vulnerable classes in a software system by establishing the system's weighted software network. The metrics are obtained from the nodes' attributes in the weighted software network. We design and implement a crawler tool to collect all public security vulnerabilities in Mozilla Firefox. Based on these data, the prediction model is trained and tested. The results show that the VPM based on weighted software network has a good performance in accuracy, precision, and recall. Compared to other studies, it shows that the performance of prediction has been improved greatly in Pr and Re. © 2018 IEEE.","","Forecasting; Software testing; Communications software; Design and implements; Prediction model; Prediction-based; Software modules; Software security; Software security testing; Software systems; Network security","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85063477400"
"Kaya A.; Keceli A.S.; Catal C.; Tekinerdogan B.","Kaya, Aydin (35102550900); Keceli, Ali Seydi (12769505400); Catal, Cagatay (22633325800); Tekinerdogan, Bedir (15761578600)","35102550900; 12769505400; 22633325800; 15761578600","The impact of feature types, classifiers, and data balancing techniques on software vulnerability prediction models","2019","Journal of Software: Evolution and Process","31","9","e2164","","","","","10.1002/smr.2164","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073516330&doi=10.1002%2fsmr.2164&partnerID=40&md5=d3e234fa0c0c8e889abaa21c1fa6b35f","Software vulnerabilities form an increasing security risk for software systems, that might be exploited to attack and harm the system. Some of the security vulnerabilities can be detected by static analysis tools and penetration testing, but usually, these suffer from relatively high false positive rates. Software vulnerability prediction (SVP) models can be used to categorize software components into vulnerable and neutral components before the software testing phase and likewise increase the efficiency and effectiveness of the overall verification process. The performance of a vulnerability prediction model is usually affected by the adopted classification algorithm, the adopted features, and data balancing approaches. In this study, we empirically investigate the effect of these factors on the performance of SVP models. Our experiments consist of four data balancing methods, seven classification algorithms, and three feature types. The experimental results show that data balancing methods are effective for highly unbalanced datasets, text-based features are more useful, and ensemble-based classifiers provide mostly better results. For smaller datasets, Random Forest algorithm provides the best performance and for the larger datasets, RusboostTree achieves better performance. © 2019 John Wiley & Sons, Ltd.","classification models; data sampling; imbalance datasets; machine learning; performance analysis; software vulnerability prediction","Balancing; Decision trees; Forecasting; Learning systems; Security of data; Software testing; Static analysis; Verification; Classification models; Data sampling; imbalance datasets; Performance analysis; Software vulnerabilities; Classification (of information)","Conference paper","Final","","Scopus","2-s2.0-85073516330"
"Yang M.; Wu J.; Ji S.; Luo T.; Wu Y.","Yang, Mutian (55930177900); Wu, Jingzheng (36607190700); Ji, Shouling (36918358000); Luo, Tianyue (56747188000); Wu, Yanjun (7406899147)","55930177900; 36607190700; 36918358000; 56747188000; 7406899147","Pre-patch: Find hidden threats in open software based on machine learning method","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10975 LNCS","","","48","65","17","","10.1007/978-3-319-94472-2_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049362889&doi=10.1007%2f978-3-319-94472-2_4&partnerID=40&md5=21e266a811842bada9017035ce8f8e2c","The details of vulnerabilities are always kept confidential until fixed, which is an efficient way to avoid the exploitations and attacks. However, the Security Related Commits (SRCs), used to fix the vulnerabilities in open source software, usually lack proper protections. Most SRCs are released in code repositories such as Git, Github, Sourceforge, etc. earlier than the corresponding vulnerabilities published. These commits often previously disclose the vital information which can be used by the attackers to locate and exploit the vulnerable code. Therefore, we defined the pre-leaked SRC as the Pre-Patch problem and studied its hidden threats to the open source software. In this paper, we presented an Automatic Security Related Commits Detector (ASRCD) to rapidly identify the Pre-Patch problems from the numerous commits in code repositories by learning the features of SRCs. We implemented ASRCD and evaluated it with 78,218 real-world commits collected from Linux Kernel, OpenSSL, phpMyadmin and Mantisbt released between 2016 to 2017, which contain 227 confirmed SRCs. ASRCD successfully identified 206 SRCs from the 4 projects, including 140 known SRCs (recall rate: 61.7% on average) and 66 new high-suspicious. In addition, 5 of the SRCs have been published after our prediction. The results show that: (1) the Pre-Patch is really a hidden threat to open source software; and (2) the proposed ASRCD is effective in identifying such SRCs. Finally, we recommended the identified SRCs should be fixed as soon as possible. © Springer International Publishing AG, part of Springer Nature 2018.","Code repository; Open source software; Pre-Patch; Vulnerability","Codes (symbols); Computer operating systems; Learning systems; Open systems; Code repository; Linux kernel; On-machines; Open software; Pre-Patch; Recall rate; SourceForge; Vulnerability; Open source software","Conference paper","Final","","Scopus","2-s2.0-85049362889"
"Benthall S.","Benthall, Sebastian (23570794800)","23570794800","Assessing software supply chain risk using public data","2017","2017 IEEE 28th Annual Software Technology Conference, STC 2017","2017-January","","","1","5","4","","10.1109/STC.2017.8234461","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049264611&doi=10.1109%2fSTC.2017.8234461&partnerID=40&md5=006dfb5a9be8972fe6028b63c4d7b6ba","The software supply chain is a source of cybersecurity risk for many commercial and government organizations. Public data may be used to inform automated tools for detecting software supply chain risk during continuous integration and deployment. We link data from the National Vulnerability Database (NVD) with open version control data for the open source project OpenSSL, a widely used secure networking library that made the news when a significant vulnerability, Heartbleed, was discovered in 2014. We apply the Alhazmi-Malaiya Logistic (AML) model for software vulnerability discovery to this case. This model predicts a sigmoid cumulative vulnerability discovery function over time. Some versions of OpenSSL do not conform to the predictions of the model because they contain a temporary plateau in the cumulative vulnerability discovery plot. This temporary plateau feature is an empirical signature of a security failure mode that may be useful in future studies of software supply chain risk. © 2017 IEEE.","security; supply chain risk; Vulnerability discovery","Risk assessment; Supply chains; Continuous integrations; Government organizations; National vulnerability database; security; Software supply chains; Software vulnerabilities; Supply chain risk; Vulnerability discovery; Open source software","Conference paper","Final","","Scopus","2-s2.0-85049264611"
"Weiser S.; Zankl A.; Spreitzer R.; Miller K.; Mangard S.; Sigl G.","Weiser, Samuel (56348703700); Zankl, Andreas (57204336385); Spreitzer, Raphael (55814044800); Miller, Katja (57191476275); Mangard, Stefan (8840189200); Sigl, Georg (7006399344)","56348703700; 57204336385; 55814044800; 57191476275; 8840189200; 7006399344","DATA - Differential address trace analysis: Finding address-based side-channels in binaries","2018","Proceedings of the 27th USENIX Security Symposium","","","","603","620","17","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060006982&partnerID=40&md5=0e89cb76c32fcd103cdefe8270f64d04","Cryptographic implementations are a valuable target for address-based side-channel attacks and should, thus, be protected against them. Countermeasures, however, are often incorrectly deployed or completely omitted in practice. Moreover, existing tools that identify information leaks in programs either suffer from imprecise abstraction or only cover a subset of possible leaks. We systematically address these limitations and propose a new methodology to test software for information leaks. In this work, we present DATA, a differential address trace analysis framework that detects address-based side-channel leaks in program binaries. This accounts for attacks exploiting caches, DRAM, branch prediction, controlled channels, and likewise. DATA works in three phases. First, the program under test is executed to record several address traces. These traces are analyzed using a novel algorithm that dynamically re-aligns traces to increase detection accuracy. Second, a generic leakage test filters differences caused by statistically independent program behavior, e.g., randomization, and reveals true information leaks. The third phase classifies these leaks according to the information that can be obtained from them. This provides further insight to security analysts about the risk they pose in practice. We use DATA to analyze OpenSSL and PyCrypto in a fully automated way. Among several expected leaks in symmetric ciphers, DATA also reveals known and previously unknown leaks in asymmetric primitives (RSA, DSA, ECDSA), and DATA identifies erroneous bug fixes of supposedly fixed constant-time vulnerabilities. © 2018 Proceedings of the 27th USENIX Security Symposium. All rights reserved.","","Software testing; Trace analysis; Branch prediction; Cryptographic implementation; Detection accuracy; Fully automated; Novel algorithm; Program behavior; Program binary; Symmetric cipher; Side channel attack","Conference paper","Final","","Scopus","2-s2.0-85060006982"
"Akula R.; Yousefi N.; Garibay I.","Akula, Ramya (57209295811); Yousefi, Niloofar (56439502300); Garibay, Ivan (6505821746)","57209295811; 56439502300; 6505821746","DeepFork: Supervised prediction of information diffusion in GitHub","2019","Proceedings of the International Conference on Industrial Engineering and Operations Management","2019","MAR","","3640","3651","11","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067262945&partnerID=40&md5=4e0567bfae1054fe1c9a05e085c7d568","Information spreads on complex social networks extremely fast. A piece of information can go viral in no time and can be harmful. Often it is hard to stop this information spread causing social unrest. An intentional spread of software vulnerabilities in GitHub has caused millions of dollars in losses. GitHub is a social coding platform that enables a huge number of open source software projects to thrive. To better understand how the information spreads on GitHub, we develop a deep neural network model: ""DeepFork"", a supervised machine learning based approach that aims to predict information diffusion; considering node as well as topological features in complex social networks. In our empirical studies, we observed that information diffusion can be detected by link prediction using supervised learning. This model investigates the followee-follower influence that underlay information dynamics in social coding platform. DeepFork outperforms other machine learning models as it better learns the discriminative patterns from the input features. DeepFork helps us in understand human influence on information spread and evolution. © IEOM Society International.","Deep Neural Networks; GitHub; Information Diffusion; Link Prediction; Supervised Learning","","Conference paper","Final","","Scopus","2-s2.0-85067262945"
"Qiu Y.; Liu Y.; Liu A.; Zhu J.; Xu J.","Qiu, Yu (57212485076); Liu, Yun (57191434431); Liu, Ao (57208837040); Zhu, Jingwen (57194067799); Xu, Jing (57221066497)","57212485076; 57191434431; 57208837040; 57194067799; 57221066497","Automatic Feature Exploration and an Application in Defect Prediction","2019","IEEE Access","7","","8794540","112097","112112","15","","10.1109/ACCESS.2019.2934530","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097288578&doi=10.1109%2fACCESS.2019.2934530&partnerID=40&md5=91ccc4f52218189f2ddb510dfa03bfa3","Many software engineering tasks heavily rely on hand-crafted software features, e.g., defect prediction, vulnerability discovery, software requirements, code review, and malware detection. Previous solutions to these tasks usually directly use the hand-crafted features or feature selection techniques for classification or regression, which usually leads to suboptimal results due to their lack of powerful representations of the hand-crafted features. To address the above problem, in this paper, we adopt the effort-aware just-in-time software defect prediction (JIT-SDP), which is a typical hand-crafted-feature-based task, as an example, to exploit new possible solutions. We propose a new model, named neural forest (NF), which uses the deep neural network and decision forest to build a holistic system for the automatic exploration of powerful feature representations that are used for the following classification. NF first employs a deep neural network to learn new feature representations from hand-crafted features. Then, a decision forest is connected after the neural network to perform classification and in the meantime, to guide the learning of feature representation. NF mainly aims at solving the challenging problem of combining the two different worlds of neural networks and decision forests in an end-to-end manner. When compared with previous state-of-the-art defect predictors and five designed baselines on six well-known benchmarks for within- and cross-project defect prediction, NF achieves significantly better results. The proposed NF model is generic to the classification problems which rely on the hand-crafted features. © 2013 IEEE.","defect prediction; Feature exploration; hand-crafted features","Classification (of information); Deep neural networks; Defects; Feature extraction; Forecasting; Forestry; Just in time production; Software engineering; Defect prediction; Feature representation; Malware detection; Selection techniques; Software defect prediction; Software features; Software requirements; Vulnerability discovery; Neural networks","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85097288578"
"Bilgin Z.; Ersoy M.A.; Soykan E.U.; Tomur E.; Comak P.; Karacay L.","Bilgin, Zeki (57221762573); Ersoy, Mehmet Akif (57215122777); Soykan, Elif Ustundag (57188583178); Tomur, Emrah (6504092986); Comak, Pinar (57195774159); Karacay, Leyli (57217278858)","57221762573; 57215122777; 57188583178; 6504092986; 57195774159; 57217278858","Vulnerability Prediction from Source Code Using Machine Learning","2020","IEEE Access","8","","9167194","150672","150684","12","","10.1109/ACCESS.2020.3016774","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090353514&doi=10.1109%2fACCESS.2020.3016774&partnerID=40&md5=c694d3ad52bfd9aefe51c3c8e1766cee","As the role of information and communication technologies gradually increases in our lives, software security becomes a major issue to provide protection against malicious attempts and to avoid ending up with noncompensable damages to the system. With the advent of data-driven techniques, there is now a growing interest in how to leverage machine learning (ML) as a software assurance method to build trustworthy software systems. In this study, we examine how to predict software vulnerabilities from source code by employing ML prior to their release. To this end, we develop a source code representation method that enables us to perform intelligent analysis on the Abstract Syntax Tree (AST) form of source code and then investigate whether ML can distinguish vulnerable and nonvulnerable code fragments. To make a comprehensive performance evaluation, we use a public dataset that contains a large amount of function-level real source code parts mined from open-source projects and carefully labeled according to the type of vulnerability if they have any.We show the effectiveness of our proposed method for vulnerability prediction from source code by carrying out exhaustive and realistic experiments under different regimes in comparison with state-of-art methods. © 2013 IEEE.","AST; machine learning; source code; vulnerability prediction","Arts computing; Codes (symbols); Computer programming languages; Forecasting; Large dataset; Machine learning; Open source software; Predictive analytics; Trees (mathematics); Abstract Syntax Trees; Comprehensive performance evaluation; Data driven technique; Information and Communication Technologies; Software vulnerabilities; Source code representations; State-of-art methods; Trustworthy software systems; Open systems","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85090353514"
"Kalouptsoglou I.; Siavvas M.; Tsoukalas D.; Kehagias D.","Kalouptsoglou, Ilias (57219327969); Siavvas, Miltiadis (57194500913); Tsoukalas, Dimitrios (57208865760); Kehagias, Dionysios (7003972544)","57219327969; 57194500913; 57208865760; 7003972544","Cross-Project Vulnerability Prediction Based on Software Metrics and Deep Learning","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12252 LNCS","","","877","893","16","","10.1007/978-3-030-58811-3_62","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092243334&doi=10.1007%2f978-3-030-58811-3_62&partnerID=40&md5=bafb1df7efaa8d10910b7cd27e3df663","Vulnerability prediction constitutes a mechanism that enables the identification and mitigation of software vulnerabilities early enough in the development cycle, improving the security of software products, which is an important quality attribute according to ISO/IEC 25010. Although existing vulnerability prediction models have demonstrated sufficient accuracy in predicting the occurrence of vulnerabilities in the software projects with which they have been trained, they have failed to demonstrate sufficient accuracy in cross-project prediction. To this end, in the present paper we investigate whether the adoption of deep learning along with software metrics may lead to more accurate cross-project vulnerability prediction. For this purpose, several machine learning (including deep learning) models are constructed, evaluated, and compared based on a dataset of popular real-world PHP software applications. Feature selection is also applied with the purpose to examine whether it has an impact on cross-project prediction. The results of our analysis indicate that the adoption of software metrics and deep learning may result in vulnerability prediction models with sufficient performance in cross-project vulnerability prediction. Another interesting conclusion is that the performance of the models in cross-project prediction is enhanced when the projects exhibit similar characteristics with respect to their software metrics. © 2020, Springer Nature Switzerland AG.","Security; Software quality; Vulnerability prediction","Application programs; Forecasting; Learning systems; Predictive analytics; Development cycle; Prediction model; Prediction-based; Quality attributes; Software applications; Software products; Software project; Software vulnerabilities; Deep learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85092243334"
"El Hachem J.; Chiprianov V.; Graciano Neto V.V.; Aniorte P.","El Hachem, Jamal (57021808400); Chiprianov, Vanea (35089995400); Graciano Neto, Valdemar Vicente (37104663200); Aniorte, Philippe (6507669143)","57021808400; 35089995400; 37104663200; 6507669143","Extending a multi-agent systems simulation architecture for systems-of-systems security analysis","2018","2018 13th System of Systems Engineering Conference, SoSE 2018","","","8428776","276","283","7","","10.1109/SYSOSE.2018.8428776","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052316350&doi=10.1109%2fSYSOSE.2018.8428776&partnerID=40&md5=5b3d694e1dbbde47c850a206feb9ed24","Security is an important concern for software-intensive Systems-of-Systems (SoS). Architectural analysis for SoS secturity assessment should be performed at early stages of development. Such activity could prevent vulnerabilities and avoid potential cascading attack emergent behaviors, i.e., a succession of security vulnerabilities that emerge from individual constituents security fragilities, potentially causing interruption and collapse of SoS operation. Model simulation can prevent these issues by predicting, at design-time, how SoS will behave regarding its reaction to potential attacks. As security is a quality attribute, i.e., a property that comes up from the relation between software parts, software architecture analysis and simulation are an additional support for the prediction of SoS security. However, despite recent advances in such area, few simulation approaches have tackled simulation of secure SoS architectures where the basis of the described models are the SoS behavior or the interactions among the SoS Constituent Systems (CS). The main contribution of this paper is offering a big picture of how recent advances on SoS security analysis via simulations can form a robust framework for SoS security prediction. We argue the pertinence of Multi-Agent Systems (MAS) for SoS simulation due to similarities between MAS and SoS concepts, and we report how MAS simulation enables the visualization of emergent behaviors and how they impact the SoS security. Our results to foster SoS security analysis include (i) an extension of a MAS conceptual model and platform to include security concepts, (ii) a Model-Driven Engineering (MDE) approach that adopts automatic mappings between secure SoS architecture modeled using an existing SysML-based modeling language, namely the SoSSecML, and (iii) a MAS platform to support such analysis. © 2018 IEEE.","","Computer software; Forecasting; Mapping; Modeling languages; Multi agent systems; Quality control; Security systems; Systems engineering; Architectural analysis; Model-driven Engineering; Quality attributes; Security vulnerabilities; Simulation approach; Software architecture analysis; Software intensive systems; Systems of systems; System of systems","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85052316350"
"Siavvas M.; Kehagias D.; Tzovaras D.","Siavvas, Miltiadis (57194500913); Kehagias, DIonysios (7003972544); Tzovaras, DImitrios (13105681700)","57194500913; 7003972544; 13105681700","A Preliminary Study on the Relationship among Software Metrics and Specific Vulnerability Types","2018","Proceedings - 2017 International Conference on Computational Science and Computational Intelligence, CSCI 2017","","","8560918","916","921","5","","10.1109/CSCI.2017.159","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060655560&doi=10.1109%2fCSCI.2017.159&partnerID=40&md5=d44a685adcec72bc7653398cdc144af4","Several studies have highlighted the ability of software metrics to predict vulnerabilities. However, limited attention has been given on the capacity of software metrics to discriminate between different types of vulnerabilities, while the existence of potential interdependencies among different vulnerability types has not been studied yet. For this purpose, an empirical study was conducted based on 100 widely-used Java libraries. A wide range of software metrics were calculated for each project of the code base, along with the densities of a carefully selected set of vulnerability categories, which were quantified through static analysis. Correlation analysis was employed in order to find statistically significant relationships. The preliminary results suggest that: (i) software metrics may not be sufficient indicators of specific vulnerability types, (ii) software metrics are more capable of discriminating between security-specific and quality-specific weaknesses, than between specific vulnerability types, (iii) previously uninvestigated metrics may be good indicators of security issues, and (iv) important interdependencies may exist among security-specific issues. To the best of our knowledge, this is the largest study in terms of code base size, while it is the first attempt for finding interdependencies among different vulnerability types. © 2017 IEEE.","software metrics; software security; static analysis; vulnerability density; vulnerability prediction","Artificial intelligence; Correlation analysis; Empirical studies; Java library; Limited attentions; Security issues; Software metrics; Software security; Static analysis","Conference paper","Final","","Scopus","2-s2.0-85060655560"
"Perwira R.I.; Fauziah Y.; Mahendra I.P.R.; Prasetyo D.B.; Simanjuntak O.S.","Perwira, Rifki Indra (57209452958); Fauziah, Yuli (57215431653); Mahendra, I Putu Retya (57215415941); Prasetyo, Dessyanto Boedi (57215410308); Simanjuntak, Oliver Samuel (57215415610)","57209452958; 57215431653; 57215415941; 57215410308; 57215415610","Anomaly-based Intrusion Detection and Prevention Using Adaptive Boosting in Software-defined Network","2019","Proceeding - 2019 5th International Conference on Science in Information Technology: Embracing Industry 4.0: Towards Innovation in Cyber Physical System, ICSITech 2019","","","8987531","188","192","4","","10.1109/ICSITech46713.2019.8987531","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080907524&doi=10.1109%2fICSITech46713.2019.8987531&partnerID=40&md5=5744945f9ee9fe7b4183beb194a75e2f","Anomaly-based intrusion detection and prevention technique is a technology needed in a software-defined network (SDN). The change in the SDN paradigm into a centralized architecture causes one side of weakness, namely vulnerability from denial of service (DoS) attacks. A large amount of data requested from clients to servers in a short time can be used as prediction data using decision stump to produce learning data. Learning data that have been formed will be used to make predictions. This research aims to detect and prevent DoS attacks using an anomaly-based adaptive boosting algorithm. The experimental test results obtained in this paper show that the effectiveness of the adaptive boosting algorithm in detecting attacks reaches 93.3% and can deny access in real-time. The conclusion is that the adaptive boosting algorithm can be used in building the Intrusion detection and prevention system. © 2019 IEEE.","adaptive boosting; DoS; intrusion detection; software-defined network","Adaptive boosting; Denial-of-service attack; DOS; Embedded systems; Software defined networking; Adaptive boosting algorithms; Anomaly-based intrusion detection; Centralized architecture; Decision stumps; Denial of Service; Detecting attacks; Experimental test; Intrusion detection and prevention systems; Intrusion detection","Conference paper","Final","","Scopus","2-s2.0-85080907524"
"Theisen C.; Sohn H.; Tripp D.; Williams L.","Theisen, Christopher (57216482869); Sohn, Hyunwoo (57205391673); Tripp, Dawson (57205395438); Williams, Laurie (35565101900)","57216482869; 57205391673; 57205395438; 35565101900","BP: Profiling vulnerabilities on the attack surface","2018","Proceedings - 2018 IEEE Cybersecurity Development Conference, SecDev 2018","","","8543394","110","119","9","","10.1109/SecDev.2018.00022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059869779&doi=10.1109%2fSecDev.2018.00022&partnerID=40&md5=a1eeca18f0500a988bb01c2413863987","Security practitioners use the attack surface of software systems to prioritize areas of systems to test and analyze. To date, approaches for predicting which code artifacts are vulnerable have utilized a binary classification of code as vulnerable or not vulnerable. To better understand the strengths and weaknesses of vulnerability prediction approaches, vulnerability datasets with classification and severity data are needed. The goal of this paper is to help researchers and practitioners make security effort prioritization decisions by evaluating which classifications and severities of vulnerabilities are on an attack surface approximated using crash dump stack traces. In this work, we use crash dump stack traces to approximate the attack surface of Mozilla Firefox. We then generate a dataset of 271 vulnerable files in Firefox, classified using the Common Weakness Enumeration (CWE) system. We use these files as an oracle for the evaluation of the attack surface generated using crash data. In the Firefox vulnerability dataset, 14 different classifications of vulnerabilities appeared at least once. In our study, 85.3% of vulnerable files were on the attack surface generated using crash data. We found no difference between the severity of vulnerabilities found on the attack surface generated using crash data and vulnerabilities not occurring on the attack surface. Additionally, we discuss lessons learned during the development of this vulnerability dataset. © 2018 IEEE.","Classification; Security; Severity; Vulnerabilities","Classification (of information); Software testing; Binary classification; Mozilla firefox; Prioritization; Security; Security practitioners; Severity; Software systems; Vulnerabilities; Security of data","Conference paper","Final","","Scopus","2-s2.0-85059869779"
"Anwar A.; Khormali A.; Mohaisen A.","Anwar, Afsah (57202738709); Khormali, Aminollah (55369711900); Mohaisen, Aziz (14027298300)","57202738709; 55369711900; 14027298300","POSTER: Understanding the hidden cost of software vulnerabilities: Measurements and predictions","2018","ASIACCS 2018 - Proceedings of the 2018 ACM Asia Conference on Computer and Communications Security","","","","793","795","2","","10.1145/3196494.3201580","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049155661&doi=10.1145%2f3196494.3201580&partnerID=40&md5=238d79883b70c9af9f608f1158d56ba1","In this work, we study the hidden cost of software vulnerabilities reported in the National Vulnerability Database (NVD) through stock price analysis. We perform a high-fidelity data augmentation to ensure data reliability for estimating vulnerability disclosure dates as a baseline for assessing software vulnerabilities' implication. We further build a model for stock price prediction using the NARX Neural Network model to estimate the effect of vulnerability disclosure on the stock price. Compared to prior work, which relies on linear regression models, our approach is shown to provide better accuracy. Our analysis shows that the effect of vulnerabilities on vendors varies, and greatly depends on the specific industry. © 2018 Association for Computing Machinery.","NVD; Prediction; Vulnerability Economics","Costs; Financial markets; Forecasting; Network security; Regression analysis; Software reliability; Data augmentation; Data reliability; Linear regression models; NARX neural network; National vulnerability database; Software vulnerabilities; Stock price prediction; Vulnerability disclosure; Cost benefit analysis","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85049155661"
"Bhuse V.; Hekhuis K.","Bhuse, Vijay (16634696800); Hekhuis, Kyle (57203408725)","16634696800; 57203408725","Support for secure code execution in server operating systems","2018","Proceedings of the 13th International Conference on Cyber Warfare and Security, ICCWS 2018","2018-March","","","21","30","9","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051628732&partnerID=40&md5=977b926eb54b94a015a9c9be7d3656cf","Servers are critical part of the cyber infrastructure because they not only serve clients but also store sensitive data about users, governments and organizations. With ongoing war taking place in cyber space against governments, businesses, and average citizens, it is extremely important to make sure that our servers are secure. There are many steps that can be taken to ensure that servers are secure. One such step is ensuring that the server Operating System (OS) provides a secure and trusted environment for applications to run in. We investigate the correlation between security features and vulnerabilities of OSs such as OpenBSD, FreeBSD, NetBSD, Red Hat Enterprise Linux, Debian, Ubuntu, Solaris, and Windows Server 2012 (WS2012). We looked at the most important security features such as (1) privilege management for processes, (2) memory protection for applications, and (3) the extent of code auditing done by OS developers. Privilege management isolates processes and controls access to resources. For memory protection, we looked at executable space protection, defeating randomness prediction, and stack protection. We also looked at the policies for code auditing by each OS’s supporting organization as well as the ease and effectiveness of package management systems so that users can install patches and updates quickly. The number and severity of the vulnerabilities for each OS is taken into consideration for a fair comparison among OSs. Taking everything into account, OpenBSD was found to be the best contender for a secure OS. However, features alone do not make an OS secure. It ultimately depends on how the administrator hardens the server and how the application developers utilize underlying security features provided by an OS. Our results will help network administrators and software developers choose the most secure server OS to develop and host applications that use underlying security features provided by an OS. © 2018 Academic Conferences and Publishing International Limited. All Rights Reserved.","Code auditing; Memory protection; Privilege management; Secure code; Server operating system; Vulnerability protection","Application programs; Codes (symbols); Computer crime; Linux; Network security; Security systems; Code auditing; Memory protection; Privilege management; Secure codes; Vulnerability protection; Windows operating system","Conference paper","Final","","Scopus","2-s2.0-85051628732"
"Oreski D.; Androcec D.","Oreski, Dijana (24829591800); Androcec, Darko (55150699400)","24829591800; 55150699400","Hybrid Data Mining Approaches for Intrusion Detection in the Internet of Things","2018","Proceedings of International Conference on Smart Systems and Technologies 2018, SST 2018","","","8564573","221","226","5","","10.1109/SST.2018.8564573","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060469376&doi=10.1109%2fSST.2018.8564573&partnerID=40&md5=ed7e3a399a0086862fd11dabe6cc6c41","Internet of things devices and services are often not designed with security in mind. For this reason, malicious users can create botnets and other malicious software targeting things' vulnerabilities. In this work, we have tested various data mining techniques and proposed one that gives representing intrusion detection results with small percentage of false positives. Development of a successful prediction model largely depends on data preprocessing phase. Feature reduction implemented as feature extraction or feature selection is main step of preprocessing phase. This paper compares the applications of principal component analysis as feature extraction method and Relief, Information Gain, Gini Index and SfFS as feature selection methods to reduce features for decision tree classification. By examining NSL-KDD data set, the experiment shows that decision trees by feature selection using SfFS can perform significantly better than other approaches. © 2018 IEEE.","data mining; feature selection; Internet of things; intrusion detection; security","Classification (of information); Decision trees; Extraction; Feature extraction; Internet of things; Intrusion detection; Principal component analysis; Trees (mathematics); Data preprocessing; Decision tree classification; Feature extraction methods; Feature reduction; Feature selection methods; Hybrid data mining approach; Preprocessing phase; security; Data mining","Conference paper","Final","","Scopus","2-s2.0-85060469376"
"Tavabi N.; Goyal P.; Almukaynizi M.; Shakarian P.; Lerman K.","Tavabi, Nazgol (57192665367); Goyal, Palash (57191745928); Almukaynizi, Mohammed (57201734872); Shakarian, Paulo (36635055900); Lerman, Kristina (6603963324)","57192665367; 57191745928; 57201734872; 36635055900; 6603963324","DarkEmbed: Exploit prediction with neural language models","2018","Proceedings of the 30th Innovative Applications of Artificial Intelligence Conference, IAAI 2018","","","","7849","7854","5","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102203426&partnerID=40&md5=39ce20a7fe5fc9b52f7e84443d9b78dd","Software vulnerabilities can expose computer systems to attacks by malicious actors. With the number of vulnerabilities discovered in the recent years surging, creating timely patches for every vulnerability is not always feasible. At the same time, not every vulnerability will be exploited by attackers; hence, prioritizing vulnerabilities by assessing the likelihood they will be exploited has become an important research problem. Recent works used machine learning techniques to predict exploited vulnerabilities by analyzing discussions about vulnerabilities on social media. These methods relied on traditional text processing techniques, which represent statistical features of words, but fail to capture their context. To address this challenge, we propose DarkEmbed, a neural language modeling approach that learns low dimensional distributed representations, i.e., embeddings, of darkweb/deepweb discussions to predict whether vulnerabilities will be exploited. By capturing linguistic regularities of human language, such as syntactic, semantic similarity and logic analogy, the learned embeddings are better able to classify discussions about exploited vulnerabilities than traditional text analysis methods. Evaluations demonstrate the efficacy of learned embeddings on both structured text (such as security blog posts) and unstructured text (darkweb/deepweb posts). DarkEmbed outperforms state-of-the-art approaches on the exploit prediction task with an F1-score of 0.74. © 2018 Proceedings of the 30th Innovative Applications of Artificial Intelligence Conference, IAAI 2018. All rights reserved.","","Artificial intelligence; Computational linguistics; Embeddings; Forecasting; Modeling languages; Natural language processing systems; Network security; Semantics; Text processing; Distributed representation; Machine learning techniques; Semantic similarity; Software vulnerabilities; State-of-the-art approach; Statistical features; Text-analysis methods; Unstructured texts; Learning systems","Conference paper","Final","","Scopus","2-s2.0-85102203426"
"Sultana K.Z.; Chong T.-Y.","Sultana, Kazi Zakia (23494078600); Chong, Tai-Yin (57215345965)","23494078600; 57215345965","A proposed approach to build an automated software security assessment framework using mined patterns and metrics","2019","Proceedings - 22nd IEEE International Conference on Computational Science and Engineering and 17th IEEE International Conference on Embedded and Ubiquitous Computing, CSE/EUC 2019","","","8919532","176","181","5","","10.1109/CSE/EUC.2019.00042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077018620&doi=10.1109%2fCSE%2fEUC.2019.00042&partnerID=40&md5=f37fe679e53e370787a989bffe10e226","Software security is a major concern of the developers who intend to deliver a reliable software. Although there is research that focuses on vulnerability prediction and discovery, there is still a need for building security-specific metrics to measure software security and vulnerability-proneness quantitatively. The existing methods are either based on software metrics (defined on the physical characteristics of code; e.g. complexity or lines of code) which are not security-specific or some generic patterns known as nano-patterns (Java method-level traceable patterns that characterize a Java method or function). Other methods predict vulnerabilities using text mining approaches or graph algorithms which perform poorly in cross-project validation and fail to be a generalized prediction model for any system. In this paper, we envision to construct an automated framework that will assist developers to assess the security level of their code and guide them towards developing secure code. To accomplish this goal, we aim to refine and redefine the existing nano-patterns and software metrics to make them more security-centric so that they can be used for measuring the software security level of a source code (either file or function) with higher accuracy. In this paper, we present our visionary approach through a series of three consecutive studies where we (1) will study the challenges of the current software metrics and nano-patterns in vulnerability prediction, (2) will redefine and characterize the nano-patterns and software metrics so that they can capture security-specific properties of code and measure the security level quantitatively, and finally (3) will implement an automated framework for the developers to automatically extract the values of all the patterns and metrics for the given code segment and then flag the estimated security level as a feedback based on our research results. We accomplished some preliminary experiments and presented the results which indicate that our vision can be practically implemented and will have valuable implications in the community of software security. © 2019 IEEE.","Machine learning; Metrics; Patterns; Software Security; Software vulnerability","Automation; Computer software; Forecasting; Learning systems; Ubiquitous computing; Building security; Generic patterns; Metrics; Patterns; Physical characteristics; Software security; Software vulnerabilities; Specific properties; Java programming language","Conference paper","Final","","Scopus","2-s2.0-85077018620"
"Kudjo P.K.; Chen J.; Mensah S.; Amankwah R.","Kudjo, Patrick Kwaku (57195678643); Chen, Jinfu (56485257600); Mensah, Solomon (57191254462); Amankwah, Richard (57205487973)","57195678643; 56485257600; 57191254462; 57205487973","Predicting vulnerable software components via bellwethers","2019","Communications in Computer and Information Science","960","","","389","407","18","","10.1007/978-981-13-5913-2_24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060212333&doi=10.1007%2f978-981-13-5913-2_24&partnerID=40&md5=9d68dfe503ebe0dfdcc2600d7b769c9b","Software vulnerabilities are weakness, flaws or errors introduced during the life cycle of a software system. Although, previous studies have demonstrated the practical significance of using software metrics to predict vulnerable software components, empirical evidence shows that these metrics are plagued with issues pertaining to their effectiveness and robustness. This paper investigates the feasibility of using Bellwethers (i.e., exemplary data) for predicting and classifying software vulnerabilities. We introduced a Bellwether method using the following operators, PARTITION, SAMPLE + TRAIN and APPLY. The Bellwethers sampled by the three operators are used to train a learner (i.e., deep neural networks) with the aim of predicting essential or non-essential vulnerabilities. We evaluate the proposed Bellwether method using vulnerability reports extracted from three popular web browsers offered by CVE. Again, the mean absolute error (MAE), Welch’s t-test and Cliff’s δ effect size are used to further evaluate the prediction performance and practical statistical significant difference between the Bellwethers and the growing portfolio. We found that there exist subsets of vulnerability records (Bellwethers) in the studied datasets that can yield improved accuracy for software vulnerability prediction. The result shows that recall and precision measures from the text mining process were in a range of 73.9%–85.3% and 67.9%–81.8% respectively across the three studied datasets. The findings further show that the use of the Bellwethers for predictive modelling is a promising research direction for assisting software engineers and practitioners when seeking to predict instances of vulnerability records that demand much attention prior to software release. © 2019, Springer Nature Singapore Pte Ltd.","Bellwethers; Growing portfolio; Software metrics; Software vulnerability; Web browsers","Data mining; Deep neural networks; Forecasting; Life cycle; Trusted computing; Bellwethers; Mean absolute error; Prediction performance; Predictive modelling; Recall and precision; Software component; Software metrics; Software vulnerabilities; Web browsers","Conference paper","Final","","Scopus","2-s2.0-85060212333"
"Vijayan A.; Kiamehr S.; Ebrahimi M.; Chakrabarty K.; Tahoori M.B.","Vijayan, Arunkumar (56875301500); Kiamehr, Saman (36470313100); Ebrahimi, Mojtaba (38561212000); Chakrabarty, Krishnendu (57203198425); Tahoori, Mehdi B. (6603381884)","56875301500; 36470313100; 38561212000; 57203198425; 6603381884","Online soft-error vulnerability estimation for memory arrays and logic cores","2018","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","37","2","","499","511","12","","10.1109/TCAD.2017.2706558","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040967773&doi=10.1109%2fTCAD.2017.2706558&partnerID=40&md5=076b546cc41f63cbd2324074730a6346","Radiation-induced soft errors are a major reliability concern in circuits fabricated at advanced technology nodes. Online soft-error vulnerability estimation offers the flexibility of exploiting dynamic fault-tolerant mechanisms for cost-effective reliability enhancement. We propose a generic run-time method with low area and power overhead to predict the soft-error vulnerability of on-chip memory arrays as well as logic cores. The vulnerability prediction is based on signal probabilities (SPs) of a small set of flip-flops, chosen at design time, by studying the correlation between the soft-error vulnerability and the flip-flop SPs for representative workloads. We exploit machine learning to develop a predictive model that can be deployed in the system in software form. Simulation results on two processor designs show that the proposed technique can accurately estimate the soft-error vulnerability of on-chip logic core, such as sequential pipeline logic and functional units as well as memory arrays that constitute the instruction cache, the data cache, and the register file. © 2017 IEEE.","Architectural vulnerability factor; gradient boosting; machine learning; signal probabilities; soft error; static aging; support vector machines","Adaptive boosting; Artificial intelligence; Computer circuits; Cost effectiveness; Error correction; Errors; Flip flop circuits; Learning systems; Memory architecture; Microprocessor chips; Radiation hardening; Support vector machines; Architectural vulnerability factor; Gradient boosting; Signal probability; Soft error; Static aging; Cache memory","Article","Final","","Scopus","2-s2.0-85040967773"
"Siavvas M.; Jankovic M.; Kehagias D.; Tzovaras D.","Siavvas, Miltiadis (57194500913); Jankovic, Marija (56800124800); Kehagias, Dionysios (7003972544); Tzovaras, Dimitrios (13105681700)","57194500913; 56800124800; 7003972544; 13105681700","Is Popularity an Indicator of Software Security?","2018","9th International Conference on Intelligent Systems 2018: Theory, Research and Innovation in Applications, IS 2018 - Proceedings","","","8710484","692","697","5","","10.1109/IS.2018.8710484","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065985778&doi=10.1109%2fIS.2018.8710484&partnerID=40&md5=578f9eca4b3052752232b030c9c54b25","Although numerous research attempts can be found in the related literature focusing on the ability of software-related factors (e.g. software metrics) to indicate the existence of vulnerabilities in software applications, none of them have demonstrated perfect results. In addition, none of the existing studies have focused on the popularity of software products, which is an important characteristic of open-source software applications and libraries. To this end, in this paper, the ability of popularity (i.e. utilization) to indicate the existence of vulnerabilities and, in turn, to highlight the internal security level of software products is investigated. For this purpose, a relatively large software repository based on well-known libraries retrieved from the Maven Repository was constructed and its security was analyzed using a widely-used open-source static code analyzer. Correlation analysis was employed in order to examine whether a statistically significant correlation exists between the security and popularity of the selected software products. The preliminary results of the analysis suggest that popularity may not constitute a reliable indicator of the security level of software products. To the best of our knowledge, this is the first study that examines the relationship between the popularity of software products and their security level. © 2018 IEEE.","Popularity; Software security; Static analysis; Vulnerability assessment; Vulnerability prediction","Application programs; Intelligent systems; Libraries; Open source software; Open systems; Correlation analysis; Internal security; Popularity; Software applications; Software products; Software repositories; Software security; Vulnerability assessments; Static analysis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85065985778"
"Campos J.R.; Vieira M.; Costa E.","Campos, Joao R. (57197560698); Vieira, Marco (7202140748); Costa, Ernesto (7402527365)","57197560698; 7202140748; 7402527365","Propheticus: Machine Learning Framework for the Development of Predictive Models for Reliable and Secure Software","2019","Proceedings - International Symposium on Software Reliability Engineering, ISSRE","2019-October","","8987543","173","182","9","","10.1109/ISSRE.2019.00026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077189546&doi=10.1109%2fISSRE.2019.00026&partnerID=40&md5=b9f879aa5a5fc4c4e7df99334f5ed664","The growing complexity of software calls for innovative solutions that support the deployment of reliable and secure software. Machine Learning (ML) has shown its applicability to various complex problems and is frequently used in the dependability domain, both for supporting systems design and verification activities. However, using ML is complex and highly dependent on the problem in hand, increasing the probability of mistakes that compromise the results. In this paper, we introduce Propheticus, a ML framework that can be used to create predictive models for reliable and secure software systems. Propheticus attempts to abstract the complexity of ML whilst being easy to use and accommodating the needs of the users. To demonstrate its use, we present two case studies (vulnerability prediction and online failure prediction) that show how it can considerably ease and expedite a thorough ML workflow. © 2019 IEEE.","Failure Prediction; Machine Learning; Reliability; Security; Vulnerability Prediction","Forecasting; Learning systems; Machine learning; Reliability; Complex problems; Failure prediction; Innovative solutions; Predictive models; Secure software; Security; Supporting systems; Verification activities; Software reliability","Conference paper","Final","","Scopus","2-s2.0-85077189546"
"Rizwan M.; Nadeem A.; Sindhu M.A.","Rizwan, Muhammad (57538534100); Nadeem, Aamer (21935004800); Sindhu, Mudassar Azam (36452253600)","57538534100; 21935004800; 36452253600","Theoretical Evaluation of Software Coupling Metrics","2020","Proceedings of 2020 17th International Bhurban Conference on Applied Sciences and Technology, IBCAST 2020","","","9044548","413","421","8","","10.1109/IBCAST47879.2020.9044548","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085526303&doi=10.1109%2fIBCAST47879.2020.9044548&partnerID=40&md5=5898e3dcb59c6b1f2212ca3e85d098c2","Software module coupling is an important design parameter that can be used in various studies including fault prediction, impact analysis, re-modularization assessment, software vulnerabilities assessment, etc. However, two couplings can vary in important coupling factors' coverage. This paper aims to evaluate the coupling metrics to their coverage of important coupling factors'. We perform a thorough survey of coupling metrics, that ends up with the collection of numerous coupling metrics. After that, we evaluate these metrics by their coverage of important coupling factors. The mapping of coupling metrics with the coupling factors shows that the coupling levels been considered by many metrics. Yet, the difference between these levels has been ignored by most of the metrics. Moreover, the broadness, hiddenness, and rigidness of data flow are ignored by most coupling metrics. Apart from that, the combined effect of these aspects is ignored by all the metrics. © 2020 IEEE.","Coupling levels; Software coupling; Software fault prediction","Modular construction; Coupling factor; Design parameters; Fault prediction; Modularizations; Software coupling; Software modules; Software vulnerabilities; Theoretical evaluation; Object oriented programming","Conference paper","Final","","Scopus","2-s2.0-85085526303"
"Bulut F.G.; Altunel H.; Tosun A.","Bulut, Fatma Gul (57212208204); Altunel, Haluk (57502476300); Tosun, Ayse (26649652100)","57212208204; 57502476300; 26649652100","Predicting Software Vulnerabilities Using Topic Modeling with Issues","2019","UBMK 2019 - Proceedings, 4th International Conference on Computer Science and Engineering","","","8907170","739","744","5","","10.1109/UBMK.2019.8907170","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076201669&doi=10.1109%2fUBMK.2019.8907170&partnerID=40&md5=9be7c160a4744f41e0969a9f624e60b0","The existence of software vulnerabilities is an indicator of the reliability and safety of software products. Software vulnerabilities can be predicted using metrics derived from developers, organization, code and textual data. In this work, we aim to predict the software vulnerabilities using issue records in two different datasets. The first dataset consists of six-months of issue records collected in a corporate, whereas the second dataset consists of Wireshark project bug records from 2017 to 2018. Prediction models were established using six different machine learning for which textual descriptions of issue records were converted into topic models. A regression model was established for the corporate company in which textual description of issue records were used as the input, and the number of vulnerabilities were used as the output of the model. A classification model was established for Wireshark dataset in which textual descriptions of bug records were used as input of the model, and the class of vulnerable-prone or not is used as the output. The best regression model results are 0.23, 0.30, 0.44 MdMRE values, respectively. The best classification model result is 74% recall score. © 2019 IEEE.","bug report; issue record; software vulnerability prediction; textual description; topic modeling","Classification (of information); Forecasting; Regression analysis; Bug reports; issue record; Software vulnerabilities; Textual description; Topic Modeling; Software reliability","Conference paper","Final","","Scopus","2-s2.0-85076201669"
"Wang Y.; Wu Z.; Wei Q.; Wang Q.","Wang, Yunchao (57204437152); Wu, Zehui (56342733200); Wei, Qiang (55553728348); Wang, Qingxian (55913649200)","57204437152; 56342733200; 55553728348; 55913649200","NeuFuzz: Efficient Fuzzing with Deep Neural Network","2019","IEEE Access","7","","8672949","36340","36352","12","","10.1109/ACCESS.2019.2903291","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063928720&doi=10.1109%2fACCESS.2019.2903291&partnerID=40&md5=5765ab67d85c47add90cdfc74acb5074","Coverage-guided graybox fuzzing is one of the most popular and effective techniques for discovering vulnerabilities due to its nature of high speed and scalability. However, the existing techniques generally focus on code coverage but not on vulnerable code. These techniques aim to cover as many paths as possible rather than to explore paths that are more likely to be vulnerable. When selecting the seeds to test, the existing fuzzers usually treat all seed inputs equally, ignoring the fact that paths exercised by different seed inputs are not equally vulnerable. This results in wasting time testing uninteresting paths rather than vulnerable paths, thus reducing the efficiency of vulnerability detection. In this paper, we present a solution, NeuFuzz, using the deep neural network to guide intelligent seed selection during graybox fuzzing to alleviate the aforementioned limitation. In particular, the deep neural network is used to learn the hidden vulnerability pattern from a large number of vulnerable and clean program paths to train a prediction model to classify whether paths are vulnerable. The fuzzer then prioritizes seed inputs that are capable of covering the likely to be vulnerable paths and assigns more mutation energy (i.e., the number of inputs to be generated) to these seeds. We implemented a prototype of NeuFuzz based on an existing fuzzer PTfuzz and evaluated it on two different test suites: LAVA-M and nine real-world applications. The experimental results showed that NeuFuzz can find more vulnerabilities than the existing fuzzers in less time. We have found 28 new security bugs in these applications, 21 of which have been assigned as CVE IDs. © 2013 IEEE.","deep neural network; Fuzzing; seed selection; software security; vulnerability detection","Network security; Program debugging; Code coverage; Fuzzing; High Speed; Prediction model; Security bugs; Seed selection; Software security; Vulnerability detection; Deep neural networks","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85063928720"
"Khalid M.N.; Farooq H.; Iqbal M.; Alam M.T.; Rasheed K.","Khalid, Muhammad Noman (57202149965); Farooq, Humera (35434708000); Iqbal, Muhammad (58544925000); Alam, Muhammad Talha (57208175444); Rasheed, Kamran (57208175893)","57202149965; 35434708000; 58544925000; 57208175444; 57208175893","Predicting Web Vulnerabilities in Web Applications Based on Machine Learning","2019","Communications in Computer and Information Science","932","","","473","484","11","","10.1007/978-981-13-6052-7_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064052494&doi=10.1007%2f978-981-13-6052-7_41&partnerID=40&md5=389e38dfae0ca3ebd69f806359623730","Building a secure website is time-consuming, expensive and challenging task for web developers. Researchers to identify webpage sinks to address security efforts, as it helps to reduce time and money to secure web application, are introducing different web vulnerabilities prediction models. Some of the well-known web vulnerabilities are SQL Injection, Cross Site Scripting (XSS) and Cross Site Request Forgery (CSRF). Different machine learning methods are being employed by the existing vulnerability prediction models to prevent vulnerable components in web applications. However, majority of these methods cannot challenge all web vulnerabilities. Therefore, this paper proposed a method namely NMPREDICTOR to predict vulnerable files in website for vulnerability prediction as a classification problem by predicting legitimate or vulnerable code. In addition, it is an effort to employ the classification on different classifier of machine learning algorithms to judge elimination of vulnerable components. Numerous experiments have been conducted in our study to evaluate the performance of our proposed model. Through our proposed method, we have builds 6 classifiers on a training set of labeled files represented by their software metrics and text features. Additionally, we builds a Meta classifier, which combines the six underlying classifiers i.e. J48, Naive Bayes and Random forest. NMPREDICTOR is evaluated on datasets of three web applications, which offers 223 superior quality vulnerabilities found in PHPMyAdmin, Moodle and Drupal. Our proposed method shows a clearly has an advantage over results of existing studies in case of Drupal, PhpMyAdmin and Moodle. © 2019, Springer Nature Singapore Pte Ltd.","Machine learning; Text mining; Vulnerable file; Web vulnerabilities","Classification (of information); Data mining; Decision trees; Forecasting; Learning algorithms; Learning systems; Machine components; Quality control; Text processing; Websites; Cross-site scripting; Machine learning methods; Meta-classifiers; Prediction model; Software metrics; Text mining; Vulnerable file; Web vulnerabilities; Machine learning","Conference paper","Final","","Scopus","2-s2.0-85064052494"
"Sultana K.Z.; Williams B.J.; Bhowmik T.","Sultana, Kazi Zakia (23494078600); Williams, Byron J. (24478168500); Bhowmik, Tanmay (57679627000)","23494078600; 24478168500; 57679627000","A study examining relationships between micro patterns and security vulnerabilities","2019","Software Quality Journal","27","1","","5","41","36","","10.1007/s11219-017-9397-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035139496&doi=10.1007%2fs11219-017-9397-z&partnerID=40&md5=ee7a3616e778979d876638b196a2a1dc","Software security is an integral part of software quality and reliability. Software vulnerabilities make the software susceptible to attacks which violates software security. Metric-based software vulnerability prediction is one way to evaluate vulnerabilities beforehand so that developers can take preventative measures against attacks. In this study, we explore the correlation between software vulnerabilities and code-level constructs called micro patterns. These code patterns characterize class-level object-oriented program features. Existing research addressed micro pattern correlation with software defects. We analyzed the correlation between vulnerabilities and micro patterns from different viewpoints and explored whether they are related. We studied the distribution of micro patterns and their associations with vulnerable classes in 42 versions of the Apache Tomcat and three Java web applications. This study shows that certain micro patterns are frequently present in vulnerable classes. We also show that there is a high correlation between certain patterns that coexist in a vulnerable class. © 2017, Springer Science+Business Media, LLC, part of Springer Nature.","Micro patterns; Software quality; Software security; Software vulnerabilities","Computer software selection and evaluation; Security of data; Software reliability; Micro pattern; Object-oriented program; Security vulnerabilities; Software defects; Software Quality; Software security; Software vulnerabilities; WEB application; Object oriented programming","Article","Final","","Scopus","2-s2.0-85035139496"
"Elish M.O.","Elish, Mahmoud O. (55881992600)","55881992600","Comparison of different types of ANNs for identification of vulnerable web components","2019","Advances in Intelligent Systems and Computing","857","","","1042","1055","13","","10.1007/978-3-030-01177-2_76","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057097279&doi=10.1007%2f978-3-030-01177-2_76&partnerID=40&md5=bbafea2635f07970392438a8be5bf796","Effective and efficient software security inspection is crucial since the existence of vulnerabilities represents severe risks to software users. This paper aims to empirically evaluate and compare different types of Artificial Neural Networks (ANNs) in predicting components that are prone to vulnerabilities in web applications. We considered five different types of ANNs, which were trained, optimized, and cross validated using vulnerability datasets from multiple versions of two open source web applications written in PHP. The prediction performance of these models have been evaluated and compared based on accuracy, precision, recall and F-measure. The results indicate that the Multilayer Perceptron (MLP) neural network outperforms the other models, in both datasets, in accuracy and precision. The Cascade Correlation Neural Network (CCNN), however, outperforms the other models in recall and F-measure in one dataset, whereas the Radial Basis Function (RBF) neural network was the best in the other dataset. It has been also observed that variables selection improved some performance measures and degraded the other measures of several models. © Springer Nature Switzerland AG 2019.","Empirical studies; Neural networks; Prediction; Software security; Vulnerability","Forecasting; Intelligent computing; Neural networks; Radial basis function networks; Accuracy and precision; Cascade correlation neural networks; Empirical studies; Multilayer perceptron neural networks; Prediction performance; Radial basis function neural networks; Software security; Vulnerability; Open source software","Conference paper","Final","","Scopus","2-s2.0-85057097279"
"Litchfield A.; Shahzad A.","Litchfield, Alan (55505724700); Shahzad, Abid (57202452383)","55505724700; 57202452383","Vulnerability and risk assessment of XEN hypervisor","2018","Americas Conference on Information Systems 2018: Digital Disruption, AMCIS 2018","","","","","","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054248065&partnerID=40&md5=38b1b69e717a3b9fc09bbc33613b58f0","A vulnerability prediction and risk assessment process for the Xen hypervisor that predicts the number of vulnerabilities and levels of risk a specific software version provides is presented. 89 hypervisor is a key component of virtualisation and is thus a target of attackers. When such critical infrastructure is compromised, then the assets of service consumers are consequently at risk. The benefit of a risk analysis process is that it provides surety for Cloud services consumers (making the Cloud Computing option more attractive) and assists Systems Administrators in decision making about software choices and version upgrades. The process has been tested on three popular open source, infrastructure level software packages. In each case, the level of predictive accuracy is excellent to good. The study combines quantitative and qualitative methods to predict vulnerabilities and determine risk levels. © 2018 Association for Information Systems. All rights reserved.","Threat actors; Threat level identification; User classification; Vulnerability and risk assessment; Xen hypervisor","Decision making; Distributed computer systems; Information systems; Information use; Open source software; Open systems; Risk analysis; Software testing; Assessment process; Hypervisor; Predictive accuracy; Quantitative and qualitative methods; Systems administrator; Threat actors; Threat levels; User classification; Risk assessment","Conference paper","Final","","Scopus","2-s2.0-85054248065"
"Yan G.; Lu J.; Shu Z.; Kucuk Y.","Yan, Guanhua (8951665100); Lu, Junchen (57201945473); Shu, Zhan (57201941977); Kucuk, Yunus (57191258301)","8951665100; 57201945473; 57201941977; 57191258301","ExploitMeter: Combining Fuzzing with Machine Learning for Automated Evaluation of Software Exploitability","2017","Proceedings - 2017 IEEE Symposium on Privacy-Aware Computing, PAC 2017","2017-January","","","164","175","11","","10.1109/PAC.2017.10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045935605&doi=10.1109%2fPAC.2017.10&partnerID=40&md5=b8f33273eed5a5fa4c7f6fa7910a7364","Exploitable software vulnerabilities pose severe threats to its information security and privacy. Although a great amount of efforts have been dedicated to improving software security, research on quantifying software exploitability is still in its infancy. In this work, we propose ExploitMeter, a fuzzing-based framework of quantifying software exploitability that facilitates decision-making for software assurance and cyber insurance. Designed to be dynamic, efficient and rigorous, ExploitMeter integrates machine learning-based prediction and dynamic fuzzing tests in a Bayesian manner. Using 100 Linux applications, we conduct extensive experiments to evaluate the performance of ExploitMeter in a dynamic environment. © 2017 IEEE.","Bayesian learning; fuzzing; machine learning; Software exploitability","Artificial intelligence; Computer operating systems; Decision making; Security of data; Statistical tests; Automated evaluation; Bayesian learning; Cyber insurances; Dynamic environments; fuzzing; Software assurance; Software security; Software vulnerabilities; Learning systems","Conference paper","Final","","Scopus","2-s2.0-85045935605"
"Li Z.; Shao Y.","Li, ZhanJun (57197733968); Shao, Yan (57207838428)","57197733968; 57207838428","A survey of feature selection for vulnerability prediction using feature-based machine learning","2019","ACM International Conference Proceeding Series","Part F148150","","","36","42","6","","10.1145/3318299.3318345","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066463016&doi=10.1145%2f3318299.3318345&partnerID=40&md5=f2f8923485e8a93cddda794cbc3d2155","This paper summarized the basic process of software vulnerability prediction using feature-based machine learning for the first time. In addition to sorting out the related types and basis of vulnerability features definition, the advantages and disadvantages of different methods are compared. Finally, this paper analyzed the difficulties and challenges in this research field, and put forward some suggestions for future work. © 2019 Association for Computing Machinery.","Feature; Machine learning; Software vulnerability prediction","Forecasting; Learning systems; Predictive analytics; Feature; Feature-based; Research fields; Software vulnerabilities; Sorting outs; Machine learning","Conference paper","Final","","Scopus","2-s2.0-85066463016"
"Gong X.; Xing Z.; Li X.; Feng Z.; Han Z.","Gong, Xi (57211628975); Xing, Zhenchang (8347413500); Li, Xiaohong (57022407900); Feng, Zhiyong (56984876600); Han, Zhuobing (55267205100)","57211628975; 8347413500; 57022407900; 56984876600; 55267205100","Joint prediction of multiple vulnerability characteristics through multi-task learning","2019","Proceedings of the IEEE International Conference on Engineering of Complex Computer Systems, ICECCS","2019-November","","8882756","31","40","9","","10.1109/ICECCS.2019.00011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074655280&doi=10.1109%2fICECCS.2019.00011&partnerID=40&md5=d7976c48e126fc79158483e7e2ec8188","Software vulnerabilities seriously affect the security of computing systems and they are continuously disclosed and reported. When documenting software vulnerabilities, characterizing the severity, exploitability and impact of a vulnerability is critical for effective triaging and management of software vulnerabilities. Faced with ever-growing number of new vulnerabilities, we observe a significant lag between the disclosure of a vulnerability and the specification of its characteristics. This lag calls for automated, reliable assessment of vulnerability characteristics to assist security analysts in allocating their limited efforts to potentially most serious vulnerabilities. Existing automated techniques for vulnerability assessment require hand-crafted features and balanced data, and consider each specific characteristic independently at a time. In this paper, we propose a multi-task machine learning approach for the joint prediction of multiple vulnerability characteristics based on the vulnerability descriptions. Our approach gets rid of the requirement of balanced data, and it relies on neural networks that learn to extract features from training data. Using the large-scale vulnerability data in the Common Vulnerabilities and Exposure(CVE) database, we conduct extensive experiments to compare different configurations of neural network feature extractors, study the impact of multi-task learning versus independent-task learning, and investigate the performance of our approach for predicting the characteristics of newly disclosed vulnerabilities and the minimum requirement of historical vulnerability data for training reliable prediction model. © 2019 IEEE.","Vulnerability analysis Multi-task learning Feature representation learning","Learning systems; Network security; Automated techniques; Computing system; Feature representation; Multitask learning; Reliable assessment; Securities analysts; Software vulnerabilities; Vulnerability analyse multi-task learning feature representation learning; Vulnerability analysis; Vulnerability assessments; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85074655280"
"Periyasamy K.; Arirangan S.","Periyasamy, Kola (57195470477); Arirangan, Saranya (57209069142)","57195470477; 57209069142","Prediction of future vulnerability discovery in software applications using vulnerability syntax tree (PFVD-VST)","2019","International Arab Journal of Information Technology","16","2","","288","294","6","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066424691&partnerID=40&md5=482acae4778f93b5fdd8ab83db477835","Software applications are the origin to spread vulnerabilities in systems, networks and other software applications. Vulnerability Discovery Model (VDM) helps to encounter the susceptibilities in the problem domain. But preventing the software applications from known and unknown vulnerabilities is quite difficult and also need large database to store the history of attack information. We proposed a vulnerability prediction scheme named as Prediction of Future Vulnerability Discovery in Software Applications using Vulnerability Syntax Tree (PFVD-VST) which consists of five steps to address the problem of new vulnerability discovery and prediction. First, Classification and Clustering are performed based on the software application name, status, phase, category and attack types. Second, Code Quality is analyzed with the help of code quality measures such as, Cyclomatic Complexity, Functional Point Analysis, Coupling, Cloning between the objects, etc,. Third, Genetic based Binary Code Analyzer (GABCA) is used to convert the source code to binary code and evaluates each bit of the binary code. Fourth, Vulnerability Syntax Tree (VST) is trained with the help of vulnerabilities collected from National Vulnerability Database (NVD). Finally, a combined Naive Bayesian and Decision Tree based prediction algorithm is implemented to predict future vulnerabilities in new software applications. The experimental results of this system depicts that the prediction rate, recall, precision has improved significantly. © 2019, Zarka Private University. All rights reserved.","Binary code analyzer; Classification and clustering; Code quality metrics; Prediction; Vulnerability discovery; Vulnerability syntax tree","","Article","Final","","Scopus","2-s2.0-85066424691"
"Imtiaz S.M.; Bhowmik T.","Imtiaz, Sayem Mohammad (57205018935); Bhowmik, Tanmay (57679627000)","57205018935; 57679627000","Towards data-driven vulnerability prediction for requirements","2018","ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","","","","744","748","4","","10.1145/3236024.3264836","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058270875&doi=10.1145%2f3236024.3264836&partnerID=40&md5=4f1eefb1891194965157ea6d4272ef3d","Due to the abundance of security breaches we continue to see, the software development community is recently paying attention to a more proactive approach towards security. This includes predicting vulnerability before exploitation employing static code analysis and machine learning techniques. Such mechanisms, however, are designed to detect post-implementation vulnerabilities. As the root of a vulnerability can often be traced back to the requirement specification, and vulnerability discovered later in the development life cycle is more expensive to fix, we need additional preventive mechanisms capable of predicting vulnerability at a much earlier stage. In this paper, we propose a novel framework providing an automated support to predict vulnerabilities for a requirement as early as during requirement engineering. We further present a preliminary demonstration of our framework and the promising results we observe clearly indicate the value of this new research idea. © 2018 Association for Computing Machinery.","information retrieval; software requirements; software security; software vulnerability; Traceability; vulnerability prediction","Forecasting; Information retrieval; Learning systems; Life cycle; Development community; Machine learning techniques; Requirement engineering; Requirement specification; Software requirements; Software security; Software vulnerabilities; Traceability; Software design","Conference paper","Final","","Scopus","2-s2.0-85058270875"
"Almukaynizi M.; Nunes E.; Dharaiya K.; Senguttuvan M.; Shakarian J.; Shakarian P.","Almukaynizi, Mohammed (57201734872); Nunes, Eric (57188726507); Dharaiya, Krishna (57201734445); Senguttuvan, Manoj (57201738373); Shakarian, Jana (54414903300); Shakarian, Paulo (36635055900)","57201734872; 57188726507; 57201734445; 57201738373; 54414903300; 36635055900","Proactive identification of exploits in the wild through vulnerability mentions online","2017","2017 IEEE International Conference on Cyber Conflict U.S., CyCon U.S. 2017 - Proceedings","2017-December","","","82","88","6","","10.1109/CYCONUS.2017.8167501","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045989630&doi=10.1109%2fCYCONUS.2017.8167501&partnerID=40&md5=3ece2ec8cbf6097bba6a15ba7d6ac29e","The number of software vulnerabilities discovered and publicly disclosed is increasing every year; however, only a small fraction of them is exploited in real-world attacks. With limitations on time and skilled resources, organizations often look at ways to identify threatened vulnerabilities for patch prioritization. In this paper, we present an exploit prediction model that predicts whether a vulnerability will be exploited. Our proposed model leverages data from a variety of online data sources (white-hat community, vulnerability researchers community, and darkweb/deepweb sites) with vulnerability mentions. Compared to the standard scoring system (CVSS base score), our model outperforms the baseline models with an F1 measure of 0.40 on the minority class (266% improvement over CVSS base score) and also achieves high True Positive Rate at low False Positive Rate (90%, 13%, respectively). The results demonstrate that the model is highly effective as an early predictor of exploits that could appear in the wild. We also present a qualitative and quantitative study regarding the increase in the likelihood of exploitation incurred when a vulnerability is mentioned in each of the data sources we examine. © 2017 IEEE.","adversarial machine learning; darkweb analysis; online vulnerability mentions; vulnerability exploit prediction","darkweb analysis; False positive rates; Online data sources; online vulnerability mentions; Quantitative study; Real-world attack; Software vulnerabilities; True positive rates; Learning systems","Conference paper","Final","","Scopus","2-s2.0-85045989630"
"Neuhaus S.; Zimmermann T.","Neuhaus, Stephan (35303613700); Zimmermann, Thomas (16308551800)","35303613700; 16308551800","The beauty and the beast: Vulnerabilities in red hat's packages","2019","Proceedings of the 2009 USENIX Annual Technical Conference","","","","383","396","13","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077041914&partnerID=40&md5=33d41eca7071e690aa1b923b3476e5c9","In an empirical study of 3241 Red Hat packages, we show that software vulnerabilities correlate with dependencies between packages. With formal concept analysis and statistical hypothesis testing, we identify dependencies that decrease the risk of vulnerabilities (“beauties”) or increase the risk (“beasts”). Using support vector machines on dependency data, our prediction models successfully and consistently catch about two thirds of vulnerable packages (median recall of 0.65). When our models predict a package as vulnerable, it is correct more than eight times out of ten (median precision of 0.83). Our findings help developers to choose new dependencies wisely and make them aware of risky dependencies.","","Risk assessment; Support vector machines; Testing; Empirical studies; Prediction model; Red hats; Software vulnerabilities; Statistical hypothesis testing; Formal concept analysis","Conference paper","Final","","Scopus","2-s2.0-85077041914"
"Shahzad A.; Litchfield A.","Shahzad, Abid (57202452383); Litchfield, Alan (55505724700)","57202452383; 55505724700","A vulnerability prediction and risk assessment process for the open source xen hypervisor","2020","Proceedings of the 24th Pacific Asia Conference on Information Systems: Information Systems (IS) for the Future, PACIS 2020","","","","","","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089129142&partnerID=40&md5=3317ce607a2fa333dbec65a5f7c6a1c2","This paper presents a vulnerability prediction and risk assessment process developed with the open source Xen hypervisor. Xen provides a very large installation base in leading data centres worldwide. The vulnerability prediction process predicts the number of unknown vulnerabilities. The process allows organizations to identify and quantify risks they could face after moving their critical services to cloud virtual infrastructure. Organizations can determine the adequacy of their security controls to eliminate or reduce risks to Xen by considering the security control recommendations provided in this paper. This process will be quite useful for the organizations that are planning to use Xen as their core hypervisor for a private cloud. The process is evaluated by applying it to Apache HTTP and Squid Proxy servers to demonstrate the generalizability and applicability of the process to open source software packages. Design science research is used as the main methodology for this research. © Proceedings of the 24th Pacific Asia Conference on Information Systems: Information Systems (IS) for the Future, PACIS 2020. All rights reserved.","Cloud computing; Design science research; Risk assessment; Vulnerability prediction; Xen hypervisor","Forecasting; Information dissemination; Information systems; Information use; Open source software; Open systems; Assessment process; Critical service; Data centres; Design-science researches; Prediction process; Private clouds; Security controls; Virtual infrastructures; Risk assessment","Conference paper","Final","","Scopus","2-s2.0-85089129142"
"Alhubaiti O.; El-Alfy E.-S.M.","Alhubaiti, Omar (57202742784); El-Alfy, El-Sayed M. (7003943902)","57202742784; 7003943902","Impact of spectre/meltdown kernel patches on crypto-algorithms on windows platforms","2019","2019 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies, 3ICT 2019","","","8910282","","","","","10.1109/3ICT.2019.8910282","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076443886&doi=10.1109%2f3ICT.2019.8910282&partnerID=40&md5=189160bb691af584e7bc2d2b1d654f05","We evaluate performance impact of kernel patches of two recently discovered catastrophic hardware vulnerabilities (Spectre and Meltdown) That can cause significant harm to security and privacy by memory leaks on a wide range of modern processors including Intel, AMD, and ARM processors and also can affect iOS and Android mobile devices. These vulnerabilities were publicly disclosed in conjunction in January 2018. There are multiple variants of exploits of these flaws. The only fix since their discovery barring designing new hardware is software patches. Such patches can negate some computing speedups computing that were gained by out-of-order execution and branch prediction. Since cryptography is the heart of data security, it is more important to study the performance impact of these patches on crypto-algorithms since exploiting these vulnerabilities can disclose secret keys and other confidential information within few minutes. We first explain these vulnerabilities and their exploits, and reviews relevant related work. Then, we report a number of conducted experiments on Windows platform with and without enabling the kernel patches while running various crypto-algorithms. The results are statistically analyzed to test the hypothesis of whether there is a significant impact on performance for various data sizes and types. © 2019 IEEE.","","Mobile security; Branch prediction; Confidential information; Modern processors; Multiple variants; Out-of-order execution; Performance impact; Security and privacy; Software patches; Security of data","Conference paper","Final","","Scopus","2-s2.0-85076443886"
"Johnston R.; Sarkani S.; Mazzuchi T.; Holzer T.; Eveleigh T.","Johnston, Reuben (57201489189); Sarkani, Shahryar (7005731083); Mazzuchi, Thomas (7004222916); Holzer, Thomas (55224121400); Eveleigh, Timothy (6506128557)","57201489189; 7005731083; 7004222916; 55224121400; 6506128557","Bayesian-model averaging using MCMCBayes for web-browser vulnerability discovery","2019","Reliability Engineering and System Safety","183","","","341","359","18","","10.1016/j.ress.2018.11.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057811007&doi=10.1016%2fj.ress.2018.11.030&partnerID=40&md5=ca45338be1df34b81b8d3b40079c26a2","Most software vulnerabilities are preventable, but they continue to be present in software releases. When Blackhats, or malicious researchers, discover vulnerabilities, they often release corresponding exploit software and malware. Therefore, customer confidence could be reduced if vulnerabilities—or discoveries of them—are not prevented, mitigated, or addressed. In addressing this, managers must choose which alternatives will provide maximal impact and could use vulnerability discovery modeling techniques to support their decision-making process. Applications of these techniques have used traditional approaches to analysis and, despite the dearth of data, have not included information from experts. This article takes an alternative approach, applying Bayesian methods to modeling the vulnerability-discovery phenomenon. Relevant data was obtained from security experts in structured workshops and from public databases. The open-source framework, MCMCBayes, was developed to automate performing Bayesian model averaging via power-posteriors. It combines predictions of interval-grouped discoveries by performance-weighting results from six variants of the non-homogeneous Poisson process (NHPP), two regression models, and two growth-curve models. The methodology is applicable to software-makers and persons interested in applications of expert-judgment elicitation or in using Bayesian analysis techniques with phenomena having non-decreasing counts over time. © 2018 Elsevier Ltd","Cooke's method; Gamma process; Growth curve; Poisson process; Regression","Bayesian networks; Decision making; Malware; Open source software; Poisson distribution; Regression analysis; Bayesian model averaging; Gamma process; Growth curves; Non homogeneous poisson process; Poisson process; Regression; Software vulnerabilities; Vulnerability discovery; Application programs","Article","Final","","Scopus","2-s2.0-85057811007"
"Chen H.; Chen B.; Xue Y.; Xie X.; Liu Y.; Li Y.; Wu X.","Chen, Hongxu (57203808372); Chen, Bihuan (35224542900); Xue, Yinxing (35101120400); Xie, Xiaofei (55268560900); Liu, Yang (56911879800); Li, Yuekang (57196001970); Wu, Xiuheng (57204731312)","57203808372; 35224542900; 35101120400; 55268560900; 56911879800; 57196001970; 57204731312","Hawkeye: Towards a desired directed grey-box fuzzer","2018","Proceedings of the ACM Conference on Computer and Communications Security","","","","2095","2108","13","","10.1145/3243734.3243849","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056902568&doi=10.1145%2f3243734.3243849&partnerID=40&md5=eef71d831cc96ec97c3b4876ede03c87","Grey-box fuzzing is a practically effective approach to test real-world programs. However, most existing grey-box fuzzers lack directedness, i.e. the capability of executing towards user-specified target sites in the program. To emphasize existing challenges in directed fuzzing, we propose Hawkeye to feature four desired properties of directed grey-box fuzzers. Owing to a novel static analysis on the program under test and the target sites, Hawkeye precisely collects the information such as the call graph, function and basic block level distances to the targets. During fuzzing, Hawkeye evaluates exercised seeds based on both static information and the execution traces to generate the dynamic metrics, which are then used for seed prioritization, power scheduling and adaptive mutating. These strategies help Hawkeye to achieve better directedness and gravitate towards the target sites. We implemented Hawkeye as a fuzzing framework and evaluated it on various real-world programs under different scenarios. The experimental results showed that Hawkeye can reach the target sites and reproduce the crashes much faster than state-of-the-art grey-box fuzzers such as AFL and AFLGo. Specially, Hawkeye can reduce the time to exposure for certain vulnerabilities from about 3.5 hours to 0.5 hour. By now, Hawkeye has detected more than 41 previously unknown crashes in projects such as Oniguruma, MJS with the target sites provided by vulnerability prediction tools; all these crashes are confirmed and 15 of them have been assigned CVE IDs. © 2018 Association for Computing Machinery.","Fuzz testing; Static analysis","Software testing; Dynamic metrics; Effective approaches; Fuzz Testing; Power Scheduling; Prediction tools; Real world projects; State of the art; Static information; Static analysis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85056902568"
"Wei S.-J.; He T.; Hu C.-Z.; Shan C.","Wei, Sheng-Jun (8952204800); He, Tao (57203271829); Hu, Chang-Zhen (7404569966); Shan, Chun (56102670900)","8952204800; 57203271829; 7404569966; 56102670900","Predicting Software Security Vulnerabilities withComponent Dependency Graphs; [基于组件依赖图的软件安全漏洞预测方法]","2018","Beijing Ligong Daxue Xuebao/Transaction of Beijing Institute of Technology","38","5","","525","530","5","","10.15918/j.tbit1001-0645.2018.05.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052108644&doi=10.15918%2fj.tbit1001-0645.2018.05.014&partnerID=40&md5=aa15c061c9b128c82589f50048c69bcd","Aiming at the prediction of vulnerability, a vulnerability prediction method based on the component dependency graph was proposed. Firstly, the complexity, coupling and cohesion metrics of a software component were defined based on the component dependency graph. Then these metrics were used to establish a machine learning model to predict vulnerabilities in a component. Finally, a crawler tool was designed and implemented to collect all public security vulnerabilities in Mozilla Firefox from version 1.0 to version 43. Based on these data, the prediction model was trained and tested. The results show that the proposed metrics are also effective in vulnerability prediction. © 2018, Editorial Department of Transaction of Beijing Institute of Technology. All right reserved.","Component dependency graph; Machine learning; Software security; Vulnerability prediction","Artificial intelligence; Couplings; Forecasting; Learning systems; Program processors; Cohesion metrics; Dependency graphs; Machine learning models; Mozilla firefox; Prediction methods; Prediction model; Software component; Software security; Security of data","Article","Final","","Scopus","2-s2.0-85052108644"
"Zhao Y.; Li Y.; Yang T.; Xie H.","Zhao, Yuyue (57216908863); Li, Yangyang (55513334700); Yang, Tengfei (57214782311); Xie, Haiyong (8905177400)","57216908863; 55513334700; 57214782311; 8905177400","Suzzer: A Vulnerability-Guided Fuzzer Based on Deep Learning","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12020 LNCS","","","134","153","19","","10.1007/978-3-030-42921-8_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085245977&doi=10.1007%2f978-3-030-42921-8_8&partnerID=40&md5=d42c22249493a440111b04491f83c81f","Fuzzing is a simple and effective way to find software bugs. Most state-of-the-art fuzzers focus on improving code coverage to enhance the possibility of causing crashes. However, a software program oftentimes has only a fairly small portion that contains vulnerabilities, leading coverage-based fuzzers to work poorly most of the time. To address this challenge, we propose Suzzer, a vulnerability-guided fuzzer, to concentrate on testing code blocks that are more likely to contain bugs. Suzzer has a light-weight static analyzer to extract ACFG vector from target programs. In order to determine which code blocks are more vulnerable, Suzzer is equipped with prediction models which get the prior probability of each ACFG vector. The prediction models will guide Suzzer to generate test inputs with higher vulnerability scores, thus improving the efficiency of finding bugs. We evaluate Suzzer using two different datasets: artificial LAVA-M dataset and a set of real-world programs. The results demonstrate that in the best case of short-term fuzzing, Suzzer saved 64.5% of the time consumed to discover vulnerabilities compared to VUzzer. © 2020, Springer Nature Switzerland AG.","Deep learning; Prediction model; Vulnerability-guided fuzzing","Cryptography; Program debugging; Security of data; Code coverage; Prediction model; Prior probability; Real world projects; Software program; State of the art; Static analyzers; Testing codes; Deep learning","Conference paper","Final","","Scopus","2-s2.0-85085245977"
"Yasasin E.; Prester J.; Wagner G.; Schryen G.","Yasasin, Emrah (56342023100); Prester, Julian (57207955832); Wagner, Gerit (57189003821); Schryen, Guido (17136223900)","56342023100; 57207955832; 57189003821; 17136223900","Forecasting IT security vulnerabilities – An empirical analysis","2020","Computers and Security","88","","101610","","","","","10.1016/j.cose.2019.101610","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072575374&doi=10.1016%2fj.cose.2019.101610&partnerID=40&md5=46858a097358255fd410447541faeb50","Today, organizations must deal with a plethora of IT security threats and to ensure smooth and uninterrupted business operations, firms are challenged to predict the volume of IT security vulnerabilities and allocate resources for fixing them. This challenge requires decision makers to assess which system or software packages are prone to vulnerabilities, how many post-release vulnerabilities can be expected to occur during a certain period of time, and what impact exploits might have. Substantial research has been dedicated to techniques that analyze source code and detect security vulnerabilities. However, only limited research has focused on forecasting security vulnerabilities that are detected and reported after the release of software. To address this shortcoming, we apply established methodologies which are capable of forecasting events exhibiting specific time series characteristics of security vulnerabilities, i.e., rareness of occurrence, volatility, non-stationarity, and seasonality. Based on a dataset taken from the National Vulnerability Database (NVD), we use the Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) to measure the forecasting accuracy of single, double, and triple exponential smoothing methodologies, Croston's methodology, ARIMA, and a neural network-based approach. We analyze the impact of the applied forecasting methodology on the prediction accuracy with regard to its robustness along the dimensions of the examined system and software package “operating systems”, “browsers” and “office solutions” and the applied metrics. To the best of our knowledge, this study is the first to analyze the effect of forecasting methodologies and to apply metrics that are suitable in this context. Our results show that the optimal forecasting methodology depends on the software or system package, as some methodologies perform poorly in the context of IT security vulnerabilities, that absolute metrics can cover the actual prediction error precisely, and that the prediction accuracy is robust within the two applied forecasting-error metrics. © 2019 Elsevier Ltd","Competition setup; Forecasting; Prediction; Security vulnerability; Time series","Decision making; Errors; Mean square error; Network security; Security systems; Software packages; Time series; Actual prediction error; Exponential smoothing; Forecasting accuracy; National vulnerability database; Network-based approach; Prediction accuracy; Root mean square errors; Security vulnerabilities; Forecasting","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85072575374"
"Song R.; Song Y.; Dong Q.; Hu A.; Gao S.","Song, Rui (57213544410); Song, Yubo (23393708100); Dong, Qihong (57201903773); Hu, Aiqun (7202699718); Gao, Shang (57191845946)","57213544410; 23393708100; 57201903773; 7202699718; 57191845946","WebLogger: Stealing your personal PINs via mobile web application","2017","2017 9th International Conference on Wireless Communications and Signal Processing, WCSP 2017 - Proceedings","2017-January","","","1","6","5","","10.1109/WCSP.2017.8171036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046395371&doi=10.1109%2fWCSP.2017.8171036&partnerID=40&md5=75d5044dad0b63c2bf307866f8a291c0","In recent years, various sensors have been integrated into smartphones to sense the slight motions of human body. However, security researchers found that these sensors can not only be used in motion detection, but also as side-channel to reveal users' privacy data by inferring keystrokes. What is worse, as defined in W3C specifications, the mobile web applications can get these sensor readings silently without permissions from users. Therefore, when cross-site scripting vulnerabilities are found in a mobile web application, attackers can get users' privacy data remotely via these sensors in theory. However, these attacks are difficult to achieve by the fact that mobile web applications can only get sensor readings with low sampling rate in practical uses. In this paper, we proposed a novel ensemble learning algorithm based on weighted voting to improve the keystroke inferring accuracy in low sensors sampling rate. Based on this novel learning algorithm, a prototype system named WebLogger is developed to demonstrate the possibility of inferring the PIN numbers or passwords entered by mobile phone users from mobile web application silently. The results of experiments show that the prediction accuracy of our learning model can be improved to 70%, which is better than 50% in single machine learning algorithms. © 2017 IEEE.","","Cellular telephone systems; Data privacy; Learning systems; Side channel attack; Signal processing; Software prototyping; Wireless telecommunication systems; Cross site scripting; Ensemble learning algorithm; Mobile web applications; Mobile-phone users; Motion detection; Prediction accuracy; Prototype system; Single- machines; Learning algorithms","Conference paper","Final","","Scopus","2-s2.0-85046395371"
"Biswas B.; Mukhopadhyay A.","Biswas, Baidyanath (7103207977); Mukhopadhyay, Arunabha (7201817059)","7103207977; 7201817059","G-RAM framework for software risk assessment and mitigation strategies in organisations","2018","Journal of Enterprise Information Management","31","2","","276","299","23","","10.1108/JEIM-05-2017-0069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042558953&doi=10.1108%2fJEIM-05-2017-0069&partnerID=40&md5=a64ddf95ab1334959a84e6aeea5a71be","Purpose: Malicious attackers frequently breach information systems by exploiting disclosed software vulnerabilities. Knowledge of these vulnerabilities over time is essential to decide the use of software products by organisations. The purpose of this paper is to propose a novel G-RAM framework for business organisations to assess and mitigate risks arising out of software vulnerabilities. Design/methodology/approach: The G-RAM risk assessment module uses GARCH to model vulnerability growth. Using 16-year data across 1999-2016 from the National Vulnerability Database, the authors estimate the model parameters and validate the prediction accuracy. Next, the G-RAM risk mitigation module designs optimal software portfolio using Markowitz’s mean-variance optimisation for a given IT budget and preference. Findings: Based on an empirical analysis, this study establishes that vulnerability follows a non-linear, time-dependent, heteroskedastic growth pattern. Further, efficient software combinations are proposed that optimise correlated risk. The study also reports the empirical evidence of a shift in efficient frontier of software configurations with time. Research limitations/implications: Existing assumption of independent and identically distributed residuals after vulnerability function fitting is incorrect. This study applies GARCH technique to measure volatility clustering and mean reversal. The risk (or volatility) represented by the instantaneous variance is dependent on the immediately previous one, as well as on the unconditional variance of the entire vulnerability growth process. Practical implications: The volatility-based estimation of vulnerability growth is a risk assessment mechanism. Next, the portfolio analysis acts as a risk mitigation activity. Results from this study can decide patch management cycle needed for each software – individual or group patching. G-RAM also ranks them into a 2×2 risk-return matrix to ensure that the correlated risk is diversified. Finally the paper helps the business firms to decide what to purchase and what to avoid. Originality/value: Contrary to the existing techniques which either analyse with statistical distributions or linear econometric methods, this study establishes that vulnerability growth follows a non-linear, time-dependent, heteroskedastic pattern. The paper also links software risk assessment to IT governance and strategic business objectives. To the authors’ knowledge, this is the first study in IT security to examine and forecast volatility, and further design risk-optimal software portfolios. © 2018, Emerald Publishing Limited.","IT governance; IT risk assessment; IT security; Software vulnerability","","Article","Final","","Scopus","2-s2.0-85042558953"
"Sharma D.; Chandra P.","Sharma, Deepak (57226656644); Chandra, Pravin (7202774432)","57226656644; 7202774432","A comparative analysis of soft computing techniques in software fault prediction model development","2019","International Journal of Information Technology (Singapore)","11","1","","37","46","9","","10.1007/s41870-018-0211-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091887728&doi=10.1007%2fs41870-018-0211-3&partnerID=40&md5=c3e700143c6609efae725ef57646bfdb","In the process of software development, software fault prediction is a useful practice to ensure reliable and high quality software products. It plays a vital role in the process of software quality assurance. A high quality software product contains minimum number of faults and failures. Software fault prediction examines the vulnerability of software product towards faults. In this paper, a comparative analysis of various soft computing approaches in terms of the process of software fault prediction is considered. In addition, an analysis of various pros and cons of soft computing techniques in terms of software fault prediction process is also mentioned. The conclusive results show that the soft computing approach has the propensity to identify faults in the process of software development. © 2018, Bharati Vidyapeeth's Institute of Computer Applications and Management.","Evolutionary computing; Fuzzy logic; Machine learning; Neural network; Soft computing; Software fault prediction; Swarm intelligence","","Article","Final","","Scopus","2-s2.0-85091887728"
"Siano R.; Roca P.; Camata G.; Pelà L.; Sepe V.; Spacone E.; Petracca M.","Siano, Rossella (56419708300); Roca, Pere (7005448369); Camata, Guido (7801425126); Pelà, Luca (26325055100); Sepe, Vincenzo (36883266400); Spacone, Enrico (7004503820); Petracca, Massimo (56997002000)","56419708300; 7005448369; 7801425126; 26325055100; 36883266400; 7004503820; 56997002000","Numerical investigation of non-linear equivalent-frame models for regular masonry walls","2018","Engineering Structures","173","","","512","529","17","","10.1016/j.engstruct.2018.07.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049729889&doi=10.1016%2fj.engstruct.2018.07.006&partnerID=40&md5=b534bf67818fa551e03b932ff259fa9f","The accuracy of the Equivalent Frame Method (EFM) in modelling the seismic non-linear behaviour of unreinforced masonry (URM) buildings is investigated for regular walls (i.e. walls with regular openings’ distribution) with different pier-to-spandrel geometrical relations. The developed EFM is composed of pier and spandrel elements with spread plasticity to simulate the flexural behaviour and lumped plasticity to simulate the shear behaviour. The investigation focuses on checking, by means of comparison with Finite Element Model (FEM) assumed as reference, the applicability of EFM to existing buildings. These structures are often characterized by geometrical schemes difficult to be represented by ideal frames. To point out the role of the geometrical configuration, the numerical results provided by the two modelling approaches are compared for different representative cases of regular walls characterized by pier-spandrel configurations rather typical in existing URM buildings. In addition to the innovative EFM approach, based on a fiber discretized beam element, also a more traditional approach, based on beam elements with lumped plasticity, is included in the comparative study. The two different EFM approaches were implemented in the software Midas GEN © [44], while an open source software was used to implement the FEM (Kratos Multiphysics [59–60]). All the models were used to perform static non-linear analyses under equivalent loading and boundary conditions. The evaluation of EFM and FEM is derived from a comparative simulation of a two-storey URM wall experimentally tested by other researchers. Two alternative approaches are assumed for the definition of piers’ effective heights in the EFM, i.e. the models proposed by Dolce [1] and Augenti [2]. The results demonstrate that remarkable differences may be detected in EFM and FEM predictions of the shear capacity and damage mechanisms as a function of pier-spandrel geometrical configurations. This result highlights the need for a cautious application of EFM to existing URM structures. © 2018 Elsevier Ltd","Equivalent-frame models; Masonry structures; Non-linear static analysis; Seismic vulnerability; URM walls","Damage detection; Masonry construction; Masonry materials; Numerical methods; Open source software; Open systems; Piers; Plasticity; Reinforcement; Seismology; Static analysis; Walls (structural partitions); Equivalent frame; Equivalent-frame model; Finite element modelling (FEM); Frame models; Geometrical configurations; Masonry structures; Non-linear static analysis; Seismic vulnerability; Un-reinforced masonry walls; Unreinforced masonry building; accuracy assessment; damage; finite element method; flexure; masonry; nonlinearity; numerical model; plasticity; seismic response; vulnerability; wall; Geometry","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85049729889"
"Li Y.; Gu C.; Dullien T.; Vinyals O.; Kohli P.","Li, Yujia (55924890500); Gu, Chenjie (57216551134); Dullien, Thomas (44461094500); Vinyals, Oriol (24342311100); Kohli, Pushmeet (14035707300)","55924890500; 57216551134; 44461094500; 24342311100; 14035707300","Graph matching networks for learning the similarity of graph structured objects","2019","36th International Conference on Machine Learning, ICML 2019","2019-June","","","6815","6832","17","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075480559&partnerID=40&md5=3bf002cd25838b4d3201e13c685f7093","This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions. First, we demonstrate how Graph Neural Networks (GNN), which have emerged as an effective model for various supervised prediction problems defined on structured data, can be trained to produce embedding of graphs in vector spaces that enables efficient similarity reasoning. Second, we propose a novel Graph Matching Network model that, given a pair of graphs as input, computes a similarity score between them by jointly reasoning on the pair through a new cross-graph attention-based matching mechanism. We demonstrate the effectiveness of our models on different domains including the challenging problem of control-flow-graph based function similarity search that plays an important role in the detection of vulnerabilities in software systems. The experimental analysis demonstrates that our models arc not only able to exploit structure in the context of similarity learning but they can also outperform domain-specific baseline systems that have been carefully hand-engineered for these problems. © 36th International Conference on Machine Learning, ICML 2019. All rights reserved.","","Data flow analysis; Graphic methods; Machine learning; Supervised learning; Vector spaces; Control flow graphs; Embedding of graphs; Experimental analysis; Function similarity; Graph neural networks; Matching mechanisms; Similarity learning; Similarity reasoning; Flow graphs","Conference paper","Final","","Scopus","2-s2.0-85075480559"
"Feutrill A.; Ranathunga D.; Yarom Y.; Roughan M.","Feutrill, Andrew (57205766303); Ranathunga, Dinesha (56655161800); Yarom, Yuval (25825786400); Roughan, Matthew (6603755029)","57205766303; 56655161800; 25825786400; 6603755029","The Effect of Common Vulnerability Scoring System Metrics on Vulnerability Exploit Delay","2018","Proceedings - 2018 6th International Symposium on Computing and Networking, CANDAR 2018","","","8594738","1","10","9","","10.1109/CANDAR.2018.00009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061512580&doi=10.1109%2fCANDAR.2018.00009&partnerID=40&md5=21a1fb666c0209919d688b5e52e5ccb3","Modern system administrators need to monitor disclosed software vulnerabilities and address applicable vulnerabilities via patching, reconfiguration and other measures. In 2017, over 14,000 new vulnerabilities were disclosed, so, a key question for administrators is which vulnerabilities to prioritise. The Common Vulnerability Scoring System (CVSS) is often used to decide which vulnerabilities pose the greatest risk and hence inform patching policy. A CVSS score is indicative of a vulnerability severity, but it doesn't predict the time to exploit for a vulnerability. A prediction of exploit delay would greatly assist vendors in prioritising their patch releases and system administrators in prioritising the installation of these patches. In this paper, we study the effect of CVSS metrics on the time until a proof of concept exploit is developed. We use the National Vulnerability Database (NVD) and the Exploit Database, which represent two of the largest listings of vulnerabilities and exploit data, to show how CVSS metrics can provide better insight into exploit delay. We also investigate the time lag associated with populating CVSS metrics and find that the median delay has increased rapidly from a single day prior to 2017 to 19 days in 2018. This is an alarming trend, given the rapid decline in median vulnerability exploit time from 296 days in 2005 to six days in 2018. © 2018 IEEE.","cvss metrics; exploit delay; vulnerability exploits","Computer programming; Common vulnerability scoring systems; cvss metrics; exploit delay; National vulnerability database; Proof of concept; Software vulnerabilities; System administrators; vulnerability exploits; Computer science","Conference paper","Final","","Scopus","2-s2.0-85061512580"
"Wang X.; Ma R.; Li B.; Tian D.; Wang X.","Wang, Xiajing (57195495909); Ma, Rui (35409848400); Li, Binbin (57208398680); Tian, Donghai (35304096400); Wang, Xuefei (57208394136)","57195495909; 35409848400; 57208398680; 35304096400; 57208394136","E-WBM: An Effort-Based Vulnerability Discovery Model","2019","IEEE Access","7","","8676014","44276","44292","16","","10.1109/ACCESS.2019.2907977","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064682614&doi=10.1109%2fACCESS.2019.2907977&partnerID=40&md5=e9776e4ab186cb6bc1545cedab464ee2","Vulnerability discovery models (VDMs) have recently been proposed to estimate the cumulative number of vulnerabilities that will be disclosed after software is released. A precise VDM would offer an available quantitative insight to assess software security. Even though VDM has demonstrated its effectiveness in multiple software, it remains limited in accuracy, especially with weak versatility. We propose a novel effort-based VDMs, named E-WBM, to improve critical vulnerability discovery rate algorithm using Weibull probability distribution function towards efficient vulnerability discovery models. E-WBM accurately portrays the trend of software security vulnerabilities disclosure. We evaluate E-WBM on eight popular real-world operating systems and show the feasibility of the proposed model. We further compare E-WBM with a state-of-the-art effort-based model AME and time-based model JW on the above eight operating systems. Our comparison also demonstrates that E-WBM consistently outperforms AME and JW both at reducing the deviations and fitting curve trends. In addition to the model fitting, predictive capabilities of two effort-based models E-WBM and AME are also examined. The results show that the E-WBM model yields a more stable prediction with a significantly less error than AME. © 2013 IEEE.","AME; E-WBM; JW; testing effort; Vulnerability discovery model","Curve fitting; Distribution functions; Cumulative number; Fitting curves; Predictive capabilities; Software security; State of the art; Testing effort; Vulnerability discovery; Weibull probability distribution; Weibull distribution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85064682614"
"Movahedi Y.; Cukier M.; Gashi I.","Movahedi, Yazdan (56582473200); Cukier, Michel (6603803353); Gashi, Ilir (6505943089)","56582473200; 6603803353; 6505943089","Vulnerability prediction capability: A comparison between vulnerability discovery models and neural network models","2019","Computers and Security","87","","101596","","","","","10.1016/j.cose.2019.101596","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071399285&doi=10.1016%2fj.cose.2019.101596&partnerID=40&md5=ccf8dcab43c8962a9180c2e84668de61","In this paper, we introduce an approach for predicting the cumulative number of software vulnerabilities that is in most cases more accurate than vulnerability discovery models (VDMs). Our approach uses a neural network model (NNM) to model the nonlinearities associated with vulnerability disclosure. Nine common VDMs were used to compare their prediction capability with our approach. The different models were applied to vulnerabilities associated with eight well-known software (four operating systems and four web browsers). The models were assessed in terms of prediction accuracy and prediction bias. Out of eight software we analyzed, the NNM outperformed the VDMs in all the cases in terms of prediction accuracy, and provided smaller values of absolute average bias in seven cases. This study shows that NNMs are promising for accurate predictions of software vulnerabilities disclosures. © 2019 Elsevier Ltd","Neural network model; Prediction; Software reliability; Time series; Vulnerability discovery model; Vulnerability discovery process","Forecasting; Software reliability; Time series; Web browsers; Accurate prediction; Cumulative number; Neural network model; Prediction accuracy; Prediction capability; Software vulnerabilities; Vulnerability disclosure; Vulnerability discovery; Network security","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85071399285"
"Jabeen G.; Ping L.","Jabeen, Gul (54791058900); Ping, Luo (55705278900)","54791058900; 55705278900","A unified measurable software trustworthy model based on vulnerability loss speed index","2019","Proceedings - 2019 18th IEEE International Conference on Trust, Security and Privacy in Computing and Communications/13th IEEE International Conference on Big Data Science and Engineering, TrustCom/BigDataSE 2019","","","8887362","18","25","7","","10.1109/TrustCom/BigDataSE.2019.00013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075152935&doi=10.1109%2fTrustCom%2fBigDataSE.2019.00013&partnerID=40&md5=83c835369448122cdc664c179c33e234","As trust becomes increasingly important in the software domain. Due to its complex composite concept, people face great challenges, especially in today's dynamic and constantly changing internet technology. In addition, measuring the software trustworthiness correctly and effectively plays a significant role in gaining users trust in choosing different software. In the context of security, trust is previously measured based on the vulnerability time occurrence to predict the total number of vulnerabilities or their future occurrence time. In this study, we proposed a new unified index called 'loss speed index' that integrates the most important variables of software security such as vulnerability occurrence time, number and severity loss, which are used to evaluate the overall software trust measurement. Based on this new definition, a new model called software trustworthy security growth model (STSGM) has been proposed. This paper also aims at filling the gap by addressing the severity of vulnerabilities and proposed a vulnerability severity prediction model, the results are further evaluated by STSGM to estimate the future loss speed index. Our work has several features such as: (1) It is used to predict the vulnerability severity/type in future, (2) Unlike traditional evaluation methods like expert scoring, our model uses historical data to predict the future loss speed of software, (3) The loss metric value is used to evaluate the risk associated with different software, which has a direct impact on software trustworthiness. Experiments performed on real software vulnerability datasets and its results are analyzed to check the correctness and effectiveness of the proposed model. © 2019 IEEE.","Software security; Trust measurement; Vulnerability Loss speed index; Vulnerability severity/type; Vulnerability time","Behavioral research; Big data; Data privacy; Forecasting; Speed; Software security; Trust measurement; Vulnerability Loss speed index; Vulnerability severity/type; Vulnerability time; Network security","Conference paper","Final","","Scopus","2-s2.0-85075152935"
"Ramos P.; Vargas V.; Baylac M.; Zergainoh N.-E.; Velazco R.","Ramos, Pablo (56286347800); Vargas, Vanessa (56340855500); Baylac, Maud (6701570332); Zergainoh, Nacer-Eddine (6602990024); Velazco, Raoul (7003492172)","56286347800; 56340855500; 6701570332; 6602990024; 7003492172","SEE Error-Rate Evaluation of an Application Implemented in COTS Multicore/Many-Core Processors","2018","IEEE Transactions on Nuclear Science","65","8","8361060","1879","1886","7","","10.1109/TNS.2018.2838526","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047197700&doi=10.1109%2fTNS.2018.2838526&partnerID=40&md5=dfc8b019dd91fb5459100b418d34a701","This paper evaluates the error rate of a memory-bound application implemented in different commercial-off-the-shelf multicore and many-core processors. To achieve this goal, two quantitative experiments are performed: fault-injection campaigns and radiation ground testing. In addition, this paper proposes an approach for predicting the application error rate by combining the results issued from both types of experiments. The usefulness of the approach is illustrated by three case studies implemented in processors having different manufacturing technologies and architectures: 45-nm silicon-on-insulator (SOI) free-scale P2041 quad-core processor, 65-nm CMOS Adapteva Epiphany multicore processor, and 28-nm CMOS Kalray multipurpose processing array-256 many-core processor. The reliability of the processors for avionics is obtained from their experimental error rates extrapolated to avionic altitudes. Reliability curves are plotted for observing the prediction accuracy. A comparison of the failure in time of the selected processors shows that the greater single-event effect vulnerability of CMOS technology compared with the SOI one can be compensated with the implementation of effective error detection and correction. These protection mechanisms allow the use of CMOS devices having huge memory capacity in applications operating in severe radiation environments. © 1963-2012 IEEE.","Accelerated testing; error rate; fault injection; many core; multicore; reliability; single-event effect (SEE); single-event upset (SEU); soft error","Avionics; CMOS integrated circuits; Error analysis; Program processors; Radiation hardening; Reliability; Software testing; Accelerated testing; Aerospace electronics; Error rate; Fault injection; Many core; Multi core; Multi-core processing; Performance evaluations; Registers; Soft error; Multicore programming","Article","Final","","Scopus","2-s2.0-85047197700"
"Heracleous C.; Michael A.","Heracleous, Chryso (57211120713); Michael, Aimilios (7201653912)","57211120713; 7201653912","Assessment of overheating risk and the impact of natural ventilation in educational buildings of Southern Europe under current and future climatic conditions","2018","Energy","165","","","1228","1239","11","","10.1016/j.energy.2018.10.051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056151016&doi=10.1016%2fj.energy.2018.10.051&partnerID=40&md5=5564f3f04fee6a05733ec150420b5389","It has become evident that southern Europe will experience more adverse climate change effects compared to other European regions. The current study aims to investigate the vulnerability of educational buildings in Cyprus, in view of future climatic conditions, by means of a dynamic simulation software. In addition, the influence of natural ventilation on the thermal comfort in the current and future climatic conditions is examined. The research indicates that educational buildings in southern Europe are unable to meet the thermal comfort criteria for more than 70% of the time, which entails a major impact on the environmental, economic and social interaction of people and buildings. The results show that night ventilation is an effective strategy for the reduction of the risk of overheating especially in the TMY; however, this strategy alone is unable to cope with future overheating predictions. Natural ventilation alone can achieve a reduction of hours where operative temperature exceeds the CIBSE maximum limits of 28–35% by 2050s and of 9–11% by 2090s. The evaluation of the resilience of existing educational buildings is useful in understanding the necessity of energy retrofitting measures in view of future climate conditions, contributing to energy efficiency policies and decision-making regarding retrofit interventions. © 2018 Elsevier Ltd","Adaptive comfort; Climate change; Cyprus; Educational buildings; Overheating","Southern Europe; Behavioral research; Computer software; Decision making; Energy efficiency; Retrofitting; Risk assessment; School buildings; Thermal comfort; Ventilation; Adaptive comfort; Cyprus; Educational buildings; Energy efficiency policies; Natural ventilation; Operative temperature; Overheating; Simulation software; building; climate conditions; climate effect; decision making; education; energy policy; environmental assessment; future prospect; risk assessment; software; ventilation; vulnerability; Climate change","Article","Final","","Scopus","2-s2.0-85056151016"
"Peters F.; Tun T.T.; Yu Y.; Nuseibeh B.","Peters, Fayola (36024233800); Tun, Thein Than (25723830600); Yu, Yijun (35236020500); Nuseibeh, Bashar (12644975200)","36024233800; 25723830600; 35236020500; 12644975200","Text Filtering and Ranking for Security Bug Report Prediction","2019","IEEE Transactions on Software Engineering","45","6","8240740","615","631","16","","10.1109/TSE.2017.2787653","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040055397&doi=10.1109%2fTSE.2017.2787653&partnerID=40&md5=b2fbc6e19d7ac30be1f95881d9e59446","Security bug reports can describe security critical vulnerabilities in software products. Bug tracking systems may contain thousands of bug reports, where relatively few of them are security related. Therefore finding unlabelled security bugs among them can be challenging. To help security engineers identify these reports quickly and accurately, text-based prediction models have been proposed. These can often mislabel security bug reports due to a number of reasons such as class imbalance, where the ratio of non-security to security bug reports is very high. More critically, we have observed that the presence of security related keywords in both security and non-security bug reports can lead to the mislabelling of security bug reports. This paper proposes FARSEC, a framework for filtering and ranking bug reports for reducing the presence of security related keywords. Before building prediction models, our framework identifies and removes non-security bug reports with security related keywords. We demonstrate that FARSEC improves the performance of text-based prediction models for security bug reports in 90 percent of cases. Specifically, we evaluate it with 45,940 bug reports from Chromium and four Apache projects. With our framework, we mitigate the class imbalance issue and reduce the number of mislabelled security bug reports by 38 percent. © 1976-2012 IEEE.","prediction models; ranking; security bug reports; Security cross words; security related keywords; text filtering; transfer learning","Buildings; Computer software; Data structures; Measurement; Program debugging; Bug reports; Computer bugs; Prediction model; Predictive models; ranking; Security; security cross words; security related keywords; Text filtering; Transfer learning; Forecasting","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85040055397"
"Spanos G.; Angelis L.","Spanos, Georgios (55650654300); Angelis, Lefteris (6602528674)","55650654300; 6602528674","A multi-target approach to estimate software vulnerability characteristics and severity scores","2018","Journal of Systems and Software","146","","","152","166","14","","10.1016/j.jss.2018.09.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053813600&doi=10.1016%2fj.jss.2018.09.039&partnerID=40&md5=25c4d80c01b268048ce816968ed79a04","Software vulnerabilities constitute a great risk for the IT community. The specification of the vulnerability characteristics is a crucial procedure, since the characteristics are used as input for a plethora of vulnerability scoring systems. Currently, the determination of the specific characteristics -that represent each vulnerability- is a process that is performed manually by the IT security experts. However, the vulnerability description can be very informative and useful to predict vulnerability characteristics. The primary goal of this research is the enhancement, the acceleration and the support of the manual procedure of the vulnerability characteristic assignment. To achieve this goal, a model, which combines texts analysis and multi-target classification techniques was developed. This model estimates the vulnerability characteristics and subsequently, calculates the vulnerability severity scores from the predicted characteristics. To perform the present research, a dataset that contains 99,091 records from a large -publicly available- vulnerability database was used. The results are encouraging, since they show accuracy in the prediction of the vulnerability characteristics and scores. © 2018 Elsevier Inc.","Information security; Multi-target classification; Software vulnerability; Text analysis","Security of data; Text processing; Model estimates; Multi-targets; Scoring systems; Software vulnerabilities; Software vulnerability characteristics; Text analysis; Vulnerability database; Vulnerability description; Classification (of information)","Article","Final","","Scopus","2-s2.0-85053813600"
"Anton C.A.; Matei O.; Avram A.","Anton, Carmen Ana (57194698626); Matei, Oliviu (36170612500); Avram, Anca (57211990681)","57194698626; 36170612500; 57211990681","Collaborative Data Mining in Agriculture for Prediction of Soil Moisture and Temperature","2019","Advances in Intelligent Systems and Computing","984","","","141","151","10","","10.1007/978-3-030-19807-7_15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065895040&doi=10.1007%2f978-3-030-19807-7_15&partnerID=40&md5=fddda7b729ee3207547ce98f642547a0","Climate change affects agriculture in many ways. Reducing the vulnerability of agricultural systems to climate change and enhancing their capacity to adapt would generate better results with fewer losses. Under the conditions, according to The United Nations Food and Agriculture Organization, the world has to produce 70% more food in 2050 than it produced in 2006, to feed the growing population, it is obvious that any innovative ideas that help agriculture are optimal and needed. An option for increasing efficiency of agriculture is a data mining process that can predict climate conditions and humidity of soil. Determining the optimal time for planting and harvesting could be based on predictions from a data mining process. In this scenario, the application of collaborative data mining techniques, would offer solution for the cases in which one sources do not poses useful data for mining, and the process uses date from another sources correlated. © 2019, Springer Nature Switzerland AG.","Collaborative data mining; Data mining; Machine learning; Prediction in agriculture; Soil moisture prediction","Agriculture; Climate change; Forecasting; Learning systems; Software engineering; Soil moisture; Soil surveys; Agricultural system; Climate condition; Collaborative data minings; Data mining process; Food and agriculture organizations; Innovative ideas; Soil moisture predictions; United Nations; Data mining","Conference paper","Final","","Scopus","2-s2.0-85065895040"
"Yang N.; Wang Y.","Yang, Na (57197870181); Wang, Yun (56802801400)","57197870181; 56802801400","Predicting the silent data corruption vulnerability of instructions in programs","2019","Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS","2019-December","","8975817","862","869","7","","10.1109/ICPADS47876.2019.00127","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078936355&doi=10.1109%2fICPADS47876.2019.00127&partnerID=40&md5=5bf40225263932dfb8e93498fa652f0a","With the decreasing size and voltage level of internal device components, soft errors are increasing and constitute a major threat on electronic devices. Silent data corruption (SDC) is the most dangerous result type of soft errors as there is no indication that an error occurs during one program execution. Identifying the SDC vulnerability of instructions is the premise for applying selective SDC detection techniques to programs. We propose proPVInsiden to predict the SDC vulnerability of instructions with a lower cost of fault injection and a better adaptability for programs and program inputs. Reducing the number of fault injections will impair the performance of the prediction. Partial fault injection is applied to control the downward slope of the performance of the prediction to maximize the reduction of fault injection. Experimental results show that the number of fault injections is reduced by 55% and 45% fault injections are sufficient to predict the relative SDC vulnerability of an instruction with respect to other instructions. The averaged Spearman's rank correlation coefficient is 0.81. proPVInsiden also shows a better applicability for programs and program inputs. © 2019 IEEE.","Fault tolerance; Partial fault injection; Reliability; Silent data corruption; Soft error","Crime; Error correction; Fault tolerance; Fault tolerant computer systems; Forecasting; Radiation hardening; Reliability; Electronic device; Fault injection; Program execution; Silent data corruption (SDC); Silent data corruptions; Soft error; Spearman's rank correlation coefficients; Voltage levels; Software testing","Conference paper","Final","","Scopus","2-s2.0-85078936355"
"Ren J.; Zheng Z.; Liu Q.; Wei Z.; Yan H.","Ren, Jiadong (56012213300); Zheng, Zhangqi (57200407791); Liu, Qian (57209558097); Wei, Zhiyao (57200419077); Yan, Huaizhi (7403396046)","56012213300; 57200407791; 57209558097; 57200419077; 7403396046","A Buffer Overflow Prediction Approach Based on Software Metrics and Machine Learning","2019","Security and Communication Networks","2019","","8391425","","","","","10.1155/2019/8391425","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063229535&doi=10.1155%2f2019%2f8391425&partnerID=40&md5=8bb3661e98608179eab3ef98f4d4904c","Buffer overflow vulnerability is the most common and serious type of vulnerability in software today, as network security issues have become increasingly critical. To alleviate the security threat, many vulnerability mining methods based on static and dynamic analysis have been developed. However, the current analysis methods have problems regarding high computational time, low test efficiency, low accuracy, and low versatility. This paper proposed a software buffer overflow vulnerability prediction method by using software metrics and a decision tree algorithm. First, the software metrics were extracted from the software source code, and data from the dynamic data stream at the functional level was extracted by a data mining method. Second, a model based on a decision tree algorithm was constructed to measure multiple types of buffer overflow vulnerabilities at the functional level. Finally, the experimental results showed that our method ran in less time than SVM, Bayes, adaboost, and random forest algorithms and achieved 82.53% and 87.51% accuracy in two different data sets. The method presented in this paper achieved the effect of accurately predicting software buffer overflow vulnerabilities in C/C++ and Java programs. © 2019 Jiadong Ren et al.","","Adaptive boosting; Buffer storage; C++ (programming language); Computer software; Decision trees; Forecasting; Machine learning; Network security; Computational time; Data mining methods; Decision-tree algorithm; Functional levels; Prediction methods; Random forest algorithm; Software source codes; Static and dynamic analysis; Data mining","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85063229535"
"Kudjo P.K.; Chen J.","Kudjo, Patrick Kwaku (57195678643); Chen, Jinfu (56485257600)","57195678643; 56485257600","A cost-effective strategy for software vulnerability prediction based on bellwether analysis*","2019","ISSTA 2019 - Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis","","","","424","427","3","","10.1145/3293882.3338985","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070571753&doi=10.1145%2f3293882.3338985&partnerID=40&md5=54ef4a04d51ab322dcb1137aa6e7bd3e","Vulnerability Prediction Models (VPMs) aims to identify vulnerable and non-vulnerable components in large software systems. Consequently, VPMs presents three major drawbacks (i) finding an effective method to identify a representative set of features from which to construct an effective model. (ii) the way the features are utilized in the machine learning setup (iii) making an implicit assumption that parameter optimization would not change the outcome of VPMs. To address these limitations, we investigate the significant effect of the Bellwether analysis on VPMs. Specifically, we first develop a Bellwether algorithm to identify and select an exemplary subset of data to be considered as the Bellwether to yield improved prediction accuracy against the growing portfolio benchmark. Next, we build a machine learning approach with different parameter settings to show the improvement of performance of VPMs. The prediction results of the suggested models were assessed in terms of precision, recall, F-measure, and other statistical measures. The preliminary result shows the Bellwether approach outperforms the benchmark technique across the applications studied with F-measure values ranging from 51.1% - 98.5%. © 2019 Copyright held by the owner/author(s). Publication rights licensed to the","","Benchmarking; Cost effectiveness; Forecasting; Machine learning; Cost effective strategies; Large software systems; Machine learning approaches; Parameter optimization; Parameter setting; Prediction accuracy; Software vulnerabilities; Statistical measures; Software testing","Conference paper","Final","","Scopus","2-s2.0-85070571753"
"Akour M.; Al Jarrah I.; Saifan A.A.","Akour, Mohammed (57750775000); Al Jarrah, Iman (57203892510); Saifan, Ahmad A. (35222299300)","57750775000; 57203892510; 35222299300","An efficient approach for test suite reduction using K-means clustering","2018","Journal of Theoretical and Applied Information Technology","96","17","","5930","5939","9","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053412157&partnerID=40&md5=cddedd9c14967b7e6096f4da6afe26fc","Software testing is the primary approach that is used to test and evaluate software under development. The main goal of testing is to find defects before customers find them out. It is very costly. Therefore, reducing the cost of the test is a big challenge. This paper aims at reducing the cost of the test by eliminating the redundant test cases. Our methodology begins with generating the test cases randomly. The Procedural Language/Structured Query Language (PL/SQL) tool is used to generate test cases from the payroll system database functions. The SPSS software package is used to apply the K-means Clustering algorithm to reduce the test cases. The results reveal that the proposed approach significantly reduces the number of test cases from 776 to 240 while keeping the same coverage. © 2005 – ongoing JATIT & LLS.","Code complexity; Fault prediction; Relation; Software complexity; Software vulnerabilities","","Article","Final","","Scopus","2-s2.0-85053412157"
"Zagane M.; Abdi M.K.; Alenezi M.","Zagane, Mohammed (57216807396); Abdi, Mustapha Kamel (56016662800); Alenezi, Mamdouh (55854089000)","57216807396; 56016662800; 55854089000","Deep Learning for Software Vulnerabilities Detection Using Code Metrics","2020","IEEE Access","8","","9069943","74562","74570","8","","10.1109/ACCESS.2020.2988557","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084735981&doi=10.1109%2fACCESS.2020.2988557&partnerID=40&md5=d5de14ad0f6ca7b7ed1960f6b6b0c635","Software vulnerability can cause disastrous consequences for information security. Earlier detection of vulnerabilities minimizes these consequences. Manual detection of vulnerable code is very difficult and very costly in terms of time and budget. Therefore, developers must use automatic vulnerabilities prediction (AVP) tools to minimize costs. Recent works on AVP begin to use techniques of deep learning (DL). All the proposed approaches are based on techniques of feature extraction inspired by previous applications of DL such as automatic language processing. Code metrics were widely used as features to build AVP models based on classic machine learning. This study bridges the gap between deep learning and machine learning features and discusses a deep-learning-based approach to finding vulnerabilities in code using code metrics. Obtained results show that code metrics are very good but not the better to use as features in DL-based AVP. © 2013 IEEE.","Automatic vulnerability prediction; code metrics; deep neural networks","Budget control; Learning algorithms; Learning systems; Security of data; Code metrics; Language processing; Learning-based approach; Software vulnerabilities; Deep learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85084735981"
"Zieja M.; Zieja M.","Zieja, Mariusz (37051566900); Zieja, Mirosław (55576179000)","37051566900; 55576179000","An outline of the method for predicting IT vulnerabilities","2018","MATEC Web of Conferences","210","","02010","","","","","10.1051/matecconf/201821002010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055557567&doi=10.1051%2fmatecconf%2f201821002010&partnerID=40&md5=b80e791fa74249adcfffda205471a05d","Majority of the currently known quantitative models for vulnerability analysis do not allow for a comprehensive vulnerability prediction process for a selected software. The article presents the outline of the method for predicting software vulnerabilities. The presented solution is based on probabilistic properties that allow to reflect external and internal factors affecting software and determining its vulnerabilities. Also, a possible direction of further method development was described, indicating the way of improving the method with elements representing preventive measures, as a result of which it may be possible to limit or eliminate potential software vulnerabilities. © 2018 The Authors, published by EDP Sciences.","","Computer circuits; Forecasting; Internal factors; Method development; Prediction process; Preventive measures; Probabilistic properties; Quantitative models; Software vulnerabilities; Vulnerability analysis; Computer software","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85055557567"
"Anwar A.; Khormali A.; Nyang D.H.; Mohaisen A.","Anwar, Afsah (57202738709); Khormali, Aminollah (55369711900); Nyang, DaeHun (6603353545); Mohaisen, Aziz (14027298300)","57202738709; 55369711900; 6603353545; 14027298300","Understanding the hidden cost of software vulnerabilities: Measurements and predictions","2018","Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST","254","","","377","395","18","","10.1007/978-3-030-01701-9_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059689535&doi=10.1007%2f978-3-030-01701-9_21&partnerID=40&md5=7ee04ed6e3b20400f8f4c42dc2c800cf","Vulnerabilities have a detrimental effect on end-users and enterprises, both direct and indirect; including loss of private data, intellectual property, the competitive edge, performance, etc. Despite the growing software industry and a push towards a digital economy, enterprises are increasingly considering security as an added cost, which makes it necessary for those enterprises to see a tangible incentive in adopting security. Furthermore, despite data breach laws that are in place, prior studies have suggested that only 4% of reported data breach incidents have resulted in litigation in federal courts, showing the limited legal ramifications of security breaches and vulnerabilities. In this paper, we study the hidden cost of software vulnerabilities reported in the National Vulnerability Database (NVD) through stock price analysis. Towards this goal, we perform a high-fidelity data augmentation to ensure data reliability and to estimate vulnerability disclosure dates as a baseline for estimating the implication of software vulnerabilities. We further build a model for stock price prediction using the NARX Neural Network model to estimate the effect of vulnerability disclosure on the stock price. Compared to prior work, which relies on linear regression models, our approach is shown to provide better accuracy. Our analysis also shows that the effect of vulnerabilities on vendors varies, and greatly depends on the specific software industry. Whereas some industries are shown statistically to be affected negatively by the release of software vulnerabilities, even when those vulnerabilities are not broadly covered by the media, some others were not affected at all. © ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018.","National vulnerability database; Prediction; Vulnerability economics","Cost benefit analysis; Costs; Data privacy; Financial markets; Forecasting; Industrial economics; Laws and legislation; Regression analysis; Software reliability; Legal ramifications; Linear regression models; NARX neural network; National vulnerability database; Security breaches; Software vulnerabilities; Stock price prediction; Vulnerability disclosure; Network security","Conference paper","Final","","Scopus","2-s2.0-85059689535"
"Elish M.","Elish, Mahmoud (55881992600)","55881992600","Enhanced prediction of vulnerable Web components using Stochastic Gradient Boosting Trees","2019","International Journal of Web Information Systems","15","2","","201","214","13","","10.1108/IJWIS-05-2018-0041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057013457&doi=10.1108%2fIJWIS-05-2018-0041&partnerID=40&md5=a0ea75480891c6e3ae14dd299975e663","Purpose: Effective and efficient software security inspection is crucial as the existence of vulnerabilities represents severe risks to software users. The purpose of this paper is to empirically evaluate the potential application of Stochastic Gradient Boosting Trees (SGBT) as a novel model for enhanced prediction of vulnerable Web components compared to common, popular and recent machine learning models. Design/methodology/approach: An empirical study was conducted where the SGBT and 16 other prediction models have been trained, optimized and cross validated using vulnerability data sets from multiple versions of two open-source Web applications written in PHP. The prediction performance of these models have been evaluated and compared based on accuracy, precision, recall and F-measure. Findings: The results indicate that the SGBT models offer improved prediction over the other 16 models and thus are more effective and reliable in predicting vulnerable Web components. Originality/value: This paper proposed a novel application of SGBT for enhanced prediction of vulnerable Web components and showed its effectiveness. © 2018, Emerald Publishing Limited.","Internet quality of service; Performance of web applications; Web design metrics","Adaptive boosting; Forecasting; Forestry; Open source software; Quality of service; Stochastic models; Stochastic systems; Design metrics; Design/methodology/approach; Internet quality of services; Machine learning models; Novel applications; Prediction performance; Stochastic gradient boosting; WEB application; Predictive analytics","Article","Final","","Scopus","2-s2.0-85057013457"
"Javed Y.; Alenezi M.; Akour M.; Alzyod A.","Javed, Yasir (56983138200); Alenezi, Mamdouh (55854089000); Akour, Mohammed (57750775000); Alzyod, Ahmad (57203277892)","56983138200; 55854089000; 57750775000; 57203277892","Discovering the relationship between software complexity and software vulnerabilities","2018","Journal of Theoretical and Applied Information Technology","96","14","","4690","4699","9","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051123219&partnerID=40&md5=96887202bcc9ac17ba9eb0ad71c2ed6b","Software vulnerabilities might be exploited badly which might eventually lead to a loss of confidentiality, integrity, and availability which translated into a loss of time and money. Although several studies indicated that complexity in software is the main cause of vulnerabilities, still the argument is poorly designed and maintained. Moreover, some studies have already related complexity to vulnerabilities and found that this cannot be generalized. In this work, we explored that what are the factors that contribute more to make a software vulnerable. Several feature selection techniques were applied to find the contribution of each feature. Five classifiers are used in this study to predict the vulnerable classes. The dataset is collected from twelve Java applications, where these applications are analyzed and based on complexity, code coverage, and security. The studied applications are varying in its characteristics regarding a number of code lines, used classes; application size, etc. The result indicates that complexity in all its components (size, depth of inheritance, etc.) can be utilized in predicting vulnerabilities. © 2005 – on going JATIT & LLS.","Code complexity; Fault prediction; Relation; Software complexity; Software vulnerabilities","","Article","Final","","Scopus","2-s2.0-85051123219"
"Zhang J.; Shu Z.","Zhang, Jian (36109684500); Shu, Zhan (57189897109)","36109684500; 57189897109","Optimal design of isolation devices for mid-rise steel moment frames using performance based methodology","2018","Bulletin of Earthquake Engineering","16","9","","4315","4338","23","","10.1007/s10518-018-0321-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041930952&doi=10.1007%2fs10518-018-0321-0&partnerID=40&md5=b8ca8158369dd468a506b4005b26846b","This paper develops and applies the performance-based analysis and design methodology to assess the seismic vulnerability of mid-rise steel moment frame structures and to optimally design the isolation devices to reduce the direct losses due to earthquake damages. An isolated steel moment frame, originally tested in the 2011 E-Defense blind prediction contest, is selected and modeled in detail. The numerical model and the predicted seismic responses of the structure are validated against the full-scale shaking table test results. Subsequently, the fragility functions are derived for the structure when subject to near-fault ground motions exhibiting distinctive acceleration or velocity pulses and far-field motions with less impulsive characteristics. To quantify the system level damage states of the building, the concept of total loss ratio (TLR) is applied as the performance index to account for the direct loss due to structural, non-structural and isolation components in relation to the total repair cost of the original structure. The TLR considers the failure probability (as defined by fragility functions), the damage percentage and related cost for each damage state. Finally, among various isolation designs, the optimal configuration is derived for cases with the minimum TLR. It is shown that the optimal design can reduce the TLR up to 90% of that of the un-isolated structure and it also outperforms the adopted design in the test program. The study demonstrates a systematic way of achieving the optimal isolation design with considerations of uncertainties in earthquake inputs and the combined structural and non-structural damages. © 2018, Springer Science+Business Media B.V., part of Springer Nature.","Base isolation; Fragility functions; Optimal design; Performance based design; Steel moment frame; Total loss ratio","Earthquakes; Optimal systems; Software testing; Base isolation; Fragility function; Optimal design; Performance based design; Steel moment frames; Total loss; architectural design; earthquake damage; ground motion; instrumentation; methodology; performance assessment; seismic isolation; seismic response; shaking table test; steel structure; vulnerability; Seismic design","Article","Final","","Scopus","2-s2.0-85041930952"
"Xiao H.; Xing Z.; Li X.; Guo H.","Xiao, Hongbo (57212536257); Xing, Zhenchang (8347413500); Li, Xiaohong (57022407900); Guo, Hao (57214151032)","57212536257; 8347413500; 57022407900; 57214151032","Embedding and predicting software security entity relationships: A knowledge graph based approach","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11955 LNCS","","","50","63","13","","10.1007/978-3-030-36718-3_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076987878&doi=10.1007%2f978-3-030-36718-3_5&partnerID=40&md5=7e52922f81f3787ed3458ee04d61cc8b","Software security knowledge involves heterogeneous security concepts (e.g., software weaknesses and attack patterns) and security instances (e.g., the vulnerabilities of a particular software product), which can be regarded as software security entities. Among software security entities, there are many within-type relationships as well as many across-type relationships. Predicting software security entity relationships helps to enrich software security knowledge (e.g., finding missing relationships among existing entities). Unfortunately, software security entities are currently documented in separate databases, such as Common Vulnerabilities and Exposures (CVE), Common Weakness Enumeration (CWE) and Common Attack Pattern Enumeration and Classification (CAPEC). This hyper-document representation cannot support effective reasoning of software entity relationships. In this paper, we propose to consolidate heterogeneous software security concepts and instances from separate databases into a coherent knowledge graph. We develop a knowledge graph embedding method which embeds the symbolic relational and descriptive information of software security entities into a continuous vector space. The resulting entity and relationship embeddings are predictive for software security entity relationships. Based on the Open World Assumption, we conduct extensive experiments to evaluate the effectiveness of our knowledge graph based approach for predicting various within-type and across-type relationships of software security entities. © Springer Nature Switzerland AG 2019.","Knowledge graph embedding; Link prediction; Software security entity relationship","Classification (of information); Embeddings; Forecasting; Graphic methods; Vector spaces; Common vulnerabilities and exposures; Descriptive information; Document Representation; Entity-relationship; Heterogeneous software; Knowledge graphs; Link prediction; Open world assumption; Security of data","Conference paper","Final","","Scopus","2-s2.0-85076987878"
"Chan G.-Y.; Chua F.-F.; Lee C.-S.","Chan, Gaik-Yee (55212751900); Chua, Fang-Fang (24469897000); Lee, Chien-Sing (57205431488)","55212751900; 24469897000; 57205431488","Intrusion detection and prevention of web service attacks for software as a service: Fuzzy association rules vs fuzzy associative patterns","2016","Journal of Intelligent and Fuzzy Systems","31","2","","749","764","15","","10.3233/JIFS-169007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988431898&doi=10.3233%2fJIFS-169007&partnerID=40&md5=7eb3dcf9506323308da493236962dd52","Cloud computing inherits all the systems, networks as well asWeb Services' security vulnerabilities, in particular for software as a service (SaaS), where business applications or services are provided over the Cloud as Web Service (WS). Hence, WS-based applications must be protected against loss of integrity, confidentiality and availability when they are deployed over to the Cloud environment. Many existing IDP systems address only attacks mostly occurring at PaaS and IaaS. In this paper, we present our fuzzy association rule-based (FAR) and fuzzy associative pattern-based (FAP) intrusion detection and prevention (IDP) systems in defending against WS attacks at the SaaS level. Our experimental results have validated the capabilities of these two IDP systems in terms of detection of known attacks and prediction of newvariant attacks with accuracy close to 100%. For each transaction transacted over the Cloud platform, detection, prevention or prediction is carried out in less than five seconds. For load and volume testing on the SaaS where the system is under stress (at a work load of 5000 concurrent users submitting normal, suspicious and malicious transactions over a time interval of 300 seconds), the FAR IDP system provides close to 95% service availability to normal transactions. Future work involves determining more quality attributes besides service availability, such as latency, throughput and accountability for a more trustworthy SaaS. © 2016 - IOS Press and the authors. All rights reserved.","Fuzzy association rule; Intrusion detection; Intrusion prevention; Software as a service; Web service","Application programs; Association rules; Distributed computer systems; Fuzzy rules; Intrusion detection; Load testing; Mercury (metal); Web services; Websites; World Wide Web; Business applications; Fuzzy association rule; Intrusion detection and prevention; Intrusion prevention; Malicious transaction; Security vulnerabilities; Service availability; Web service attacks; Software as a service (SaaS)","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84988431898"
"Li J.; Nie B.","Li, Jiachun (36094557400); Nie, Bingchuan (56830194600)","36094557400; 56830194600","Storm surge prediction: Present status and future challenges","2017","Procedia IUTAM","25","","","3","9","6","","10.1016/j.piutam.2017.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040239612&doi=10.1016%2fj.piutam.2017.09.002&partnerID=40&md5=3ff8e1a8c757bc084b849a704e803677","In the current review, the most pessimistic events of the globe in history are addressed when we present severe impacts caused by storm surges. During previous decades, great progresses in storm surge modeling have been made. As a result, people have developed a number of numerical software such as SPLASH, SLOSH etc. and implemented routine operational forecast by virtue of powerful supercomputers with the help of meteorological satellites and sensors as verification tools. However, storm surge as a killer from the sea is still threatening human being and exerting enormous impacts on human society due to economic growth, population increase and fast urbanization. To mitigate the effects of storm surge hazards, integrated research on disaster risk (IRDR) as an ICSU program is put on agenda. The most challenging issues concerned such as abrupt variation in TC's track and intensity, comprehensive study on the consequences of storm surge and the effects of climate change on risk estimation are emphasized. In addition, it is of paramount importance for coastal developing countries to set up forecast and warning system and reduce vulnerability of affected areas. © 2017 The Author(s).","Extratropical cyclone; IRDR; Risk analysis; SLOSH; SPLASH; Storm surge; Tropical cyclone","Alarm systems; Climate change; Developing countries; Economics; Floods; Forecasting; Population statistics; Risk analysis; Risk assessment; Risk perception; Storms; Supercomputers; Verification; Extratropical cyclones; IRDR; SLOSH; SPLASH; Storm surges; Tropical cyclone; Weather forecasting","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85040239612"
"Majumder R.; Som S.; Gupta R.","Majumder, Rana (37119011100); Som, Subhranil (26422606600); Gupta, Ritu (57898355100)","37119011100; 26422606600; 57898355100","Vulnerability prediction through self-learning model","2017","2017 International Conference on Infocom Technologies and Unmanned Systems: Trends and Future Directions, ICTUS 2017","2018-January","","","400","402","2","","10.1109/ICTUS.2017.8286040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047079014&doi=10.1109%2fICTUS.2017.8286040&partnerID=40&md5=0c71d1c47ccfe0c0a44cb2e21db20f4d","Vulnerability being the buzz word in the modern time is the most important jargon related to software and operating system. Since every now and then, software is developed some loopholes and incompleteness lie in the development phase, so there always remains a vulnerability of abruptness in it which can come into picture anytime. Detecting vulnerability is one thing and predicting its occurrence in the due course of time is another thing. If we get to know the vulnerability of any software in the due course of time then it acts as an active alarm for the developers to again develop sound and improvised software the second time. The proposal talks about the implementation of the idea using the artificial neural network, where different data sets are being given as input for being used for further analysis for successful results. As of now, there are models for studying the vulnerabilities in the software and networks, this paper proposal in addition to the current work, will throw light on the predictability of vulnerabilities over the due course of time. © 2017 IEEE.","Activation Function; Artificial Neural Network; Vulnerability impact","Curricula; Network security; 'current; Activation functions; Data set; Development phasis; Modern time; Self-learning models; Vulnerability impact; Neural networks","Conference paper","Final","","Scopus","2-s2.0-85047079014"
"Morrison P.; Herzig K.; Murphy B.; Williams L.","Morrison, Patrick (55800525800); Herzig, Kim (35078523000); Murphy, Brendan (7402698081); Williams, Laurie (35565101900)","55800525800; 35078523000; 7402698081; 35565101900","Challenges with applying vulnerability prediction models","2015","ACM International Conference Proceeding Series","21-22-April-2015","","2746198","","","","","10.1145/2746194.2746198","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986586478&doi=10.1145%2f2746194.2746198&partnerID=40&md5=f8981ce2e869f5fe4cc59a0f4aed9bf1","Vulnerability prediction models (VPM) are believed to hold promise for providing software engineers guidance on where to prioritize precious verification resources to search for vulnerabilities. However, while Microsoft product teams have adopted defect prediction models, they have not adopted vulnerability prediction models (VPMs). The goal of this research is to measure whether vulnerability prediction models built using standard recommendations perform well enough to provide actionable results for engineering resource allocation. We define 'actionable' in terms of the inspection effort required to evaluate model results. We replicated a VPM for two releases of the Windows Operating System, varying model granularity and statistical learners. We reproduced binary-level prediction precision (∼0.75) and recall (∼0.2). However, binaries often exceed1 million lines of code, too large to practically inspect, and engineers expressed preference for source file level predictions. Our source file level models yield precision below 0.5 and recall below 0.2. We suggest that VPMs must be refined to achieve actionable performance, possibly through security-specific metrics. Copyright 2015 ACM.","Churn; Complexity; Coverage; Dependencies; Metrics; Prediction; Vulnerabilities","Verification; Windows operating system; Churn; Complexity; Coverage; Dependencies; Metrics; Vulnerabilities; Forecasting","Conference paper","Final","","Scopus","2-s2.0-84986586478"
"Abunadi I.; Alenezi M.","Abunadi, Ibrahim (57189636917); Alenezi, Mamdouh (55854089000)","57189636917; 55854089000","An empirical investigation of security vulnerabilities within web applications","2016","Journal of Universal Computer Science","22","4","","537","551","14","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978389596&partnerID=40&md5=3c758310511b82bea24437db47b090c1","Building secure software is challenging, time-consuming, and expensive. Software vulnerability prediction models that identify vulnerable software components are usually used to focus security efforts, with the aim of helping to reduce the time and effort needed to secure software. Existing vulnerability prediction models use process or product metrics and machine learning techniques to identify vulnerable software components. Cross-project vulnerability prediction plays a significant role in appraising the most likely vulnerable software components, specifically for new or inactive projects. Little effort has been spent to deliver clear guidelines on how to choose the training data for project vulnerability prediction. In this work, we present an empirical study aiming at clarifying how useful cross-project prediction techniques are in predicting software vulnerabilities. Our study employs the classification provided by different machine learning techniques to improve the detection of vulnerable components. We have elaborately compared the prediction performance of five well-known classifiers. The study is conducted on a publicly available dataset of several PHP open-source web applications in the context of cross-project vulnerability prediction, which represents one of the main challenges in the vulnerability prediction field. © 2016 J.UCS.","Cross-project vulnerability prediction; Data mining; Software quality; Software security","","Article","Final","","Scopus","2-s2.0-84978389596"
"Kamtuo K.; Soomlek C.","Kamtuo, Krit (57193736227); Soomlek, Chitsutha (36024421200)","57193736227; 36024421200","Machine learning for SQL injection prevention on server-side scripting","2017","20th International Computer Science and Engineering Conference: Smart Ubiquitos Computing and Knowledge, ICSEC 2016","","","7859950","","","","","10.1109/ICSEC.2016.7859950","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016195096&doi=10.1109%2fICSEC.2016.7859950&partnerID=40&md5=6656fb777ba4ca20e6b16b7552cfa6a1","SQL injection is the most common web application vulnerability. The vulnerability can be generated unintentionally by software developer during the development phase. To ensure that all secure coding practices are adopted to prevent the vulnerability. The framework of SQL injection prevention using compiler platform and machine learning is proposed. The machine learning part will be described primarily since it is the core of this framework to support SQL injection prediction by conducting 1,100 datasets of vulnerabilities to train machine learning model. The results indicated that decision tree is the best model in term of processing time, highest efficiency in prediction. © 2016 IEEE.","Machine learning; SQL injection; Web application vulnerability","Artificial intelligence; Decision trees; Web crawler; Development phase; Machine learning models; Processing time; Secure coding; Server sides; Software developer; SQL injection; Web application vulnerability; Learning systems","Conference paper","Final","","Scopus","2-s2.0-85016195096"
"Murakami H.; Hotta K.; Higo Y.; Kusumoto S.","Murakami, Hiroaki (55556618500); Hotta, Keisuke (36668756300); Higo, Yoshiki (7004831134); Kusumoto, Shinji (7102741360)","55556618500; 36668756300; 7004831134; 7102741360","Predicting next changes at the fine-grained level","2014","Proceedings - Asia-Pacific Software Engineering Conference, APSEC","1","","7091300","119","126","7","","10.1109/APSEC.2014.27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951284086&doi=10.1109%2fAPSEC.2014.27&partnerID=40&md5=a13faa748d1e6e309f75894fc5ef3cde","Changing source code is not an easy task. Developers occasionally change source code incorrectly. Such mistakes entail additional cost in having to reedit the source code correctly, and repeated changes themselves can be a vulnerability to software quality. We are conducting research into realizing automated code changing as a countermeasure for human errors. As the first step of this research, we propose a technique to predict the types of program elements deleted and added in a next change to Java methods. This technique is designed to support developers in deciding how to change source code after they have identified a method to be changed. We evaluated predictions using the proposed technique with two thresholds, which are sizes of source code changes. For predictions with the smaller threshold where only a single type of program element was added or deleted, the accuracy of the proposed technique was 74%-85%. However, for the larger threshold, where 5 or fewer types of program elements were added or deleted, the accuracy was 44%-48%. © 2014 IEEE.","Automated Code Evolution; Fine-Grained Change Prediction; Static Code Analysis","Computer programming languages; Computer software; Computer software selection and evaluation; Forecasting; Java programming language; Software engineering; Additional costs; Automated code; Fine-grained changes; Java methods; Program elements; Software Quality; Source code changes; Static code analysis; Codes (symbols)","Conference paper","Final","","Scopus","2-s2.0-84951284086"
"Joh H.C.; Malaiya Y.K.","Joh, Hyun Chul (26648448600); Malaiya, Yashwant K. (35619571700)","26648448600; 35619571700","Modeling skewness in vulnerability discovery","2014","Quality and Reliability Engineering International","30","8","","1445","1459","14","","10.1002/qre.1567","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912019227&doi=10.1002%2fqre.1567&partnerID=40&md5=d7e26f0a138258c697a539e384b05813","A vulnerability discovery model attempts to model the rate at which the vulnerabilities are discovered in a software product. Recent studies have shown that the S-shaped Alhazmi-Malaiya Logistic (AML) vulnerability discovery model often fits better than other models and demonstrates superior prediction capabilities for several major software systems. However, the AML model is based on the logistic distribution, which assumes a symmetrical discovery process with a peak in the center. Hence, it can be expected that when the discovery process does not follow a symmetrical pattern, an asymmetrical distribution based discovery model might perform better. Here, the relationship between performance of S-shaped vulnerability discovery models and the skewness in target vulnerability datasets is examined. To study the possible dependence on the skew, alternative S-shaped models based on the Weibull, Beta, Gamma and Normal distributions are introduced and evaluated. The models are fitted to data from eight major software systems. The applicability of the models is examined using two separate approaches: goodness of fit test to see how well the models track the data, and prediction capability using average error and average bias measures. It is observed that an excellent goodness of fit does not necessarily result in a superior prediction capability. The results show that when the prediction capability is considered, all the right skeweddatasets are represented better with the Gamma distribution-based model. The symmetrical models tend to predict better for left skewed datasets; the AML model is found to be the best among them. © 2013 John Wiley & Sons, Ltd.","Data models; Empirical studies; Security; Skewness; Vulnerability discovery model (VDM)","Computer software; Data structures; Forecasting; Higher order statistics; Normal distribution; Asymmetrical distributions; Empirical studies; Goodness-of-fit test; Logistic distributions; Prediction capability; Security; Skewness; Vulnerability discovery; Weibull distribution","Article","Final","","Scopus","2-s2.0-84912019227"
"Qingfeng D.; Kun S.; Kanglin Y.; Juan Q.","Qingfeng, Du (7202060077); Kun, Shi (57204719262); Kanglin, Yin (57196393177); Juan, Qiu (57196373218)","7202060077; 57204719262; 57196393177; 57196373218","Metrics analysis based on call graph of class methods","2017","Proceedings of 2017 International Conference on Progress in Informatics and Computing, PIC 2017","","","","18","24","6","","10.1109/PIC.2017.8359507","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048174506&doi=10.1109%2fPIC.2017.8359507&partnerID=40&md5=7056e9482f2b8b8bad1f60e55dfd8c26","Evaluating software quality is a significant step in the process of developing software. Object-oriented metrics is an important way for software quality assessment and defect prediction. However, the existing object-oriented metric methods are difficult to reflect the complexity of software and vulnerabilities from the level of classes' method call relationship. A method is proposed in this paper, firstly we construct a call graph according to different classes methods call relationship in the system, and then three new software metrics are defined and analyzed by the call graph. The results of experiment indict that the metrics can reflect the complexity of software systems. These metrics can provide recommendations for evaluating software quality from the call graph. © 2017 IEEE.","Call Graph; Class Method; Software Metrics Analysis","Autonomous agents; Computer software selection and evaluation; Call graphs; Class Method; Defect prediction; Evaluating software; Object oriented metrics; Software metrics; Software quality assessment; Software systems; Object oriented programming","Conference paper","Final","","Scopus","2-s2.0-85048174506"
"Geng J.; Ye D.; Luo P.","Geng, Jinkun (57023035600); Ye, Daren (57022987800); Luo, Ping (55705278900)","57023035600; 57022987800; 55705278900","Predicting severity of software vulnerability based on grey system theory","2015","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9532","","","143","152","9","","10.1007/978-3-319-27161-3_13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952053661&doi=10.1007%2f978-3-319-27161-3_13&partnerID=40&md5=a2a385b763e98e97fca53c120a23da15","Vulnerabilities usually represents the risk level of software, therefore, it is of high value to predict vulnerabilities so as to evaluate the security level of software. Current researches mainly focus on predicting the number of vulnerabilities or the occurrence time of vulnerabilities, however, to our best knowledge, there are no other researches focusing on the prediction of vulnerabilities’ severity, which we think is an important aspect reflecting vulnerabilities and software security. To compensate for this deficiency, we propose a novel method based on grey system theory to predict the severity of vulnerabilities. The experiment is carried on the real data collected from CVE and proves the feasibility of our predicting method. © Springer International Publishing Switzerland 2015.","Grey system theory; Prediction; Severity; Software security; Vulnerability","Computer architecture; Forecasting; Parallel architectures; Security of data; Grey system theory; Predicting method; Risk levels; Security level; Severity; Software security; Software vulnerabilities; Vulnerability; System theory","Conference paper","Final","","Scopus","2-s2.0-84952053661"
"Galanis P.; Shin Y.B.; Moehle J.P.","Galanis, Panagiotis (56625779500); Shin, Yoon Bong (58441616300); Moehle, Jack P. (35512773800)","56625779500; 58441616300; 35512773800","Laboratory and computer simulations of reinforced concrete frames with different ductility","2016","Earthquake Engineering and Structural Dynamics","45","10","","1603","1619","16","","10.1002/eqe.2741","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964344129&doi=10.1002%2feqe.2741&partnerID=40&md5=b7e294ec18fb545970413d60bced1d95","Reinforced concrete structures that lack proper seismic detailing commonly have increased vulnerability to collapse during strong earthquake shaking. A major contributor to building collapse vulnerability is the prevalence of columns with widely spaced and poorly configured transverse reinforcement. Such columns are susceptible to shear failures, which can lead to axial failures and local or global building collapse. This study presents a laboratory test program that was designed to gain insight into the effects of column detailing on the dynamic response including collapse of concrete frames. Twelve concrete frames were tested on a shaking table and were subjected to two types of ground motions: one relatively short-duration motion with strong velocity pulse and one relatively long-duration motion with multiple cycles. The study also evaluates the effectiveness of modern analytical methods to simulate the nonlinear dynamic response of concrete structures. Two lumped plasticity models employing nonlinear rotational and shear springs at the column ends were used to simulate the collapse response of the tested specimens. The study demonstrates that modern analytical techniques can simulate reliably the nonlinear dynamic response of concrete structures in the post-yielding stage and can identify the onset of shear failure and collapse of concrete frames. However, in the majority of cases, the analytical models overestimated the effects of damage accumulation, especially for long-duration motions. Response predictions were improved by adjusting the damage modeling parameters. Copyright © 2016 John Wiley & Sons, Ltd. Copyright © 2016 John Wiley & Sons, Ltd.","analysis; collapse; concrete; dynamic; frames; test","Concrete buildings; Concrete construction; Concretes; Dynamic response; Dynamics; Earthquakes; Software testing; Testing; Analysis; Collapse; Damage model parameters; Frames; Lumped plasticity model; Reinforced concrete frames; Response prediction; Transverse reinforcement; building; column; computer simulation; concrete structure; ductility; dynamic response; ground motion; reinforced concrete; seismic response; shaking table test; Reinforced concrete","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-84964344129"
"Freitas E.N.D.A.; Camilo-Junior C.G.; Vincenzi A.M.R.","Freitas, Eduardo Noronha De Andrade (15073791300); Camilo-Junior, Celso Goncalves (26421044900); Vincenzi, Auri Marcelo Rizzo (57205344487)","15073791300; 26421044900; 57205344487","SCOUT: A Multi-objective Method to Select Components in Designing Unit Testing","2016","Proceedings - International Symposium on Software Reliability Engineering, ISSRE","","","7774505","36","46","10","","10.1109/ISSRE.2016.52","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013304618&doi=10.1109%2fISSRE.2016.52&partnerID=40&md5=74d0141be9679c113bc64761106fb688","The creation of a suite of unit testing is preceded by the selection of which components (code units) should be tested. This selection is a significant challenge, usually made based on the team member's experience or guided by defect prediction or fault localization models. We modeled the selection of components for unit testing with limited resources as a multi-objective problem, addressing two different objectives: maximizing benefits and minimizing testing cost. To measure the benefit of a component, we made use of metrics from static analysis (cost of future maintenance), dynamic analysis (risk of fault, and frequency of calls), and business value. We tackled gaps and challenges in the literature to formulate an effective method, the Selector of Software Components for Unit Testing (SCOUT). SCOUT provides an automated extraction of all necessary data followed by a multi-objective optimization process. SCOUT is a method able to assist testers in different domains, and the Android platform was chosen to perform our experiments, taking nine leading open-source applications as our subjects. SCOUT was compared with two of the most frequently used strategies in terms of efficacy. We also compared the effectiveness and efficiency of seven algorithms in solving a multi-objective component selection problem. Our experiments were performed under different scenarios, and reveal the potential of SCOUT in reducing the market vulnerability, compared to others approaches. To the best of our knowledge, SCOUT is the first method to assist in an automated way software testing managers in selecting components for the development of unit testing, combining static and dynamic metrics and business value. © 2016 IEEE.","component selection; multi-objective optimization; Search Based Software Testing (SBST); software testing; unit testing","Computer software selection and evaluation; Cost benefit analysis; Multiobjective optimization; Open source software; Risk analysis; Risk assessment; Software reliability; Static analysis; Automated extraction; Component selection; Effectiveness and efficiencies; Fault localization; Multi-objective problem; Open source application; Search-based software testing; Unit testing; Software testing","Conference paper","Final","","Scopus","2-s2.0-85013304618"
"Sun B.; Zhang G.","Sun, Baitao (13104458800); Zhang, Guixin (55738939100)","13104458800; 55738939100","Study on seismic disaster risk distribution of buildings in mainland China","2017","Tumu Gongcheng Xuebao/China Civil Engineering Journal","50","9","","1","7","6","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044086361&partnerID=40&md5=d3114f88e7f0d7bc28ef0066a3e58009","Macroscopically mastering the distribution and the situation of seismic capacity of buildings in China is of great importance. It plays an essential role in government-directed construction planning, seismic strengthening of buildings and enhancing the public's awareness of disaster prevention prior to the occurrence of earthquakes. Besides it can guide the earthquake emergency response, loss assessment, and so on during earthquakes. In this study, a large-scale sampling survey on the seismic capacity of buildings across the mainland China is carried out. On this basis, the correlation of the seismic capacity of buildings in different regions with many factors such as the economy, land use, seismic fortification levels, population density, and level of administrative division is studied. Meanwhile, a comprehensive division and classification method for demonstrating seismic capacity of buildings in mainland China was proposed. By using the prediction method for seismic damages to building groups, seismic vulnerability matrixes of buildings with different structures in various regions were established based on the seismic damage data and the earthquake prediction data. In addition, the seismic disaster risk and loss distribution maps of buildings in mainland China under the earthquake actions with basic intensity were drawn on the basis of the GIS software and the stocks of buildings with different structures in various regions. A whole thought and method for seismic disaster risk and loss evaluation of buildings in different regions were proposed, which could provide references for the further studies on the distribution of the seismic disaster risk loss of lifeline engineering and casualties. © 2017, Editorial Office of China Civil Engineering Journal. All right reserved.","Comprehensive division and classification; Sampling survey; Seismic capacity; Seismic disaster risk; Seismic vulnerability","Buildings; Disaster prevention; Disasters; Economics; Forecasting; Geophysics; Land use; Population statistics; Seismology; Structural analysis; Surveys; Classification methods; Earthquake emergency response; Sampling survey; Seismic capacity; Seismic disaster; Seismic fortification; Seismic strengthening; Seismic vulnerability; Earthquakes","Article","Final","","Scopus","2-s2.0-85044086361"
"Singh R.D.; Aggarwal N.","Singh, Raahat Devender (56488829400); Aggarwal, Naveen (36875216400)","56488829400; 36875216400","Optical Flow and Prediction Residual Based Hybrid Forensic System for Inter-Frame Tampering Detection","2017","Journal of Circuits, Systems and Computers","26","7","1750107","","","","","10.1142/S0218126617501079","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015374244&doi=10.1142%2fS0218126617501079&partnerID=40&md5=f6600c5ed65f4baaad8b2b137e9815f8","In the wake of widespread proliferation of inexpensive and easy-to-use digital content editing software, digital videos have lost the idealized reputation they once held as universal, objective and infallible evidence of occurrence of events. The pliability of digital content and its innate vulnerability to unobtrusive alterations causes us to become skeptical of its validity. However, in spite of the fact that digital videos may not always present a truthful picture of reality, their usefulness in today's world is incontrovertible. Therefore, the need to verify the integrity and authenticity of the contents of a digital video becomes paramount, especially in critical scenarios such as defense planning and legal trials where reliance on untrustworthy evidence could have grievous ramifications. Inter-frame tampering, which involves insertion/removal/replication of sets of frames into/from/within a video sequence, is among the most un-convoluted and elusive video forgeries. In this paper, we propose a potent hybrid forensic system that detects inter-frame forgeries in compressed videos. The system encompasses two forensic techniques. The first is a novel optical flow analysis based frame-insertion and removal detection procedure, where we focus on the brightness gradient component of optical flow and detect irregularities caused therein by post-production frame-tampering. The second component is a prediction residual examination based scheme that expedites detection and localization of replicated frames in video sequences. Subjective and quantitative results of comprehensive tests on an elaborate dataset under diverse experimental set-ups substantiate the effectuality and robustness of the proposed system. © 2017 World Scientific Publishing Company.","Digital video forensics; frame-deletion; frame-duplication; frame-insertion; frame-removal; frame-replication; inter-frame forgery detection","Digital forensics; Forensic engineering; Multimedia systems; Optical flows; Statistical tests; Video recording; Video signal processing; Digital video forensics; Forgery detections; frame-deletion; frame-duplication; frame-insertion; frame-replication; Computer graphics","Article","Final","","Scopus","2-s2.0-85015374244"
"Hassan R.; Ismail I.; Kasim S.; Othman M.R.; Ramlan R.","Hassan, Rohayanti (35310505600); Ismail, Iszaida (57194262179); Kasim, Shahreen (36155431900); Othman, Muhammad Razib (57196076163); Ramlan, Rohaizan (55539714700)","35310505600; 57194262179; 36155431900; 57196076163; 55539714700","Investigation of attributes that influence the insider trust","2017","International Journal on Advanced Science, Engineering and Information Technology","7","5","","1777","1783","6","","10.18517/ijaseit.7.5.3406","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032664443&doi=10.18517%2fijaseit.7.5.3406&partnerID=40&md5=639bdb0d62e731afcaf5917fd4cc48f3","A study of cyber-attack incidents emanating from insiders identifies some characteristic of the malicious user including trust, attack on hardware, software and network, and vulnerabilities of threat. Among the research that has been conducted, insider trust is identified as a critical characteristic where trust of insider is categorized as a major potential to attack system information either high, medium or low risk to access the sensitive document. Trust characteristics is hard to be analyzed due to the different human behaviour. Thus, a survey was conducted that includes hypothesis to support the investigation of insider threat characteristic. To obtain the result of finding prominent insider trust criteria, a regression analysis is used to get the actual value. A survey has been distributed to multiple user roles of three systems namely e-Plantation System (ePS), eCampus System (eCampus) and Human Resources Management System (eHRMS). The outcome of this study demonstrates that skill and experience are two prominent factors that mainly influence the characteristic of insider trust.","Insider threat; Insider threat prediction; Insider trust; Insider trust criteria; Trust characteristics","","Article","Final","","Scopus","2-s2.0-85032664443"
"Alenezi M.; Abunadi I.","Alenezi, Mamdouh (55854089000); Abunadi, Ibrahim (57189636917)","55854089000; 57189636917","Evaluating software metrics as predictors of software vulnerabilities","2015","International Journal of Security and its Applications","9","10","","231","240","9","","10.14257/ijsia.2015.9.10.21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979753686&doi=10.14257%2fijsia.2015.9.10.21&partnerID=40&md5=f57960c8260521aaeaf153c04d58625f","Web application security is an important problem in today's Internet. A major cause of this is that many developers are not equipped with the right skills to develop secure code. Because of limited time and resources, web engineers need help in recognizing vulnerable components. A useful approach to predict vulnerable code would allow them to prioritize security-auditing efforts. In this work, we compare the performance of different classification techniques in predicting vulnerable PHP files and propose an application of these classification rules. We performed empirical case studies on three large open source web-projects. Software metrics are investigated whether they are discriminative and predictive of vulnerable code, and can guide actions for improvement of code and development team and can prioritize validation and verification efforts. The results indicate that the metrics are discriminative and predictive of vulnerabilities. © 2015 SERSC.","Software metrics; Vulnerability prediction; Web security","Codes (symbols); Forecasting; Network security; Open source software; Verification; Classification rules; Classification technique; Empirical case studies; Software metrics; Software vulnerabilities; Validation and verification; Web application security; WEB security; World Wide Web","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-84979753686"
"Khazaei A.; Ghasemzadeh M.; Derhami V.","Khazaei, Atefeh (57062271400); Ghasemzadeh, Mohammad (24528334900); Derhami, Vali (26434197700)","57062271400; 24528334900; 26434197700","An automatic method for CVSS score prediction using vulnerabilities description","2016","Journal of Intelligent and Fuzzy Systems","30","1","","89","96","7","","10.3233/IFS-151733","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954502690&doi=10.3233%2fIFS-151733&partnerID=40&md5=d90b54974f6633611d1e6be570fd8a74","In this paper we introduce an objective method for CVSS score calculation. CVSS is a well known and mostly used method for giving priority to software vulnerabilities. Currently it is being calculated by some slightly subjective methods which require enough skill and knowledge. This research shows how we can benefit from natural language description of vulnerabilities for CVSS calculation. The data that were used for implementation and evaluation of the proposed models consists of the available CVE vulnerability descriptions and their corresponding CVSS scores from the OSVDB database. First, feature vectors were extracted using text mining tools and techniques, and then the SVM and Random-Forest algorithms as well as fuzzy systems were examined to predict the concerned CVSS scores. In spite of the fact that SVM and Random-Forest are mostly used and trusted methods in prediction, results of this research bear a witness that using fuzzy systems can give comparable and even better results. In addition, implementation of the fuzzy based system is much easier and faster. Although so far, there have been so little efforts in using the information embedded in textual materials regarding vulnerabilities, this research shows that it will be valuable to utilize them in systems security establishment. © 2016 - IOS Press and the authors. All rights reserved.","CommonVulnerability Scoring System (CVSS); Description of software vulnerability; fuzzy systems; Random-Forest; SupportVector Machine (SVM)","Data mining; Decision trees; Embedded systems; Fuzzy systems; Natural language processing systems; Objective methods; Random forest algorithm; Random forests; Scoring systems; Software vulnerabilities; Subjective methods; Tools and techniques; Vulnerability description; Forecasting","Article","Final","","Scopus","2-s2.0-84954502690"
"Hovsepyan A.; Scandariato R.; Joosen W.","Hovsepyan, Aram (14056192200); Scandariato, Riccardo (23095243000); Joosen, Wouter (57202521929)","14056192200; 23095243000; 57202521929","Is Newer Always Better?: The Case of Vulnerability Prediction Models","2016","International Symposium on Empirical Software Engineering and Measurement","08-09-September-2016","","a26","","","","","10.1145/2961111.2962612","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991627902&doi=10.1145%2f2961111.2962612&partnerID=40&md5=bec9dda130d100b4bd1241184d4e86b0","Finding security vulnerabilities in the source code as early as possible is becoming more and more essential. In this respect, vulnerability prediction models have the potential to help the security assurance activities by identifying code locations that deserve the most attention. In this paper, we investigate whether prediction models behave like milk (i.e., they turn with time) or wine (i.e., the improve with time) when used to predict future vulnerabilities. Our findings indicate that the recall values are largely in favor of predictors based on older versions. However, the better recall comes at the price of much higher file inspection ratio values. © 2016 ACM.","prediction models; Security vulnerabilities","Forecasting; Software engineering; Identifying code; Prediction model; Security assurance; Security vulnerabilities; Source codes; Security of data","Conference paper","Final","","Scopus","2-s2.0-84991627902"
"Wei H.; Yueke W.; Kefei X.; Wei D.; Pengcheng L.","Wei, He (54391967600); Yueke, Wang (7601505104); Kefei, Xing (13905124500); Wei, Deng (57191515434); Pengcheng, Liu (57194089426)","54391967600; 7601505104; 13905124500; 57191515434; 57194089426","Total ionizing dose vulnerability analysis and on-orbit prediction for spaceborne signal processing platform","2017","2016 IEEE International Conference on Signal and Image Processing, ICSIP 2016","","","7888308","478","483","5","","10.1109/SIPROCESS.2016.7888308","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018695210&doi=10.1109%2fSIPROCESS.2016.7888308&partnerID=40&md5=c04f11c714baca7cca2718de0110adc9","To evaluate the total ionizing dose vulnerability of spaceborne signal processing platform, an accelerated radiation experiment is set up and on-orbit prediction method is proposed. For signal processing platform which consists of SRAM-based FPGA and DSP, function parameter and current variety are the two criterion of experiment. 60Co γ radial radiation experiment indicates that total dose is about 95 k rad(Si), and DSP is the most vulnerability device. The total ionizing dose of typical satellite orbit is predicted by MULASSIS software. Different shielding schemes for electron have been simulated with Monte-Carlo method. With 3 mm aluminium reinforce, the platform can be applied in the low earth orbit(LEO) orbit (800km, 1200km) for 28 years. With 2.8 mm aluminium and 0.2mm tantalum reinforce, the platform can be applied in the (geosynchronous earth orbit)GEO orbit for 28 years. © 2016 IEEE.","DSP; FPGA; prediction; radiation experiment; shielding; Total ionizing dose","Aluminum; Field programmable gate arrays (FPGA); Forecasting; Image processing; Ionizing radiation; Monte Carlo methods; Radiation shielding; Shielding; Signal processing; Function parameters; Geosynchronous Earth orbit; Low earth orbit(LEO); Radiation experiment; Satellite orbit; SRAM-based FPGA; Total Ionizing Dose; Vulnerability analysis; Orbits","Conference paper","Final","","Scopus","2-s2.0-85018695210"
"Koslandra T.; Murugan A.","Koslandra, T. (57193258254); Murugan, A. (56656227700)","57193258254; 56656227700","Predicting web services vulnerability using static and dynamic program analysis","2016","International Journal of Control Theory and Applications","9","16","","7885","7896","11","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012050874&partnerID=40&md5=a5bad1cc73aae58b82d3003da8d49c3e","Web software developers or engineers always in a search of support in finding vulnerable code any practical method for valuable code predicting would enable to give security efforts. While moving in this paper we are going to propose a set of static along with dynamic code attributes that tokenize inputs validating and sanitizing code patterns and is expected to provide the same security features. As all know static and dynamic program analysis always complement each other and both are needed to give accurate and scalable way to predict vulnerability. For many applications in real world may not have past vulnerability data or at least not available fully. hence, to address both situations where labeled past data is fully there or not, we use both supervised and semi supervised learning when building both static and dynamic code patterns. We describe how to use this learning effectively about schema for vulnerability prediction empirical case studies on seven open source projects where we will be creating evaluated supervised and semi-supervised models. The semi-supervised model showed an average improvement of 24% more recall and 3 % less probability of false alarm, thus suggesting semi-supervised learning will be a better solution for many problematic applications where vulnerability data is missing. The supervised are ready to get 77% of recall of false alarm for predicting SQL injection, cross site scripting, remote code execution as vulnerabilities. © International Science Press.","Empirical study; Input validation and sanitization; Program analysis; Security measures; Vulnerability prediction","","Article","Final","","Scopus","2-s2.0-85012050874"
"Han Z.; Li X.; Xing Z.; Liu H.; Feng Z.","Han, Zhuobing (55267205100); Li, Xiaohong (57022407900); Xing, Zhenchang (8347413500); Liu, Hongtao (57200280821); Feng, Zhiyong (56984876600)","55267205100; 57022407900; 8347413500; 57200280821; 56984876600","Learning to predict severity of software vulnerability using only vulnerability description","2017","Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017","","","8094415","125","136","11","","10.1109/ICSME.2017.52","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040543079&doi=10.1109%2fICSME.2017.52&partnerID=40&md5=d6bc056dcf42a561a981cc72da25e123","Software vulnerabilities pose significant security risks to the host computing system. Faced with continuous disclosure of software vulnerabilities, system administrators must prioritize their efforts, triaging the most critical vulnerabilities to address first. Many vulnerability scoring systems have been proposed, but they all require expert knowledge to determine intricate vulnerability metrics. In this paper, we propose a deep learning approach to predict multi-class severity level of software vulnerability using only vulnerability description. Compared with intricate vulnerability metrics, vulnerability description is the ""surface level"" information about how a vulnerability works. To exploit vulnerability description for predicting vulnerability severity, discriminative features of vulnerability description have to be defined. This is a challenging task due to the diversity of software vulnerabilities and the richness of vulnerability descriptions. Instead of relying on manual feature engineering, our approach uses word embeddings and a one-layer shallow Convolutional Neural Network (CNN) to automatically capture discriminative word and sentence features of vulnerability descriptions for predicting vulnerability severity. We exploit large amounts of vulnerability data from the Common Vulnerabilities and Exposures (CVE) database to train and test our approach. © 2017 IEEE.","Deep learning; Mining software repositories; Multi-class classification; Vulnerability severity prediction","Computer software maintenance; Deep learning; Forecasting; Neural networks; Common vulnerabilities and exposures; Convolutional neural network; Discriminative features; Mining software repositories; Multi-class classification; Software vulnerabilities; Vulnerability description; Vulnerability metrics; Network security","Conference paper","Final","","Scopus","2-s2.0-85040543079"
"Murtaza S.S.; Khreich W.; Hamou-Lhadj A.; Bener A.B.","Murtaza, Syed Shariyar (14056661100); Khreich, Wael (35097304600); Hamou-Lhadj, Abdelwahab (24314702300); Bener, Ayse Basar (21742123700)","14056661100; 35097304600; 24314702300; 21742123700","Mining trends and patterns of software vulnerabilities","2016","Journal of Systems and Software","117","","","218","228","10","","10.1016/j.jss.2016.02.048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961838451&doi=10.1016%2fj.jss.2016.02.048&partnerID=40&md5=d162dcea14d04abda1936da4d7923843","Zero-day vulnerabilities continue to be a threat as they are unknown to vendors; when attacks occur, vendors have zero days to provide remedies. New techniques for the detection of zero-day vulnerabilities on software systems are being developed but they have their own limitations; e.g., anomaly detection techniques are prone to false alarms. To better protect software systems, it is also important to understand the relationship between vulnerabilities and their patterns over a period of time. The mining of trends and patterns of vulnerabilities is useful because it can help software vendors prepare solutions ahead of time for vulnerabilities that may occur in a software application. In this paper, we investigate the use of historical patterns of vulnerabilities in order to predict future vulnerabilities in software applications. In addition, we examine whether the trends of vulnerabilities in software applications have any significant meaning or not. We use the National Vulnerability Database (NVD) as the main resource of vulnerabilities in software applications. We mine vulnerabilities of the last six years from 2009 to 2014 from NVD. Our results show that sequences of the same vulnerabilities (e.g., buffer errors) may occur 150 times in a software product. Our results also depict that the number of SQL injection vulnerabilities have decreased in the last six years while cryptographic vulnerabilities have seen an important increase. However, we have not found any statistical significance in the trends of the occurrence of vulnerabilities over time. The most interesting finding is that the sequential patterns of vulnerability events follow a first order Markov property; that is, we can predict the next vulnerability by using only the previous vulnerability with a recall of approximately 80% and precision of around 90%. © 2016 Elsevier Inc. All rights reserved.","Software vulnerabilities; Vulnerability prediction; Vulnerability trends","Computer software; Forecasting; National vulnerability database; Sequential patterns; Software applications; Software products; Software vulnerabilities; Statistical significance; Vulnerability trends; Zero day vulnerabilities; Application programs","Article","Final","","Scopus","2-s2.0-84961838451"
"Akkus Z.; Van Burken G.; Van Den Oord S.C.H.; Schinkel A.F.L.; De Jong N.; Van Der Steen A.F.W.; Bosch J.G.","Akkus, Zeynettin (41560908300); Van Burken, Gerard (8878660000); Van Den Oord, Stijn C. H. (51764560300); Schinkel, Arend F. L. (56197184200); De Jong, Nico (55893957500); Van Der Steen, Antonius F. W. (35397523500); Bosch, Johan G. (7402325591)","41560908300; 8878660000; 51764560300; 56197184200; 55893957500; 35397523500; 7402325591","Carotid intraplaque neovascularization quantification software (CINQS)","2015","IEEE Journal of Biomedical and Health Informatics","19","1","6740838","332","338","6","","10.1109/JBHI.2014.2306454","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920895392&doi=10.1109%2fJBHI.2014.2306454&partnerID=40&md5=1b4d2465a4052f232b5c0f2492e7f9c0","Intraplaque neovascularization (IPN) is an important biomarker of atherosclerotic plaque vulnerability. As IPN can be detected by contrast enhanced ultrasound (CEUS), imaging-biomarkers derived from CEUS may allow early prediction of plaque vulnerability. To select the best quantitative imaging-biomarkers for prediction of plaque vulnerability, a systematic analysis of IPN with existing and new analysis algorithms is necessary. Currently available commercial contrast quantification tools are not applicable for quantitative analysis of carotid IPN due to substantial motion of the carotid artery, artifacts, and intermittent perfusion of plaques. We therefore developed a specialized software package called Carotid intraplaque neovascularization quantification software (CINQS). It was designed for effective and systematic comparison of sets of quantitative imaging biomarkers. CINQS includes several analysis algorithms for carotid IPN quantification and overcomes the limitations of current contrast quantification tools and existing carotid IPN quantification approaches. CINQS has a modular design which allows integrating new analysis tools. Wizard-like analysis tools and its graphical-user-interface facilitate its usage. In this paper, we describe the concept, analysis tools, and performance of CINQS and present analysis results of 45 plaques of 23 patients. The results in 45 plaques showed excellent agreement with visual IPN scores for two quantitative imaging-biomarkers (The area under the receiver operating characteristic curve was 0.92 and 0.93). © 2014 IEEE.","Carotid plaques; contrast enhanced ultrasound; microbubble; neovascularization; plaque perfusion analysis; quantification of neovascularization","Algorithms; Artificial Intelligence; Carotid Stenosis; Contrast Media; Humans; Image Interpretation, Computer-Assisted; Neovascularization, Pathologic; Pattern Recognition, Automated; Reproducibility of Results; Sensitivity and Specificity; Software; Software Validation; Ultrasonography; Chemical analysis; Graphical user interfaces; contrast medium; Carotid plaques; Contrast enhanced ultrasound; Micro-bubble; Neo-vascularization; Perfusion analysis; algorithm; artificial intelligence; automated pattern recognition; carotid artery obstruction; complication; computer assisted diagnosis; computer program; echography; human; neovascularization (pathology); procedures; reproducibility; sensitivity and specificity; Biomarkers","Article","Final","","Scopus","2-s2.0-84920895392"
"Silva M.L.D.; Frère A.F.; Oliveira H.J.Q.D.; Martucci Neto H.; Scardovelli T.A.","Silva, Meire Luci da (57192577563); Frère, Annie France (7003645104); Oliveira, Henrique Jesus Quintino de (56850558700); Martucci Neto, Helio (57192576590); Scardovelli, Terigi Augusto (6504334343)","57192577563; 7003645104; 56850558700; 57192576590; 6504334343","Computer tool to evaluate the cue reactivity of chemically dependent individuals","2017","Computer Methods and Programs in Biomedicine","140","","","139","149","10","","10.1016/j.cmpb.2016.11.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006839781&doi=10.1016%2fj.cmpb.2016.11.014&partnerID=40&md5=f974f4f7254daf26a05d5b09d18f5a2f","Background and objective Anxiety is one of the major influences on the dropout of relapse and treatment of substance abuse treatment. Chemically dependent individuals need (CDI) to be aware of their emotional state in situations of risk during their treatment. Many patients do not agree with the diagnosis of the therapist when considering them vulnerable to environmental stimuli related to drugs. This research presents a cue reactivity detection tool based on a device acquiring physiological signals connected to personal computer. Depending on the variations of the emotional state of the drug addict, alteration of the physiological signals will be detected by the computer tool (CT) which will modify the displayed virtual sets without intervention of the therapist. Methods Developed in 3ds Max® software, the CT is composed of scenarios and objects that are in the habit of marijuana and cocaine dependent individual's daily life. The interaction with the environment is accomplished using a Human-Computer Interface (HCI) that converts incoming physiological signals indicating anxiety state into commands that change the scenes. Anxiety was characterized by the average variability from cardiac and respiratory rate of 30 volunteers submitted stress environment situations. To evaluate the effectiveness of cue reactivity a total of 50 volunteers who were marijuana, cocaine or both dependent were accompanied. Results Prior to CT, the results demonstrated a poor correlation between the therapists’ predictions and those of the chemically dependent individuals. After exposure to the CT, there was a significant increase of 73% in awareness of the risks of relapse. Conclusion We confirmed the hypothesis that the CT, controlled only by physiological signals, increases the perception of vulnerability to risk situations of individuals with dependence on marijuana, cocaine or both. © 2016 Elsevier Ireland Ltd","Anxiety; Chemically dependent; Computer tool; Cue reactivity","Awareness; Computer Simulation; Humans; Recurrence; Substance-Related Disorders; Diagnosis; Drug therapy; Interface states; Personal computers; Physiology; Risk perception; cannabis; cocaine; Anxiety; Chemically dependent; Computer tools; Detection tools; Environmental stimuli; Human computer interfaces; Physiological signals; Respiratory rate; adult; anxiety disorder; Article; association; breathing rate; cannabis addiction; cocaine dependence; conflict; cue reactivity; emotion; environmental stress; heart rate; human; human computer interaction; male; personal computer; quantitative analysis; questionnaire; software; virtual reality; awareness; computer simulation; drug dependence; psychology; recurrent disease; Human computer interaction","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85006839781"
"Catal C.; Akbulut A.; Ekenoglu E.; Alemdaroglu M.","Catal, Cagatay (22633325800); Akbulut, Akhan (25960607500); Ekenoglu, Ecem (57196050402); Alemdaroglu, Meltem (57196049650)","22633325800; 25960607500; 57196050402; 57196049650","Development of a software vulnerability prediction web service based on artificial neural networks","2017","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10526 LNAI","","","59","67","8","","10.1007/978-3-319-67274-8_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031421694&doi=10.1007%2f978-3-319-67274-8_6&partnerID=40&md5=b56910ca40383ce34d0bc7616d1d67a4","Detecting vulnerable components of a web application is an important activity to allocate verification resources effectively. Most of the studies proposed several vulnerability prediction models based on private and public datasets so far. In this study, we aimed to design and implement a software vulnerability prediction web service which will be hosted on Azure cloud computing platform. We investigated several machine learning techniques which exist in Azure Machine Learning Studio environment and observed that the best overall performance on three datasets is achieved when Multi-Layer Perceptron method is applied. Software metrics values are received from a web form and sent to the vulnerability prediction web service. Later, prediction result is computed and shown on the web form to notify the testing expert. Training models were built on datasets which include vulnerability data from Drupal, Moodle, and PHPMyAdmin projects. Experimental results showed that Artificial Neural Networks is a good alternative to build a vulnerability prediction model and building a web service for vulnerability prediction purpose is a good approach for complex systems. © 2017, Springer International Publishing AG.","Artificial neural networks; Machine learning; Prediction model; Vulnerabilities; Vulnerability prediction; Web service","Administrative data processing; Artificial intelligence; Data mining; Enterprise resource management; Forecasting; Learning systems; Neural networks; Sensory analysis; Testing; Websites; Cloud computing platforms; Design and implements; Machine learning techniques; Multi layer perceptron; Prediction model; Software metrics; Software vulnerabilities; Vulnerabilities; Web services","Conference paper","Final","","Scopus","2-s2.0-85031421694"
"Theisen C.; Herzig K.; Murphy B.; Williams L.","Theisen, Christopher (57216482869); Herzig, Kim (35078523000); Murphy, Brendan (7402698081); Williams, Laurie (35565101900)","57216482869; 35078523000; 7402698081; 35565101900","Risk-based attack surface approximation: How much data is enough?","2017","Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017","","","7965451","273","282","9","","10.1109/ICSE-SEIP.2017.9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026831298&doi=10.1109%2fICSE-SEIP.2017.9&partnerID=40&md5=ca63c6ab09ae680790dd75f307fb3a6d","Proactive security reviews and test efforts are a necessary component of the software development lifecycle. Resource limitations often preclude reviewing the entire code base. Making informed decisions on what code to review can improve a team's ability to find and remove vulnerabilities. Risk-based attack surface approximation (RASA) is a technique that uses crash dump stack traces to predict what code may contain exploitable vulnerabilities. The goal of this research is to help software development teams prioritize security efforts by the efficient development of a risk-based attack surface approximation. We explore the use of RASA using Mozilla Firefox and Microsoft Windows stack traces from crash dumps. We create RASA at the file level for Firefox, in which the 15.8% of the files that were part of the approximation contained 73.6% of the vulnerabilities seen for the product. We also explore the effect of random sampling of crashes on the approximation, as it may be impractical for organizations to store and process every crash received. We find that 10-fold random sampling of crashes at a rate of 10% resulted in 3% less vulnerabilities identified than using the entire set of stack traces for Mozilla Firefox. Sampling crashes in Windows 8.1 at a rate of 40% resulted in insignificant differences in vulnerability and file coverage as compared to a rate of 100%. © 2017 IEEE.","attack surface; prediction models; stack traces","Codes (symbols); Software engineering; Software testing; Web browsers; Microsoft windows; Prediction model; Proactive security; Resource limitations; Software development life cycle; Software development teams; stack traces; Surface approximation; Software design","Conference paper","Final","","Scopus","2-s2.0-85026831298"
"Edkrantz M.; Said A.","Edkrantz, Michel (57188766787); Said, Alan (27868044600)","57188766787; 27868044600","Predicting cyber vulnerability exploits with machine learning","2015","Frontiers in Artificial Intelligence and Applications","278","","","48","57","9","","10.3233/978-1-61499-589-0-48","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963682732&doi=10.3233%2f978-1-61499-589-0-48&partnerID=40&md5=413958edd1457d9799d408231393fa93","For an information security manager it can be a daunting task to keep up and assess which new cyber vulnerabilities to prioritize patching first. Every day numerous new vulnerabilities and exploits are reported for a wide variety of different software configurations. We use machine learning to make automatic predictions for unseen vulnerabilities based on previous exploit patterns. As sources for historic vulnerability data, we use the National Vulnerability Database (NVD) and the Exploit Database (EDB). Our work shows that common words from the vulnerability descriptions, external references, and vendor products, are the most important features to consider. Common Vulnerability Scoring System (CVSS) scores and categorical parameters, and Common Weakness Enumeration (CWE) numbers are redundant when a large number of common words are used, since this information is often contained within the vulnerability description. Using machine learning algorithms, it is possible to get a prediction accuracy of 83% for binary classification. In comparison, the performance differences between some of the algorithms are marginal with respect to metrics such as accuracy, precision, and recall. The best classifier with respect to both performance metrics and execution time is a linear time Support Vector Machine (SVM) algorithm. We conclude that in order to get better predictions the data quality must be enhanced. © 2015 The authors and IOS Press. All rights reserved.","Cyber security; Data mining; Exploits; Information security; Machine learning; SVM; Vulnerability prediction","Cybersecurity; Data mining; Fuzzy systems; Intelligent systems; Learning algorithms; Support vector machines; Automatic prediction; Cyber security; Cyber vulnerabilities; Exploit; Machine-learning; Security manager; Software configuration; Support vectors machine; Vulnerability description; Vulnerability prediction; Forecasting","Conference paper","Final","","Scopus","2-s2.0-84963682732"
"Moshtari S.; Sami A.","Moshtari, Sara (56016562300); Sami, Ashkan (7004124604)","56016562300; 7004124604","Evaluating and comparing complexity, coupling and a new proposed set of coupling metrics in cross-project vulnerability prediction","2016","Proceedings of the ACM Symposium on Applied Computing","04-08-April-2016","","","1415","1421","6","","10.1145/2851613.2851777","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975889769&doi=10.1145%2f2851613.2851777&partnerID=40&md5=b37543a72bc9b31d42b6412795114cc5","Software security is an important concern in the world moving towards Information Technology. Detecting software vulnerabilities is a difficult and resource consuming task. Therefore, automatic vulnerability prediction would help development teams to predict vulnerability-prone components and prioritize security inspection efforts. Software source code metrics and data mining techniques have been recently used to predict vulnerability-prone components. Some of previous studies used a set of unit complexity and coupling metrics to predict vulnerabilities. In this study, first, we compare the predictability power of these two groups of metrics in cross-project vulnerability prediction. In cross-project vulnerability prediction we create the prediction model based on datasets of completely different projects and try to detect vulnerabilities in another project. The experimental results show that unit complexity metrics are stronger vulnerability predictors than coupling metrics. Then, we propose a new set of coupling metrics which are called Included Vulnerable Header (IVH) metrics. These new coupling metrics, which consider interaction of application modules with outside of the application, predict vulnerabilities highly better than regular coupling metrics. Furthermore, adding IVH metrics to the set of complexity metrics improves Recall of the best predictor from 60.9% to 87.4% and shows the best set of metrics for cross-project vulnerability prediction. © 2016 ACM.","Complexity metrics; Coupling metrics; Cross-project; Software security; Vulnerability prediction","Computational complexity; Data mining; Application module; Complexity metrics; Cross-project; Development teams; Prediction model; Software security; Software source codes; Software vulnerabilities; Forecasting","Conference paper","Final","","Scopus","2-s2.0-84975889769"
"Dobrovoljc A.; Trček D.; Likar B.","Dobrovoljc, Andrej (56108543900); Trček, Denis (55897351200); Likar, Borut (56227842000)","56108543900; 55897351200; 56227842000","Predicting exploitations of information systems vulnerabilities through attackers’ characteristics","2017","IEEE Access","5","","8094238","26063","26075","12","","10.1109/ACCESS.2017.2769063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033376973&doi=10.1109%2fACCESS.2017.2769063&partnerID=40&md5=b7198b3373413bdc58029d64932863e6","The main goal of proactive security is to prevent attacks before they happen. In modern information systems it largely depends on the vulnerability management process, where prioritization is one of the key steps. A widely used prioritization policy based only upon a common vulnerability scoring system (CVSS) score is frequently criticised for bad effectiveness. The main reason is that the CVSS score alone is not a good predictor of vulnerability exploitation in the wild. Therefore, the aim of the research in this field is to determine in what way we can improve our prediction abilities. Clearly, software vulnerabilities are commodities used by attackers. Hence, it makes sense considering their characteristics in vulnerability prioritization. In contrast, one should be able to measure and compare the effectiveness of various policies. Therefore, an important goal of this paper was to develop an evaluation model, which would allow such comparisons. For this purpose, we developed an agent-based simulation model which measures the exposure of information system to exploitable vulnerabilities. Besides, some policies which take into account human threats were defined and then compared with the most popular existing methods. Experimental results imply that the proposed policy, which is based on CVSS vectors and attacker characteristics, achieves the highest effectiveness among existing methods. © 2017 IEEE.","CVSS; Prioritization policy; Security management; Threat agent; Vulnerability","Computer software; Database systems; Information systems; Information use; Libraries; Mathematical models; Software agents; Standards; CVSS; Prioritization; Security; Security management; Threat agents; vulnerability; Information management","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85033376973"
"Walden J.; Stuckman J.; Scandariato R.","Walden, James (22036834700); Stuckman, Jeff (35303887700); Scandariato, Riccardo (23095243000)","22036834700; 35303887700; 23095243000","Predicting vulnerable components: Software metrics vs text mining","2014","Proceedings - International Symposium on Software Reliability Engineering, ISSRE","","","6982351","23","33","10","","10.1109/ISSRE.2014.32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928669827&doi=10.1109%2fISSRE.2014.32&partnerID=40&md5=bd6c2bd91f0e2d04895fb3c8c37f0566","Building secure software is difficult, time-consuming, and expensive. Prediction models that identify vulnerability prone software components can be used to focus security efforts, thus helping to reduce the time and effort required to secure software. Several kinds of vulnerability prediction models have been proposed over the course of the past decade. However, these models were evaluated with differing methodologies and datasets, making it difficult to determine the relative strengths and weaknesses of different modeling techniques. In this paper, we provide a high-quality, public dataset, containing 223 vulnerabilities found in three web applications, to help address this issue. We used this dataset to compare vulnerability prediction models based on text mining with models using software metrics as predictors. We found that text mining models had higher recall than software metrics based models for all three applications. © 2014 IEEE.","","Application programs; Forecasting; Predictive analytics; Text mining; Modeling technique; Prediction model; Public dataset; Relative strength; Secure software; Software component; Software metrics; WEB application; Software reliability","Conference paper","Final","","Scopus","2-s2.0-84928669827"
"Padmanabhuni B.M.; Tan H.B.K.","Padmanabhuni, Bindu Madhavi (54395923600); Tan, Hee Beng Kuan (7403011293)","54395923600; 7403011293","Predicting buffer overflow vulnerabilities through mining light-weight static code attributes","2014","Proceedings - IEEE 25th International Symposium on Software Reliability Engineering Workshops, ISSREW 2014","","","6983860","317","322","5","","10.1109/ISSREW.2014.26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922606460&doi=10.1109%2fISSREW.2014.26&partnerID=40&md5=8051a27b2601a40a81c80eb26c5b9844","Static code attributes are widely used in defect prediction studies as an abstraction model because they capture general properties of the program. To counter buffer overflow exploits, programmers use buffer size checking and input validation schemes. In this paper, we propose light-weight static code attributes that can be extracted easily, to characterize buffer overflow safety mechanisms and input validation checks implemented in the code for predicting buffer overflows. We then use data mining methods on the collected static code attributes to predict buffer overflows in application programs. In our experiments across five applications, our best classifier could achieve a recall of 95% and precision over 80% suggesting that our proposed static code attributes are effective indicators in predicting buffer overflows. © 2014 IEEE.","Buffer overflow; Data mining; Input validation; Prediction; Static analysis; Static code attributes; Vulnerability","Application programs; Buffer storage; Codes (symbols); Forecasting; Software reliability; Static analysis; Abstraction model; Buffer overflows; Data mining methods; Defect prediction; Input validation; Safety mechanisms; Static codes; Vulnerability; Data mining","Conference paper","Final","","Scopus","2-s2.0-84922606460"
"Arthur W.; Madeka S.; Das R.; Austin T.","Arthur, William (57197593243); Madeka, Sahil (57156620900); Das, Reetuparna (22833936700); Austin, Todd (7101675665)","57197593243; 57156620900; 22833936700; 7101675665","Locking down insecure indirection with hardware-based control-data isolation","2015","Proceedings of the Annual International Symposium on Microarchitecture, MICRO","05-09-December-2015","","","115","127","12","","10.1145/2830772.2830801","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959886547&doi=10.1145%2f2830772.2830801&partnerID=40&md5=5c816a68d4fea58d11db6f7bb05671c3","Arbitrary code injection pervades as a central issue in computer security where attackers seek to exploit the software attack surface. A key component in many exploits today is the successful execution of a control-flow attack. Control-Data Isolation (CDI) has emerged as a work which eliminates the root cause of contemporary control-flow attacks: indirect control flow instructions. These instructions are replaced by direct control flow edges dictated by the programmer and encoded into the application by the compiler. By subtracting the root cause of control-flow attack, Control-Data Isolation sidesteps the vulnerabilities and restrictive threat models adopted by other solutions in this space (e.g., Control-Flow Integrity). The CDI approach, while eliminating contemporary control-flow attacks, introduces non-trivial overheads to validate indirect targets at runtime. In this work we introduce novel architectural support to accelerate the execution of CDI-compliant code. Through the addition of an edge cache, we are able to cache legal indirect target edges and eliminate nearly all execution overhead for indirection-free applications. We demonstrate that through memoization of compiler-confirmed control flow transitions, overheads are reduced from 19% to 0.5% on average for Control-Data Isolated applications. Additionally, we show that the edge cache can efficiently provide the double-duty of predicting multi-way branch targets, thus providing even speedups for some CDI-compliant executions, compared to an architecture with unsophisticated indirect control prediction (e.g., BTB). © 2015 ACM.","CFG inegrity; control-data isolation; control-flow attack; indirect control flow; program transformation; secure computation; security architectures; security policies; software vulnerabilities","Computer architecture; Locks (fasteners); Program compilers; Security of data; Security systems; CFG inegrity; Control data; Control flows; Indirect control; Program transformations; Secure computation; Security Architecture; Security policy; Software vulnerabilities; Metadata","Conference paper","Final","","Scopus","2-s2.0-84959886547"
"Sultana K.Z.","Sultana, Kazi Zakia (23494078600)","23494078600","Towards a software vulnerability prediction model using traceable code patterns and software metrics","2017","ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering","","","8115724","1022","1025","3","","10.1109/ASE.2017.8115724","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041452101&doi=10.1109%2fASE.2017.8115724&partnerID=40&md5=ec53293216c305152e7e3762b632c355","Software security is an important aspect of ensuring software quality. The goal of this study is to help developers evaluate software security using traceable patterns and software metrics during development. The concept of traceable patterns is similar to design patterns but they can be automatically recognized and extracted from source code. If these patterns can better predict vulnerable code compared to traditional software metrics, they can be used in developing a vulnerability prediction model to classify code as vulnerable or not. By analyzing and comparing the performance of traceable patterns with metrics, we propose a vulnerability prediction model. This study explores the performance of some code patterns in vulnerability prediction and compares them with traditional software metrics. We use the findings to build an effective vulnerability prediction model. We evaluate security vulnerabilities reported for Apache Tomcat, Apache CXF and three stand-alone Java web applications. We use machine learning and statistical techniques for predicting vulnerabilities using traceable patterns and metrics as features. We found that patterns have a lower false negative rate and higher recall in detecting vulnerable code than the traditional software metrics. © 2017 IEEE.","","Codes (symbols); Computer software selection and evaluation; Learning systems; Software engineering; False negative rate; Prediction model; Security vulnerabilities; Software metrics; Software Quality; Software security; Software vulnerabilities; Statistical techniques; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85041452101"
"Pang Y.; Xue X.; Namin A.S.","Pang, Yulei (56131822700); Xue, Xiaozhen (37666341100); Namin, Akbar Siami (13805425800)","56131822700; 37666341100; 13805425800","Predicting vulnerable software components through N-gram analysis and statistical feature selection","2016","Proceedings - 2015 IEEE 14th International Conference on Machine Learning and Applications, ICMLA 2015","","","7424372","543","548","5","","10.1109/ICMLA.2015.99","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969673989&doi=10.1109%2fICMLA.2015.99&partnerID=40&md5=1285451cc71c5df12493bf8883f58d6b","Vulnerabilities need to be detected and removed from software. Although previous studies demonstrated the usefulness of employing prediction techniques in deciding about vulnerabilities of software components, the accuracy and improvement of effectiveness of these prediction techniques is still a grand challenging research question. This paper proposes a hybrid technique based on combining N-gram analysis and feature selection algorithms for predicting vulnerable software components where features are defined as continuous sequences of token in source code files, i.e., Java class file. Machine learning-based feature selection algorithms are then employed to reduce the feature and search space. We evaluated the proposed technique based on some Java Android applications, and the results demonstrated that the proposed technique could predict vulnerable classes, i.e., software components, with high precision, accuracy and recall. © 2015 IEEE.","Feature selection; N-gram; Vulnerability prediction; Wilcoxon test","Application programs; Artificial intelligence; Computer software; Forecasting; Java programming language; Learning systems; Android applications; Continuous sequences; Feature selection algorithm; N-grams; Prediction techniques; Research questions; Statistical features; Wilcoxon test; Feature extraction","Conference paper","Final","","Scopus","2-s2.0-84969673989"
"Younis A.; Malaiya Y.K.; Anderson C.; Ray I.","Younis, Awad (56115098000); Malaiya, Yashwant K. (35619571700); Anderson, Charles (57203054330); Ray, Indrajit (57614122900)","56115098000; 35619571700; 57203054330; 57614122900","To fear or not to fear that is the question: Code characteristics of a vulnerable function with an existing exploit","2016","CODASPY 2016 - Proceedings of the 6th ACM Conference on Data and Application Security and Privacy","","","","97","104","7","","10.1145/2857705.2857750","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964881401&doi=10.1145%2f2857705.2857750&partnerID=40&md5=f9d9ef09cdcca726ffd3c25799a4d1df","Not all vulnerabilities are equal. Some recent studies have shown that only a small fraction of vulnerabilities that have been reported has actually been exploited. Since finding and addressing potential vulnerabilities in a program can take considerable time and effort, recently effort has been made to identify code that is more likely to be vulnerable. This paper tries to identify the attributes of the code containing a vulnerability that makes the code more likely to be exploited. We examine 183 vulnerabilities from the National Vulnerability Database for Linux Kernel and Apache HTTP server. These include eighty-two vulnerabilities that have been found to have an exploit according to the Exploit Database. We characterize the vulnerable functions that have no exploit and the ones that have an exploit using eight metrics. The results show that the difference between a vulnerability that has no exploit and the one that has an exploit can potentially be characterized using the chosen software metrics. However, predicting exploitation of vulnerabilities is more complex than predicting just the presence of vulnerabilities and further research is needed using metrics that consider security domain knowledge for enhancing the predictability of vulnerability exploits. © 2016 ACM.","Data mining and machine learning; Exploitability; Exploits; Feature selection; Prediction; Software metrics; Software security; Vulnerabilities severity","Artificial intelligence; Codes (symbols); Computer operating systems; Data mining; Data privacy; Feature extraction; Forecasting; Learning systems; Exploitability; Exploits; Software metrics; Software security; Vulnerabilities severity; Security of data","Conference paper","Final","","Scopus","2-s2.0-84964881401"
"Luckow K.; Kersten R.; Pasareanu C.","Luckow, Kasper (54383756000); Kersten, Rody (36632338300); Pasareanu, Corina (57204255037)","54383756000; 36632338300; 57204255037","Symbolic Complexity Analysis Using Context-Preserving Histories","2017","Proceedings - 10th IEEE International Conference on Software Testing, Verification and Validation, ICST 2017","","","7927963","58","68","10","","10.1109/ICST.2017.13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020744529&doi=10.1109%2fICST.2017.13&partnerID=40&md5=07ec341ba15a230ba1f4bc39bef7935f","We propose a technique based on symbolic execution for analyzing the algorithmic complexity of programs. The technique uses an efficient guided analysis to compute bounds on the worst-case complexity (for increasing input sizes) and to generate test values that trigger the worst-case behaviors. The resulting bounds are fitted to a function to obtain a prediction of the worst-case program behavior at any input sizes. Comparing these predictions to the programmers' expectations or to theoretical asymptotic bounds can reveal vulnerabilities or confirm that a program behaves as expected. To achieve scalability we use path policies to guide the symbolic execution towards worst-case paths. The policies are learned from the worst-case results obtained with exhaustive exploration at small input sizes and are applied to guide exploration at larger input sizes, where un-guided exhaustive exploration is no longer possible. To achieve precision we use path policies that take into account the history of choices made along the path when deciding which branch to execute next in the program. Furthermore, the history computation is context-preserving, meaning that the decision for each branch depends on the history computed with respect to the enclosing method. We implemented the technique in the Symbolic PathFinder tool. We show experimentally that it can find vulnerabilities in complex Java programs and can outperform established symbolic techniques. © 2017 IEEE.","Complexity Analysis; Guided Exploration; Symbolic Execution","Computational complexity; Computer software; Model checking; Parallel processing systems; Verification; Algorithmic complexity; Asymptotic bounds; Complexity analysis; Program behavior; Symbolic execution; Symbolic techniques; Worst-case behaviors; Worst-case complexity; Software testing","Conference paper","Final","","Scopus","2-s2.0-85020744529"
"Zhu X.; Cao C.; Zhang J.","Zhu, Xiaoling (55696710600); Cao, Chenglong (57194013467); Zhang, Jing (56023445400)","55696710600; 57194013467; 56023445400","Vulnerability severity prediction and risk metric modeling for software","2017","Applied Intelligence","47","3","","828","836","8","","10.1007/s10489-017-0925-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018284367&doi=10.1007%2fs10489-017-0925-0&partnerID=40&md5=d87bda92f23da2183e505fea0eafbd68","As more users suffer serious security threats from software vulnerabilities, software security becomes increasingly important. Vulnerability prediction and risk evaluation are two of the most concerning issues in software security management. In this paper, we propose a prediction model for software vulnerability in which the probability and severity of vulnerability occurrence are determined by the logistic function and binomial distribution, respectively. Using the parameters obtained by prediction, we developed a new risk metric model. We provided some metrics, including mean time to vulnerability, local risk rate, mean risk rate, and overall risk value, from the viewpoint of time and probability. Experiments were conducted on real software vulnerability datasets. The results show that the prediction is effective and the evaluation is easy to operate. Our work has several features: (1) users can predict the vulnerability state in the future, in particular, vulnerability severity; (2) unlike traditional evaluation methods with expert scoring, our evaluation model is based on prediction and uses historical vulnerability data; and (3) the risk metric value can be used in risk assessment, security rating, and patch management. © 2017, Springer Science+Business Media New York.","Prediction model; Risk metric; Software vulnerability; Vulnerability severity","Forecasting; Probability distributions; Risk assessment; Binomial distribution; Evaluation methods; Evaluation modeling; Logistic functions; Prediction model; Risk metric; Software vulnerabilities; Vulnerability severity; Risks","Article","Final","","Scopus","2-s2.0-85018284367"
"Krishnaveni S.; Sathiyakumari K.","Krishnaveni, S. (58364124600); Sathiyakumari, K. (56565484000)","58364124600; 56565484000","SpiderNet: An interaction tool for predicting malicious web pages","2015","2014 International Conference on Information Communication and Embedded Systems, ICICES 2014","","","7033878","","","","","10.1109/ICICES.2014.7033878","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925337920&doi=10.1109%2fICICES.2014.7033878&partnerID=40&md5=a83f68123d47d0efa51e3b21a8f57aa0","Malicious code injection poses a serious security issue over the Internet or over the web application. In malicious code injection attacks, hackers can take advantage of defectively coded web application software to initiate malicious code into the organization's systems and network. The vulnerability persevere when a web application do not properly sanitize the data entered by the user on a web page. Attacker can steal confidential data of the user like password, pin number, and etc., these attacks resulting defeat of market value of the organization. This research work is model the malicious web page prediction as a classification task and provides a convenient solution by using a powerful machine learning technique such as Support Vector Machine (SVM), Extreme Learning Machine (ELM). The main aim of this research work is to predict the type of the malicious attack like Redirect, Script injection and XSS using the machine learning approaches; in this case, the prediction time is taken into consideration. The supervised learning algorithms such as SVM and ELM are employed for implementing the prediction model. © 2014 IEEE.","ELM; Malicious code; Redirect; Script Injection; SpiderNet; SVM; XSS","Application programs; Codes (symbols); Embedded systems; Forecasting; Machine learning; Malware; Network security; Personal computing; Support vector machines; Websites; Classification tasks; Extreme learning machine; Machine learning approaches; Machine learning techniques; Malicious codes; Malicious web pages; Redirect; SpiderNet; Learning algorithms","Conference paper","Final","","Scopus","2-s2.0-84925337920"
"Scandariato R.; Walden J.; Hovsepyan A.; Joosen W.","Scandariato, Riccardo (23095243000); Walden, James (22036834700); Hovsepyan, Aram (14056192200); Joosen, Wouter (57202521929)","23095243000; 22036834700; 14056192200; 57202521929","Predicting vulnerable software components via text mining","2014","IEEE Transactions on Software Engineering","40","10","6860243","993","1006","13","","10.1109/TSE.2014.2340398","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908054379&doi=10.1109%2fTSE.2014.2340398&partnerID=40&md5=19e8fdefbdae728d2ff47f3f8d8926a9","This paper presents an approach based on machine learning to predict which components of a software application contain security vulnerabilities. The approach is based on text mining the source code of the components. Namely, each component is characterized as a series of terms contained in its source code, with the associated frequencies. These features are used to forecast whether each component is likely to contain vulnerabilities. In an exploratory validation with 20 Android applications, we discovered that a dependable prediction model can be built. Such model could be useful to prioritize the validation activities, e.g., to identify the components needing special scrutiny. © 2014 IEEE.","machine learning; prediction model; Vulnerabilities","Prediction model; Software component; Text mining; Vulnerabilities; Learning systems","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84908054379"
"Tang Y.; Zhao F.; Yang Y.; Lu H.; Zhou Y.; Xu B.","Tang, Yaming (57188655449); Zhao, Fei (57188652229); Yang, Yibiao (55883652400); Lu, Hongmin (55542420500); Zhou, Yuming (57022538800); Xu, Baowen (7404589262)","57188655449; 57188652229; 55883652400; 55542420500; 57022538800; 7404589262","Predicting Vulnerable Components via Text Mining or Software Metrics? An Effort-Aware Perspective","2015","Proceedings - 2015 IEEE International Conference on Software Quality, Reliability and Security, QRS 2015","","","7272911","27","36","9","","10.1109/QRS.2015.15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962082211&doi=10.1109%2fQRS.2015.15&partnerID=40&md5=f93a6d208c05bc894b990739b121cd17","In order to identify vulnerable software components, developers can take software metrics as predictors or use text mining techniques to build vulnerability prediction models. A recent study reported that text mining based models have higher recall than software metrics based models. However, this conclusion was drawn without considering the sizes of individual components which affects the code inspection effort to determine whether a component is vulnerable. In this paper, we investigate the predictive power of these two kinds of prediction models in the context of effort-aware vulnerability prediction. To this end, we use the same data sets, containing 223 vulnerabilities found in three web applications, to build vulnerability prediction models. The experimental results show that: (1) in the context of effort-aware ranking scenario, text mining based models only slightly outperform software metrics based models, (2) in the context of effort-aware classification scenario, text mining based models perform similarly to software metrics based models in most cases, and (3) most of the effect sizes (i.e. the magnitude of the differences) between these two kinds of models are trivial. These results suggest that, from the viewpoint of practical application, software metrics based models are comparable to text mining based models. Therefore, for developers, software metrics based models are practical choices for vulnerability prediction, as the cost to build and apply these models is much lower. © 2015 IEEE.","effort-aware; prediction; software metrics; text mining; vulnerability","Application programs; Computer software selection and evaluation; Forecasting; Software reliability; Text processing; effort-aware; Individual components; Predictive power; Software component; Software metrics; Text mining; Text mining techniques; vulnerability; Data mining","Conference paper","Final","","Scopus","2-s2.0-84962082211"
"Fredette L.; Beach E.","Fredette, Lee (7801515123); Beach, Elvin (7005262481)","7801515123; 7005262481","Welding simulation used in the design of metallic armor systems","2014","Advanced Materials Research","996","","","518","524","6","","10.4028/www.scientific.net/AMR.996.518","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906544399&doi=10.4028%2fwww.scientific.net%2fAMR.996.518&partnerID=40&md5=c187fdfd3c6323c71ecf47c5f30646c5","Welding steel armor reduces the armor material's protection capability. Several industrial and military welding standards exist for welding armor materials with the primary focus on joint strength rather than ballistic integrity. The Heat Affected Zone (HAZ) created by the welding process introduces vulnerabilities in the protection system. The process and designs that we have demonstrated include mitigation features that eliminate the ballistic degradation and provide uniform protection across all armor materials. In this study we used finite element simulation of the welding process to perform trade studies evaluating welded joint designs, and to show how the designs could be altered to both optimize armor performance and reduce welding heat input. Beneficial effects of reduced heat input, and the corresponding reduction in welding-induced residual stresses, created an overall reduction in distortion in the assembly and improvement of the armor performance. The simulated welding process included the creation of the heat affected zone and the development of residual stresses in the structure. ABAQUS finite element software was used for the simulation with the aid of an extensive material property database created over the wide range of welding temperatures. The finite element simulation predictions were validated and verified with excellent results by metallography and micro-hardness measurements. Live-fire ballistic tests were used as the final proof of measurable design improvements. Finite element welding simulation was shown to be an effective tool for improving upon standard welded armor designs, and above all in improving human safety. © (2014) Trans Tech Publications, Switzerland.","Armor; Ballistic testing; Finite element analysis; Metallography; Welding simulation","ABAQUS; Armor; Ballistics; Computer software; Design; Electric welding; Finite element method; Metallography; Microstructure; Residual stresses; Strength of materials; Abaqus finite element software; Ballistic testing; Finite element simulations; Finite element welding simulations; Material property database; Microhardness measurement; Welding simulation; Welding-induced residual stress; Heat affected zone","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-84906544399"
"Shakerizadeh E.","Shakerizadeh, Ebrahim (57189382940)","57189382940","Evaluation of seismic vulnerability of concrete buildings based on different indices of damage (Case Study: Office buildings in Bandar Abbas)","2016","Journal of Engineering and Applied Sciences","11","1","","63","74","11","","10.3923/jeasci.2016.63.74","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969981113&doi=10.3923%2fjeasci.2016.63.74&partnerID=40&md5=1875fcc2977719d610ce58b84917f2bd","Evaluating the vulnerability of existing buildings is in fact a prediction of damage against possible earthquakes. In this regard, the existing building structures vulnerability prediction of possible future earthquakes is one of the most pressing engineering measures due to provide solutions for seismic risk reduction in urban areas. Various constructions with concrete structure in Bandar Abbas which lack the appropriate lateral force resisting systems added to the low quality of implementation, inappropriate quality of building materials and environmental conditions have caused the vulnerability of buildings against earthquakes. Consequently, recognizing structural weaknesses and evaluating them against earthquakes is of great importance for the building's improvement. This study aims to achieve the recognition of the existing state office buildings in Bandar Abbas from the perspective of seismic resistance and seeks to assess the quantitative and qualitative vulnerability of two office buildings using different indices of damages and tries to answer the question whether the implemented concrete structure of the office buildings in Bandar Abbas have a good performance against earthquake or not. Accordingly, technical specifications and implementation plans for the two office buildings were collected including administrative plans, executive details and laboratory sheets and then the vulnerability of buildings were assessed by the modeling in Etabs and Sap 2000 Software using non-linear analysis (static and dynamic) and using qualitative evaluation methods of Aria, Canada by laws and ATC-21 as well as a quantitative assessment methods of Raphael-Mir, Suzan, Newmark and Rosen Blow. The results showed that from the 848 proposed joints in the public library building in Bandar Abbas, 24 joints are on the verge of Collapse (CP) and 14 joints are in the range of Life Safety (LS). In civil status registration organization building in Bandar Abbas, there are 236 proposed joint in the total structure of the building among which 11 jointsare on the verge of Collapse (CP) and 3 jointsare in the range of Life Safety (LS). The results of the vulnerability of buildings using Raphael-Mir, Suzan, Newmark and Rosen Blow index show that the public library building in Bandar Abbas in these methods (the design-based earthquake in the general bylaws of buildings against earthquake) will be totally damaged and civil status registration organization building will also be totally and generally damaged in these methods in the life safety and general vulnerability. © Medwell Journals, 2016.","Bandar Abbas; Concrete buildings; Damage index; Perspective; Seismic vulnerability","","Article","Final","","Scopus","2-s2.0-84969981113"
"Theisen C.; Herzig K.; Morrison P.; Murphy B.; Williams L.","Theisen, Christopher (57216482869); Herzig, Kim (35078523000); Morrison, Patrick (55800525800); Murphy, Brendan (7402698081); Williams, Laurie (35565101900)","57216482869; 35078523000; 55800525800; 7402698081; 35565101900","Approximating Attack Surfaces with Stack Traces","2015","Proceedings - International Conference on Software Engineering","2","","7202964","199","208","9","","10.1109/ICSE.2015.148","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951760648&doi=10.1109%2fICSE.2015.148&partnerID=40&md5=0b57b5e1837e7061a1229293977a9adf","Security testing and reviewing efforts are a necessity for software projects, but are time-consuming and expensive to apply. Identifying vulnerable code supports decision-making during all phases of software development. An approach for identifying vulnerable code is to identify its attack surface, the sum of all paths for untrusted data into and out of a system. Identifying the code that lies on the attack surface requires expertise and significant manual effort. This paper proposes an automated technique to empirically approximate attack surfaces through the analysis of stack traces. We hypothesize that stack traces from user-initiated crashes have several desirable attributes for measuring attack surfaces. The goal of this research is to aid software engineers in prioritizing security efforts by approximating the attack surface of a system via stack trace analysis. In a trial on Windows 8, the attack surface approximation selected 48.4% of the binaries and contained 94.6% of known vulnerabilities. Compared with vulnerability prediction models (VPMs) run on the entire codebase, VPMs run on the attack surface approximation improved recall from.07 to.1 for binaries and from.02 to.05 for source files. Precision remained at.5 for binaries, while improving from.5 to.69 for source files. © 2015 IEEE.","Attack surface; Models; Reliability; Security; Stack traces; Testing; Vulnerability","Bins; Codes (symbols); Decision making; Models; Reliability; Software design; Software engineering; Testing; Trace analysis; Automated techniques; Prediction model; Security; Security testing; Software project; Stack traces; Surface approximation; Vulnerability; Software testing","Conference paper","Final","","Scopus","2-s2.0-84951760648"
"Iervolino I.; Baltzopoulos G.; Vamvatsikos D.; Baraschino R.","Iervolino, Iunio (6506755474); Baltzopoulos, Georgios (54950198700); Vamvatsikos, Dimitrios (8213069800); Baraschino, Roberto (57191956596)","6506755474; 54950198700; 8213069800; 57191956596","SPO2FRAG V1.0: Software for pushover-based derivation of seismic fragility curves","2016","ECCOMAS Congress 2016 - Proceedings of the 7th European Congress on Computational Methods in Applied Sciences and Engineering","3","","","5962","5976","14","","10.7712/100016.2233.11553","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995495873&doi=10.7712%2f100016.2233.11553&partnerID=40&md5=0625ad1bbdf838dbd1581cd29eeeac44","This article presents SPO2FRAG V1.0, the first (beta) version of the Static PushOver to FRAGility software. The SPO2FRAG software is an interactive and user-friendly tool that can be used for approximate, computer-aided calculation of building seismic fragility functions, based on static pushover analysis. It is coded in MATLAB® environment and is currently under development at the Department of Structures for Engineering and Architecture of the University of Naples Federico II. At the core of the SPO2FRAG tool lies the SPO2IDA algorithm, which permits analytical predictions for incremental dynamic analysis summary fractiles at the single-degree-of-freedom system level. By effectively interfacing SPO2IDA with a series of operations, intended to link the results of static pushover analysis with the variability that typically characterizes non-linear dynamic structural response, SPO2FRAG provides an expedient solution to the computationally demanding task of analytically evaluating seismic building fragility, which would otherwise require a large number of non-linear dynamic analyses.","Incremental dynamic analysis; Loss assessment; Seismic risk; Vulnerability","Computational methods; Degrees of freedom (mechanics); Dynamic analysis; MATLAB; Risk assessment; Seismology; Structural analysis; Computer aided calculation; Incremental dynamic analysis; Non-linear dynamic analysis; Seismic fragility curves; Seismic risk; Single degree of freedom systems; Static pushover analysis; Vulnerability; Computer aided analysis","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-84995495873"
"Zhang S.; Ou X.; Caragea D.","Zhang, Su (42263160500); Ou, Xinming (25651998800); Caragea, Doina (35615386800)","42263160500; 25651998800; 35615386800","Predicting Cyber Risks through National Vulnerability Database","2015","Information Security Journal","24","4-6","","194","206","12","","10.1080/19393555.2015.1111961","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949623802&doi=10.1080%2f19393555.2015.1111961&partnerID=40&md5=0a7c13b9dc6daea355fcd7a95ac1b94e","Software vulnerabilities are the major cause of cyber security problems. The National Vulnerability Database (NVD) is a public data source that maintains standardized information about reported software vulnerabilities. Since its inception in 1997, NVD has published information about more than 43,000 software vulnerabilities affecting more than 17,000 software applications. This information is potentially valuable in understanding trends and patterns in software vulnerabilities so that one can better manage the security of computer systems that are pestered by the ubiquitous software security flaws. In particular, one would like to be able to predict the likelihood that a piece of software contains a yet-to-be-discovered vulnerability, which must be taken into account in security management due to the increasing trend in zero-day attacks. We conducted an empirical study on applying data-mining techniques on NVD data with the objective of predicting the time to next vulnerability for a given software application. We experimented with various features constructed using the information available in NVD and applied various machine learning algorithms to examine the predictive power of the data. Our results show that the data in NVD generally have poor prediction capability, with the exception of a few vendors and software applications. We suggest possible reasons for why the NVD data have not produced a reasonable prediction model for time to next vulnerability with our current approach, and suggest alternative ways in which the data in NVD can be used for the purpose of risk estimation. Copyright © Taylor & Francis Group, LLC.","Risk assessment; Zero-day vulnerability","Application programs; Artificial intelligence; Data mining; Forecasting; Learning algorithms; Learning systems; Network security; Risk assessment; Risk perception; Security of data; National vulnerability database; Prediction capability; Public data source; Security management; Software applications; Software security; Software vulnerabilities; Zero day vulnerabilities; Information management","Article","Final","","Scopus","2-s2.0-84949623802"
"Kanashiro L.; Ribeiro A.; Silva D.; Meirelles P.; Terceiro A.","Kanashiro, Lucas (57195316034); Ribeiro, Athos (57212280216); Silva, David (57195316807); Meirelles, Paulo (54975098500); Terceiro, Antonio (36997575800)","57195316034; 57212280216; 57195316807; 54975098500; 36997575800","A study on low complexity models to predict flaws in the Linux source code","2017","Iberian Conference on Information Systems and Technologies, CISTI","","","7975747","","","","","10.23919/CISTI.2017.7975747","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027059238&doi=10.23919%2fCISTI.2017.7975747&partnerID=40&md5=d412693663c52bb62309d5c5019d3899","Due to the constant evolution of technology, each day brings new programming languages, development paradigms, and ways of evaluating processes. This is no different with source code metrics, where there is always new metric classes. To use a software metric to support decisions, it is necessary to understand how to perform the metric collection, calculation, interpretation, and analysis. The tasks of collecting and calculating source code metrics are most often automated, but how should we monitor them during the software development cycle? Our research aims to assist the software engineer to monitor metrics of vulnerability threats present in the source code through a reference prediction model, considering that real world software have non-functional security requirements, which implies the need to know how to monitor these requirements during the software development cycle. As a first result, this paper presents an empirical study on the evolution of the Linux project. Based on static analysis data, we propose low complexity models to study flaws in the Linux source code. About 391 versions of the project were analyzed by mining the official Linux repository using an approach that can be reproduced to perform similar studies. Our results show that it is possible to predict the number of warnings triggered by a static analyzer for a given software project revision as long as the software is continuously monitored. © 2017 AISTI.","Common Weakness Enumeration; Linux; Prediction; Source Code Metrics; Source Code Static Analysis","Codes (symbols); Computer operating systems; Computer programming languages; Forecasting; Information systems; Linux; Software design; Technology transfer; Common Weakness Enumeration; Empirical studies; Evolution of technology; Security requirements; Software development cycles; Software metrices; Source code metrics; Source code static analysis; Static analysis","Conference paper","Final","","Scopus","2-s2.0-85027059238"
"Maio R.; Vicente R.; Formisano A.; Varum H.","Maio, Rui (56487331100); Vicente, Romeu (23020145000); Formisano, Antonio (54421738200); Varum, Humberto (23135674700)","56487331100; 23020145000; 54421738200; 23135674700","Seismic vulnerability of building aggregates through hybrid and indirect assessment techniques","2015","Bulletin of Earthquake Engineering","13","10","","2995","3014","19","","10.1007/s10518-015-9747-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941423478&doi=10.1007%2fs10518-015-9747-9&partnerID=40&md5=17c67472ef1cd2a987fe0dbd52e72ffd","This work approaches the seismic vulnerability assessment of an old stone masonry building aggregate, located in San Pio delle Camere (Abruzzo, Italy), slightly affected by the 2009 April 6th earthquake occurred in L’Aquila and its districts. This building aggregate has been modelled by using the 3muri® software for seismic analysis of masonry constructions. On one hand, static non-linear numerical analyses were performed to obtain capacity curves together with the prediction of damage distributions for the input seismic action (hybrid technique). On the other hand, indirect techniques, based on different vulnerability index formulations, were used for assessing the building aggregate’s behaviour under earthquake action. The activities carried out have provided a clear framework on the seismic vulnerability of building aggregates, as well as aid future retrofitting interventions. © 2015, Springer Science+Business Media Dordrecht.","Building aggregates; Damage distributions; Fragility curves; Macro-elements; Seismic vulnerability; Vulnerability index","Abruzzi; Italy; Aquila; Aggregates; Buildings; Masonry construction; Masonry materials; Seismology; Damage distribution; Fragility curves; Macro element; Seismic vulnerability; Vulnerability index; aggregate; masonry; numerical model; seismic method; seismic response; software; vulnerability; Earthquakes","Article","Final","","Scopus","2-s2.0-84941423478"
"Geng J.; Ye D.; Luo P.","Geng, Jinkun (57023035600); Ye, Daren (57022987800); Luo, Ping (55705278900)","57023035600; 57022987800; 55705278900","Forecasting severity of software vulnerability using grey model GM(1,1)","2016","Proceedings of 2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference, IAEAC 2015","","","7428572","344","348","4","","10.1109/IAEAC.2015.7428572","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966330720&doi=10.1109%2fIAEAC.2015.7428572&partnerID=40&md5=de84c1e9748b8b49cee446b7c68d7267","Vulnerabilities usually represents the risk level of software, and it is of high value to forecast vulnerabilities so as to evaluate the security level of software. Current researches mainly focus on predicting the number of vulnerabilities or the occurrence time of vulnerabilities, however, to our best knowledge, there are no other researches focusing on the prediction of vulnerabilities' severity, which we think is an important aspect reflecting vulnerabilities and software security. To compensate for this deficiency, we borrows the grey model GM(1,1) from grey system theory to forecast the severity of vulnerabilities. The experiment is carried on the real data collected from CVE and proves the feasibility of our predicting method. © 2015 IEEE.","1); GM(1; grey theory; prediction; severity; software security; vulnerability","Forecasting; System theory; GM(1; Grey theory; severity; Software security; vulnerability; Security of data","Conference paper","Final","","Scopus","2-s2.0-84966330720"
"Munaiah N.; Meneely A.","Munaiah, Nuthan (57190685740); Meneely, Andrew (23135193600)","57190685740; 23135193600","Beyond the attack surface assessing security risk with random walks on call graphs","2016","SPRO 2016 - Proceedings of the 2016 ACM Workshop on Software PROtection, co-located with CCS 2016","","","","3","14","11","","10.1145/2995306.2995311","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002083583&doi=10.1145%2f2995306.2995311&partnerID=40&md5=c0e9667f89935b47c8b3cb85c25f3a4c","When reasoning about software security, researchers and practitioners use the phrase \attack surface"" as a metaphor for risk. Enumerate and minimize the ways attackers can break in then risk is reduced and the system is better protected, the metaphor says. But software systems are much more complicated than their surfaces. We propose functionand file-level attack surface metrics|proximity and risky walk|that enable fine-grained risk assessment. Our risky walk metric is highly configurable: we use PageRank on a probability-weighted call graph to simulate attacker behavior of finding or exploiting a vulnerability. We provide evidence-based guidance for deploying these metrics, including an extensive parameter tuning study. We conducted an empirical study on two large open source projects, FFmpeg and Wireshark, to investigate the potential correlation between our metrics and historical post-release vulnerabilities. We found our metrics to be statistically significantly associated with vulnerable functions/files with a small-to-large Cohen's d effect size. Our prediction model achieved an increase of 36% (in FFmpeg) and 27% (in Wireshark) in the average value of F2-measure over a base model built with SLOC and coupling metrics. Our prediction model outperformed comparable models from prior literature with notable improvements: 58% reduction in false negative rate, 81% reduction in false positive rate, and 548% increase in F2-measure. These metrics advance vulnerability prevention by (a) being exible in terms of granularity, (b) performing better than vulnerability prediction literature, and (c) being tunable so that practitioners can tailor the metrics to their products and better assess security risk. © 2016 ACM.","Attack surface; Metric; Page rank; Risk; Vulnerability","Computer software selection and evaluation; Forecasting; Open source software; Risk assessment; XML; False negative rate; False positive rates; Metric; Open source projects; Page ranks; Software security; Vulnerability; Vulnerability prevention; Risks","Conference paper","Final","","Scopus","2-s2.0-85002083583"
"Bullough B.L.; Yanchenko A.K.; Smith C.L.; Zipkin J.R.","Bullough, Benjamin L. (57162960800); Yanchenko, Anna K. (57219478651); Smith, Christopher L. (57207223589); Zipkin, Joseph R. (56199320200)","57162960800; 57219478651; 57207223589; 56199320200","Predicting exploitation of disclosed software vulnerabilities using open-source data","2017","IWSPA 2017 - Proceedings of the 3rd ACM International Workshop on Security and Privacy Analytics, co-located with CODASPY 2017","","","","45","53","8","","10.1145/3041008.3041009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018306749&doi=10.1145%2f3041008.3041009&partnerID=40&md5=8df3f8dea42b85d13888e408668b7d42","Each year, thousands of software vulnerabilities are discovered and reported to the public. Unpatched known vulnerabilities are a significant security risk. It is imperative that software vendors quickly provide patches once vulnerabilities are known and users quickly install those patches as soon as they are available. However, most vulnerabilities are never actually exploited. Since writing, testing, and installing software patches can involve considerable resources, it would be desirable to prioritize the remediation of vulnerabilities that are likely to be exploited. Several published research studies have reported moderate success in applying machine learning techniques to the task of predicting whether a vulnerability will be exploited. These approaches typically use features derived from vulnerability databases (such as the summary text describing the vulnerability) or social media posts that mention the vulnerability by name. However, these prior studies share multiple methodological shortcomings that in-flate predictive power of these approaches. We replicate key portions of the prior work, compare their approaches, and show how selection of training and test data critically affect the estimated performance of predictive models. The results of this study point to important methodological considerations that should be taken into account so that results reflect real-world utility. © 2017 ACM.","Exploit prediction; Machine learning; Software vulnerabilities","Artificial intelligence; Forecasting; Learning systems; Open systems; Software testing; Considerable resources; Machine learning techniques; Open source datum; Predictive models; Predictive power; Real world utility; Software vulnerabilities; Vulnerability database; Open source software","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85018306749"
"Tang M.; Alazab M.; Luo Y.","Tang, Mingjian (57215896761); Alazab, Mamoun (36661792200); Luo, Yuxiu (24478024900)","57215896761; 36661792200; 24478024900","Exploiting vulnerability disclosures: Statistical framework and case study","2016","Proceedings - 2016 Cybersecurity and Cyberforensics Conference, CCC 2016","","","7600221","117","122","5","","10.1109/CCC.2016.10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994877156&doi=10.1109%2fCCC.2016.10&partnerID=40&md5=71360fce70510dcca53a5d004e55e624","With an ever-increasing trend of cybercrimes and incidents due to software vulnerabilities and exposures, effective and proactive vulnerability management becomes imperative in modern organisations regardless large or small. Forecasting models leveraging rich historical vulnerability disclosure data undoubtedly provide important insights to inform the cyber community with the anticipated risks. In this paper, we proposed a novel framework for statistically analysing long-Term vulnerability time series between January 1999 and January 2016. By utilising this sound framework, we initiated an important study on not only testing but also modelling persistent volatilities in the data. In sharp contrast to the existing models, we consider capturing both mean and conditional variance latent in the disclosure series. Through extensive empirical studies, a composite model is shown to effectively capture the sporadic nature of vulnerability time series. In addition, this paper paves the way for further study on the stochastic perspective of cyber vulnerability proliferation towards more accurate prediction models and better risk management. © 2016 IEEE.","Cyber security; Generalised autoregressive conditional heteroskedasticity; Time series; Volatility; Vulnerability","Risk management; Stochastic systems; Time series; Autoregressive conditional heteroskedasticity; Cyber security; Software vulnerabilities; Statistical framework; Volatility; Vulnerability; Vulnerability disclosure; Vulnerability management; Stochastic models","Conference paper","Final","","Scopus","2-s2.0-84994877156"
"Shar L.K.; Briand L.C.; Tan H.B.K.","Shar, Lwin Khin (36802571200); Briand, Lionel C. (7006613079); Tan, Hee Beng Kuan (7403011293)","36802571200; 7006613079; 7403011293","Web Application Vulnerability Prediction Using Hybrid Program Analysis and Machine Learning","2015","IEEE Transactions on Dependable and Secure Computing","12","6","6963442","688","707","19","","10.1109/TDSC.2014.2373377","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959283014&doi=10.1109%2fTDSC.2014.2373377&partnerID=40&md5=e64a056f2cebaef93063019958a56170","Due to limited time and resources, web software engineers need support in identifying vulnerable code. A practical approach to predicting vulnerable code would enable them to prioritize security auditing efforts. In this paper, we propose using a set of hybrid (static+dynamic) code attributes that characterize input validation and input sanitization code patterns and are expected to be significant indicators of web application vulnerabilities. Because static and dynamic program analyses complement each other, both techniques are used to extract the proposed attributes in an accurate and scalable way. Current vulnerability prediction techniques rely on the availability of data labeled with vulnerability information for training. For many real world applications, past vulnerability data is often not available or at least not complete. Hence, to address both situations where labeled past data is fully available or not, we apply both supervised and semi-supervised learning when building vulnerability predictors based on hybrid code attributes. Given that semi-supervised learning is entirely unexplored in this domain, we describe how to use this learning scheme effectively for vulnerability prediction. We performed empirical case studies on seven open source projects where we built and evaluated supervised and semi-supervised models. When cross validated with fully available labeled data, the supervised models achieve an average of 77 percent recall and 5 percent probability of false alarm for predicting SQL injection, cross site scripting, remote code execution and file inclusion vulnerabilities. With a low amount of labeled data, when compared to the supervised model, the semi-supervised model showed an average improvement of 24 percent higher recall and 3 percent lower probability of false alarm, thus suggesting semi-supervised learning may be a preferable solution for many real world applications where vulnerability data is missing. © 2015 IEEE.","empirical study; input validation and sanitization; program analysis; security measures; Vulnerability prediction","Codes (symbols); Errors; Forecasting; Learning algorithms; Machine learning; Network security; Open source software; Supervised learning; Dynamic program analysis; Empirical studies; Probability of false alarm; Program analysis; Sanitization; Security measure; Semi- supervised learning; Web application vulnerability; Application programs","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84959283014"
"Wei S.; Du X.; Hu C.; Shan C.","Wei, Shengjun (8952204800); Du, Xiaojiang (8371278000); Hu, Changzhen (7404569966); Shan, Chun (56102670900)","8952204800; 8371278000; 7404569966; 56102670900","Predicting vulnerable software components using software network graph","2017","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10581 LNCS","","","280","290","10","","10.1007/978-3-319-69471-9_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034251228&doi=10.1007%2f978-3-319-69471-9_21&partnerID=40&md5=513ab38ccca887e67eea882a50e01e4c","Vulnerability Prediction Models (VPMs) are used to predict vulnerability-prone modules and now many software security metrics have been proposed. In this paper, we predict vulnerability-prone components. Based on software network graph we define component cohesion and coupling metrics which are used as security metrics to build the VPM. To validate the prediction performance, we conduct an empirical study on Firefox 3.6. We compare the results with other works’, it shows that our model has a good performance in the accuracy, precision, and recall, and indicate that the proposed metrics are also effective in vulnerability prediction. © 2017, Springer International Publishing AG.","Component cohesion and coupling; Software network; Software security; Vulnerability prediction","Adhesion; Computers; Forecasting; Cohesion and couplings; Empirical studies; Prediction model; Prediction performance; Security metrics; Software component; Software network; Software security; Network security","Conference paper","Final","","Scopus","2-s2.0-85034251228"
"Pang Y.; Xue X.; Wang H.","Pang, Yulei (56131822700); Xue, Xiaozhen (37666341100); Wang, Huaying (57195065734)","56131822700; 37666341100; 57195065734","Predicting vulnerable software components through deep neural network","2017","ACM International Conference Proceeding Series","Part F128535","","","6","10","4","","10.1145/3094243.3094245","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025128128&doi=10.1145%2f3094243.3094245&partnerID=40&md5=942b9844cfbe0523c0da4e04247d9df6","Vulnerabilities need to be detected and removed from software. Although previous studies demonstrated the usefulness of employing prediction techniques in deciding about vulnerabilities of software components, the improvement of effectiveness of these prediction techniques is still a grand challenging research question. This paper employed a technique based on a deep neural network with rectifier linear units trained with stochastic gradient descent method and batch normalization, for predicting vulnerable software components. The features are defined as continuous sequences of tokens in source code files. Besides, a statistical feature selection algorithm is then employed to reduce the feature and search space. We evaluated the proposed technique based on some Java Android applications, and the results demonstrated that the proposed technique could predict vulnerable classes, i.e., software components, with high precision, accuracy and recall. © 2017 Association for Computing Machinery.","Android; Deep learning; Neural network; Vulnerability prediction","Android (operating system); Application programs; Communication channels (information theory); Computer software; Deep learning; Education; Forecasting; Java programming language; Network security; Neural networks; Stochastic systems; Android; Android applications; Continuous sequences; Prediction techniques; Research questions; Software component; Statistical features; Stochastic gradient descent method; Deep neural networks","Conference paper","Final","","Scopus","2-s2.0-85025128128"
"Subbulakshmi S.; Ramar K.; Arya Krishnan R.; Divya S.","Subbulakshmi, S. (57191373145); Ramar, K. (7004225698); Arya Krishnan, R. (57191370274); Divya, S. (57220368266)","57191373145; 7004225698; 57191370274; 57220368266","Web services QoS prediction basedon dynamic non-functional quality factors and WS-security policy specification of web service","2016","International Journal of Control Theory and Applications","9","10","","4591","4601","10","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989231093&partnerID=40&md5=8493e49a7f7e65ad67def77ad2194066","Web service are basically software components that support interoperable interaction between machines over a network. With booming number of WS, it's difficult for users to identify the best quality services. Existing researchers focus mainly on selection of WS based on the functional requirements. This paper proposes a system to select an optimal WS by predicting its QoS value based on dynamic non-functional quality factors response time, throughput and the static factor security of the WS. Prediction result can be used in recommendation systems to select services with optimal QoS performance among a large volume of service candidates. Users and web services are clustered to make prediction of response time and throughput of WS. Security specifications of the web service and their vulnerabilities are used for prediction of security factor of the WS. Finally, QoS of the WS is predicted by aggregation of the predicted quality values for security, response time and throughput. The QoS prediction of the system reveals optimal results as it takes both dynamic and static quality factors of WS. © International Science Press.","Dynamic quality factors; QoS prediction; Static quality factor; Web service","","Article","Final","","Scopus","2-s2.0-84989231093"
"Qingkun M.; Shameng W.; Chao F.; Chaojing T.","Qingkun, Meng (57192211415); Shameng, Wen (57200139726); Chao, Feng (57200140134); Chaojing, Tang (6505609056)","57192211415; 57200139726; 57200140134; 6505609056","Predicting integer overflow through static integer operation attributes","2017","Proceedings of 2016 5th International Conference on Computer Science and Network Technology, ICCSNT 2016","","","8070143","177","181","4","","10.1109/ICCSNT.2016.8070143","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039908815&doi=10.1109%2fICCSNT.2016.8070143&partnerID=40&md5=a78d4de9ad130891d8f91159f0238530","Integer overflow vulnerability is very difficult to locate and patch. From experience speaking the more complicate the integer operation the more error-prone the program. So in this paper, we come up with a new method to leverage static integer operation attributes to predict integer overflows based on machine learning technique. The static integer operation attributes consist of sink, integer operation accumulation, sanitization and input attributes. Every function of the testing program will be converted to a 10-dimension vector which is fed to several machine learning algorithms to make prediction. Our experiment shows that the method gets a good performance. © 2016 IEEE.","component; integer overflow; machine learning; software security; software vulnerability","Artificial intelligence; Computer networks; E-learning; Forecasting; Learning algorithms; Learning systems; Software testing; Testing; component; Error prones; Integer operations; Integer overflow; Sanitization; Software security; Software vulnerabilities; Testing programs; Integer programming","Conference paper","Final","","Scopus","2-s2.0-85039908815"
"Jindal R.; Malhotra R.; Jain A.","Jindal, Rajni (8698929800); Malhotra, Ruchika (15758058000); Jain, Abha (55470981600)","8698929800; 15758058000; 55470981600","Automated classification of security requirements","2016","2016 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2016","","","7732349","2027","2033","6","","10.1109/ICACCI.2016.7732349","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007400793&doi=10.1109%2fICACCI.2016.7732349&partnerID=40&md5=8d3c3ef8a9259f52267b7d3637f9879f","Requirement engineers are not able to elicit and analyze the security requirements clearly, that are essential for the development of secure and reliable software. Proper identification of security requirements present in the Software Requirement Specification (SRS) document has been a problem being faced by the developers. As a result, they are not able to deliver the software free from threats and vulnerabilities. Thus, in this paper, we intend to mine the descriptions of security requirements present in the SRS document and thereafter develop the classification models. The security-based descriptions are analyzed using text mining techniques and are then classified into four types of security requirements viz. authentication-authorization, access control, cryptography-encryption and data integrity using J48 decision tree method. Corresponding to each type of security requirement, a prediction model has been developed. The effectiveness of the prediction models is evaluated against requirement specifications collected from 15 projects which have been developed by MS students at DePaul University. The result analysis indicated that all the four models have performed very well in predicting their respective type of security requirements. © 2016 IEEE.","Machine learning; Non-functional requirements; Receiver Operating Characteristics; Requirement elicitation; Requirement engineering; Security requirements; Text mining","Access control; Authentication; Data mining; Decision trees; Forecasting; Information retrieval systems; Information science; Interactive computer systems; Learning algorithms; Learning systems; Requirements engineering; Specifications; Trees (mathematics); Non-functional requirements; Receiver operating characteristics; Requirement elicitation; Requirement engineering; Security requirements; Text mining; Cryptography","Conference paper","Final","","Scopus","2-s2.0-85007400793"
"Mathur N.; Asirvadam V.S.; Dass S.C.; Gill B.S.","Mathur, Nirbhay (57189522820); Asirvadam, Vijanth S. (24824003600); Dass, Sarat C. (57203199807); Gill, Balvinder Singh (56487045000)","57189522820; 24824003600; 57203199807; 56487045000","Visualization of dengue incidences using expectation maximization (EM) algorithm","2017","International Conference on Intelligent and Advanced Systems, ICIAS 2016","","","7824047","","","","","10.1109/ICIAS.2016.7824047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011964903&doi=10.1109%2fICIAS.2016.7824047&partnerID=40&md5=1a2347b7f3c5484caaf2e644dd258024","The aim of this study was to use the geographical information system (GIS) to visualize the dengue incidences on a weekly basis in Selangor, Malaysia. Along with the prediction modeling on data using centroid model and distribution model based on K-means and Expectation Maximization (EM) algorithms respectively. The results show that weekly hotspot were mainly concentrated in the central part of Petaling district of Selangor. R-GIS(R software) and clustering algorithm were used for year 2014 with several weeks to develop the relation between the visualization and prediction of reported incidences. The results are validated for a small region (Petaling district of Selangor state) in Malaysia and they showed vulnerability hotspot in visualizing the dengue incidences. Thus, the proposed method is able to localize the nature of dengue incidence which can further be utilized for vector disease controlled process. © 2016 IEEE.","clustering algorithm; Dengue; EM algorithm; GIS; K-means; weekly prediction","Forecasting; Geographic information systems; Maximum principle; Visualization; Centroid models; Controlled process; Dengue; Distribution models; EM algorithms; Expectation-maximization algorithms; K-means; Prediction model; Clustering algorithms","Conference paper","Final","","Scopus","2-s2.0-85011964903"
"Morris-King J.R.","Morris-King, James R. (55803057200)","55803057200","Agent-based ecological risk simulation of malware epidemics in tactical mobile ad HOc networks","2016","Simulation Series","48","9","","189","196","7","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994643761&partnerID=40&md5=3e45c0a3efa76a95f4a3f128d986cf3d","Securing mobile ad hoc networks (MANET) at the tactical edge has proven to be an exigent operational challenge for Net-Centric Warfare operations. As cyberattacks evolve in both sophistication and ubiquity, the task of accurately assessing risk in sensitive networks has grown more time- consuming, costly, and cumbersome. Traditional risk assessment techniques typically focus on assessing the vulnerabilities of individual systems and ignore the complex connections between software, hardware, and users which may span across entire networks. These connections often change dynamically with the respect to the environment, individual behavior, and active mission. The application of ecological risk-Assessment to this complex, adaptive system (CAS) allows for the construction of holistic models of the tactical network ecosystem and its attendant cyber risks. In this work, we propose an agent-based model of ecological risk assessment of a simulated mobile tactical network using the Overview, Design concepts, and Details protocol (ODD). This model leverages a unique metric for dynamic risk assessment ""risk flow"" which allows for the future prediction of cascading failures. © 2016 Society for Modeling & Simulation International (SCS).","Agent simulation; Ecological modeling; Malware epidemic; MANET; Risk assessment; Tactical networks","Autonomous agents; Complex networks; Computational methods; Computer crime; Ecology; Malware; Risk assessment; Agent simulation; Ecological modeling; Ecological risk assessment; MANET; Mobile tactical networks; Net-centric warfare operations; Tactical mobile ad hoc networks; Tactical network; Mobile ad hoc networks","Conference paper","Final","","Scopus","2-s2.0-84994643761"
"Hein D.; Saiedian H.","Hein, Daniel (57213734324); Saiedian, Hossein (7003876484)","57213734324; 7003876484","Predicting attack prone software components using repository mined change metrics","2016","ICISSP 2016 - Proceedings of the 2nd International Conference on Information Systems Security and Privacy","","","","554","563","9","","10.5220/0005812905540563","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968845882&doi=10.5220%2f0005812905540563&partnerID=40&md5=4a6f234913c1dedf5a2090ab81fe9271","Identification of attack-prone entities is a crucial step toward improving the state of information security in modern software based systems. Recent work in the fields of empirical software engineering and defect prediction show promise toward identifying and prioritizing attack prone entities using information extracted from software version control repositories. Equipped with knowledge of the most vulnerable entities, organizations can efficiently allocate resources to more effectively leverage secure software development practices, isolating and expunging vulnerabilities before they are released in production products. Such practices include security reviews, automated static analysis, and penetration testing, among others. Efficiently focusing secure development practices on entities of greatest need can help identify and eliminate vulnerabilities in a more cost effective manner when compared to wholesale application for large products. Copyright © 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.","Information Security; Secure Software Engineering; Software Vulnerability; Vulnerability Prediction Model","Cost effectiveness; Forecasting; Information systems; Information use; Knowledge management; Security of data; Software design; Development practices; Empirical Software Engineering; Penetration testing; Prediction model; Secure software development; Secure software engineering; Software component; Software vulnerabilities; Static analysis","Conference paper","Final","","Scopus","2-s2.0-84968845882"
"Edkrantz M.; Truve S.; Said A.","Edkrantz, Michel (57188766787); Truve, Staffan (6506076832); Said, Alan (27868044600)","57188766787; 6506076832; 27868044600","Predicting Vulnerability Exploits in the Wild","2016","Proceedings - 2nd IEEE International Conference on Cyber Security and Cloud Computing, CSCloud 2015 - IEEE International Symposium of Smart Cloud, IEEE SSC 2015","","","7371532","513","514","1","","10.1109/CSCloud.2015.56","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962863091&doi=10.1109%2fCSCloud.2015.56&partnerID=40&md5=631439971945f3cbdadcbf45f188e143","Every day numerous new vulnerabilities and exploits are reported for a wide variety of different software configurations. There is a big need to be able to quickly assess associated risks and sort out which vulnerabilities that are likely to be exploited in real-world attacks. A small percentage of all vulnerabilities account for almost all the observed attack volume. We use machine learning to make automatic predictions for unseen vulnerabilities based on previous exploit patterns. © 2015 IEEE.","Information Security; Machine Learning; Vulnerability Prediction","Artificial intelligence; Cloud computing; Risk assessment; Security of data; Automatic prediction; Real-world attack; Software configuration; Learning systems","Conference paper","Final","","Scopus","2-s2.0-84962863091"
"Abunadi I.; Alenezi M.","Abunadi, Ibrahim (57189636917); Alenezi, Mamdouh (55854089000)","57189636917; 55854089000","Towards cross project vulnerability prediction in open source web applications","2015","ACM International Conference Proceeding Series","24-26-September-2015","","a42","","","","","10.1145/2832987.2833051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988939381&doi=10.1145%2f2832987.2833051&partnerID=40&md5=baa8eccc49d16362bec2b1617e9511b6","Building secure software is challenging, time-consuming, and expensive. Software vulnerability prediction models that identify vulnerable software components are usually used to focus security efforts, with the aim of helping to reduce the time and effort needed to secure software. Existing vulnerability prediction models use process or product metrics and machine learning techniques to identify vulnerable software components. Cross project vulnerability prediction plays a significant role in appraising the most likely vulnerable software components, specifically for new or inactive projects. Little effort has been spent to deliver clear guidelines on how to choose the training data for project vulnerability prediction. In this work, we present an empirical study aiming at clarifying how useful cross project prediction techniques in predicting software vulnerabilities. Our study employs the classification provided by different machine learning techniques to improve the detection of vulnerable components. We have elaborately compared the prediction performance of five well-known classifiers. The study is conducted on a publicly available dataset of several PHP open source web applications and in the context of cross project vulnerability prediction, which represents one of the main challenges in the vulnerability prediction field. © Copyright 2015 ACM.","Cross-project vulnerability prediction; Data mining; Software quality; Software security","Artificial intelligence; Computer software selection and evaluation; Data mining; Forecasting; Learning algorithms; Learning systems; Machine components; World Wide Web; Empirical studies; Machine learning techniques; Prediction performance; Prediction techniques; Software component; Software Quality; Software security; Software vulnerabilities; Open source software","Conference paper","Final","","Scopus","2-s2.0-84988939381"
"Dureuil L.; Potet M.-L.; de Choudens P.; Dumas C.; Clédière J.","Dureuil, Louis (56225502200); Potet, Marie-Laure (8727223500); de Choudens, Philippe (57188564971); Dumas, Cécile (57208177238); Clédière, Jessy (23007576200)","56225502200; 8727223500; 57188564971; 57208177238; 23007576200","From code review to fault injection attacks: Filling the gap using fault model inference","2016","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9514","","","107","124","17","","10.1007/978-3-319-31271-2_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961654838&doi=10.1007%2f978-3-319-31271-2_7&partnerID=40&md5=2067750f47d9408a0ad0ef205bdab08d","We propose an end-to-end approach to evaluate the robustness of smartcard embedded applications against perturbation attacks. Key to this approach is the fault model inference phase, a method to determine a precise fault model according to the attacked hardware and to the attacker’s equipment, taking into account the probability of occurrence of the faults. Together with a fault injection simulator, it allows to compute a predictive metrics, the vulnerability rate, which gives a first estimation of the robustness of the application. Our approach is backed up by experiments and tools that validate its potential for prediction. © Springer International Publishing Switzerland 2016.","Attack potential; Electromagnetic attacks; Fault injection; Fault model; Perturbation attack; Smartcard; Vulnerability rating","Hardware security; Software testing; Transportation; Attack potential; Electromagnetic attack; Fault injection; Fault model; Perturbation attack; Vulnerability ratings; Smart cards","Conference paper","Final","","Scopus","2-s2.0-84961654838"
"Labunets K.; Janes A.; Felderer M.; Massacci F.","Labunets, Katsiaryna (56022656900); Janes, Andrea (7003421075); Felderer, Michael (24832720900); Massacci, Fabio (55167501300)","56022656900; 7003421075; 24832720900; 55167501300","Teaching predictive modeling to junior software engineers - Seminar format and its evaluation","2017","Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering Companion, ICSE-C 2017","","","7965351","339","340","1","","10.1109/ICSE-C.2017.62","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026746934&doi=10.1109%2fICSE-C.2017.62&partnerID=40&md5=f03dcd4bd182c5e480e34e539e09685c","Due to the increased importance of machine learning in software and security engineering, effective trainings are needed that allow software engineers to learn the required basic knowledge to understand and successfully apply prediction models fast. In this paper, we present a two-days seminar to teach machine learning-based prediction in software engineering and the evaluation ofits learning effects based on Bloom's taxonomy. As a teaching scenario for the practical part, we used a paper reporting a research study on the application ofmachine learning techniques to predict vulnerabilities in the code. The results of the evaluation showed that the seminar is an appropriate format for teaching predictive modeling to software engineers. The participants were very enthusiastic and self-motivated to learn about the topic and the empirical investigation based on Bloom's taxonomy showed positive learning effects on the knowledge, comprehension, application, analysis, and evaluation level. © 2017 IEEE.","Bloom's taxonomy; Empirical Software Engineering; Machine Learning; Predictive models","Artificial intelligence; Blooms (metal); Engineers; Forecasting; Learning systems; Software engineering; Taxonomies; Bloom's taxonomy; Empirical investigation; Empirical Software Engineering; Learning techniques; Prediction model; Predictive modeling; Predictive models; Security engineering; C (programming language)","Conference paper","Final","","Scopus","2-s2.0-85026746934"
"Rivera B.; Kosheleva O.","Rivera, Beverly (56450444800); Kosheleva, Olga (7004231271)","56450444800; 7004231271","How to predict the number of vulnerabilities in a software system: A theoretical justification for an empirical formula","2015","Journal of Uncertain Systems","9","2","","133","138","5","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929156817&partnerID=40&md5=318163110b2fddfac1ca4a2fadc2e5d8","Software systems are ubiquitous in modern life. Every time a vulnerability is discovered in one of the widely use software systems (e.g., in an operating system), a large amount of effort is spent on dealing with this vulnerability. It is therefore desirable to be able to predict the number of vulnerabilities that will be discovered at different future time intervals. There exist empirical formulas which allow us to use the past performance of the software system to provide a reasonably good prediction of this future number. In this paper, we provide a theoretical justification for these empirical formulas. © 2015 World Academic Press, UK. All rights reserved.","Function approximation; Software vulnerabilities","","Article","Final","","Scopus","2-s2.0-84929156817"
"Farooq U.; Myler P.","Farooq, Umar (57193455128); Myler, Peter (7005974494)","57193455128; 7005974494","Efficient computational modelling of carbon fibre reinforced laminated composite panels subjected to low velocity drop-weight impact","2014","Materials and Design","54","","","43","56","13","","10.1016/j.matdes.2013.08.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883528759&doi=10.1016%2fj.matdes.2013.08.004&partnerID=40&md5=16bb98fa95b5244499f7c24d94fd38a6","This paper reports on the actual and virtual low velocity impact response of carbon fibre composite laminates. It utilises the contribution of through-thickness stresses, in the prediction of the onset of internal damage created by this type impact scenario.The paper focuses on the damage imparted by the flat nose impactor since this induces a different type of damage and structural response compared to that of the standard test method of using a round nose impactor.Vulnerability of the fibrous composites to vertical drop-weight impact can result in premature failure which is a major concern in their widespread usage. The topic has been of intense research to design more damage tolerant and resistant materials. However, due to materials' anisotropic and three-dimensional nature and complicated damage mechanisms no standard model could have been achieved. Designers predict consequences of a local impact within the global structural context without full-scale testing.Majority of the existing simulation models neglect through-thickness stresses that are regarded as the major cause of catastrophic failures. Efficient and reliable investigations are required to reduce testing and include through-thickness stresses. Drop-weight impact simulation models were developed herein using ABAQUS™ software. Simulations were carried out to compute in-plane stresses subjected to flat and round nose impacts on laminates of differing thicknesses. These stresses once computed were numerically integrated employing the equilibrium equations to efficiently predict through-thickness stresses. The predicted stresses were then utilised in failure criteria to quantify the coupled and embedded damage. This provides a quick insight into the status and contribution of through-thickness stresses in failure predictions. The computed values were compared to the experimental results and found to be in good agreement. © 2013 Elsevier Ltd.","","ABAQUS; Computer simulation; Drops; Forecasting; Laminated composites; Carbon fibre composites; Catastrophic failures; Computational modelling; Drop-weight impacts; Equilibrium equation; Low velocity impact; Standard test method; Structural response; Damage tolerance","Article","Final","","Scopus","2-s2.0-84883528759"
"Last D.","Last, David (57188556579)","57188556579","Using historical software vulnerability data to forecast future vulnerabilities","2015","Proceedings - 2015 Resilience Week, RSW 2015","","","7287429","120","126","6","","10.1109/RWEEK.2015.7287429","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961644328&doi=10.1109%2fRWEEK.2015.7287429&partnerID=40&md5=722ab5722ae902e30fa4aa1c369c24cc","The field of network and computer security is a never-ending race with attackers, trying to identify and patch software vulnerabilities before they can be exploited. In this ongoing conflict, it would be quite useful to be able to predict when and where the next software vulnerability would appear. The research presented in this paper is the first step towards a capability for forecasting vulnerability discovery rates for individual software packages. This first step involves creating forecast models for vulnerability rates at the global level, as well as the category (web browser, operating system, and video player) level. These models will later be used as a factor in the predictive models for individual software packages. A number of regression models are fit to historical vulnerability data from the National Vulnerability Database (NVD) to identify historical trends in vulnerability discovery. Then, k-NN classification is used in conjunction with several time series distance measurements to select the appropriate regression models for a forecast. 68% and 95% confidence bounds are generated around the actual forecast to provide a margin of error. Experimentation using this method on the NVD data demonstrates the accuracy of these forecasts, as well as the accuracy of the confidence bounds forecasts. Analysis of these results indicates which time series distance measures produce the best vulnerability discovery forecasts. © 2015 IEEE.","cybersecurity; vulnerability discovery model; vulnerability prediction","Distance measurement; Nearest neighbor search; Regression analysis; Security of data; Software packages; Time series; Time series analysis; Confidence bounds; Cyber security; Forecasting vulnerabilities; K-NN classifications; National vulnerability database; Predictive models; Software vulnerabilities; Vulnerability discovery; Forecasting","Conference paper","Final","","Scopus","2-s2.0-84961644328"
"Nichols J.F.; Doyle G.","Nichols, J.F. (56370887500); Doyle, G. (56704628900)","56370887500; 56704628900","Current engineering models and capabilities in the vulnerability assessment and protection option (VAPO) software","2014","Structures Congress 2014 - Proceedings of the 2014 Structures Congress","","","","176","187","11","","10.1061/9780784413357.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934296496&doi=10.1061%2f9780784413357.017&partnerID=40&md5=07c82aaa259cb6d1bad55168be284d1d","In response to real world terrorist threats, the Defense Threat Reduction Agency (DTRA) has developed a fast-running vulnerability assessment software tool, Vulnerability Assessment and Protection Option (VAPO), which is used to characterize installations, facilities, and urban environments. VAPO can assess the vulnerability of an installation by predicting the effects of structural damage, equipment damage, and human injury due to a terrorist attack. For the 2014 Structures Congress, this paper will focus on the blast load predictions and structural response aspects of the program. This paper gives an overview of the current state of engineering calculations used in the VAPO program. The paper describes the individual methodologies of VAPO and illustrates how these pieces integrate into a package that provides capabilities that would otherwise be tedious to produce from independent tools. The process flow for the software program is broken down into four phases: characterization, environment modeling, vulnerability modeling, and results visualization. This paper's description of each phase provides the reader insight into how the program has evolved from solely a vulnerability assessment tool into an integrated software package useful for preliminary R&D problems, first-cut protective design, computational fluid dynamics geometry characterization, and DTRA reachback support for large events. The paper concludes with a discussion of future functionality and current engineering model limitations of the program. © 2014 American Society of Civil Engineers.","","Computational fluid dynamics; Computational geometry; Computer software; Defense threat reduction agencies; Engineering calculation; Engineering modeling; Environment modeling; Results visualization; Vulnerability assessment tool; Vulnerability assessments; Vulnerability models; Terrorism","Conference paper","Final","","Scopus","2-s2.0-84934296496"
"Iglesias C.; Giráldez E.; Taboada J.; Martínez-Alegría R.; Antunes I.M.","Iglesias, Carla (56810715900); Giráldez, Eduardo (25627662700); Taboada, Javier (8677753400); Martínez-Alegría, Roberto (6507676176); Antunes, Isabel Margarida (57226606902)","56810715900; 25627662700; 8677753400; 6507676176; 57226606902","SESGAL software for managing earthquake risk in Galicia","2013","ICSOFT 2013 - Proceedings of the 8th International Joint Conference on Software Technologies","","","","7","14","7","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887065554&partnerID=40&md5=bbf0abfcd122c31d8613f493cfe6d266","According to the laws in place in Spain, the autonomous Community of Galicia (NW Spain) has two zones -Lugo and Ourense- At greater seismic risk. In order to control and minimize the damage to buildings and to population, a Special Civil Protection Plan for Seismic Risk in Galicia (SISMIGAL) has been drawn up, including a software tool specially designed for this purpose. The Galician Seismic Scenario Simulator v1.0 (SESGAL) is based on a geographic information system (GIS) and provided with a comprehensive database of the elements and resources that intervene in the management of an emergency. In addition to the typical functions of GIS, SESGAL incorporates a seismic scenario simulator -which enables the prediction of the effects of an earthquake- And a seismic emergency manager -which provides a tool for addressing the needs of the population in case of a catastrophe. The SESGAL software presented here provides a useful, rapid tool for an effective and efficient response to the damage caused by an earthquake in the Galician territory, managing the means and resources available. Copyright © 2013 SCITEPRESS.","Earthquake; Emergency; Seismic risk; Simulation; Vulnerability","Geographic information systems; Tools; Civil protection; Earthquake risk; Emergency; NW Spain; Seismic risk; Seismic scenarios; Simulation; Vulnerability; Earthquakes","Conference paper","Final","","Scopus","2-s2.0-84887065554"
"Padmanabhuni B.M.; Tan H.B.K.","Padmanabhuni, Bindu Madhavi (54395923600); Tan, Hee Beng Kuan (7403011293)","54395923600; 7403011293","Buffer Overflow Vulnerability Prediction from x86 Executables Using Static Analysis and Machine Learning","2015","Proceedings - International Computer Software and Applications Conference","2","","7273653","450","459","9","","10.1109/COMPSAC.2015.78","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962162171&doi=10.1109%2fCOMPSAC.2015.78&partnerID=40&md5=bb23ae7d6f86488a194a958908461a17","Mining static code attributes for predicting software vulnerabilities has received some attention recently. There are a number of approaches for detecting vulnerabilities from source code, but commercial off the shelf components are, in general, distributed in binary form. Before using such third-party components it is imperative to check for presence of vulnerabilities. We investigate the use of static analysis and machine learning for predicting buffer overflow vulnerabilities from binaries in this study. To mitigate buffer overflows, developers typically perform size checks and input validation. We propose static code attributes characterizing buffer usage and defense mechanisms implemented in the code for preventing buffer overflows. The proposed approach starts by identifying potential vulnerable statement constructs during binary program analysis and extracts static code attributes for each of them as per proposed characterization scheme to capture buffer usage patterns and defensive mechanisms employed in the code. Data mining methods are then used on these collected code attributes for predicting buffer overflows. Our experimental evaluation on standard buffer overflow benchmark binaries shows that the proposed static code attributes are effective in predicting buffer overflow vulnerabilities. © 2015 IEEE.","Binary static analysis; Buffer overflow; Buffer usage pattern; Control and data dependency; Disassembly; Static code attributes; Vulnerability prediction","Application programs; Artificial intelligence; Bins; Buffer storage; Codes (symbols); Computer programming; Computer software; Data mining; Forecasting; Learning systems; Buffer overflows; Buffer usage; Data dependencies; Disassembly; Static codes; Static analysis","Conference paper","Final","","Scopus","2-s2.0-84962162171"
"Anu C.; Bhaskar Naik K.","Anu, C. (57214263523); Bhaskar Naik, K. (56736993000)","57214263523; 56736993000","Text mining for vulnerable software components envisionment","2015","International Journal of Applied Engineering Research","10","11","","29321","29326","5","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937867879&partnerID=40&md5=eecaafcd3555916eff1b666ae5c25978","This paper presents an approach to identify the vulnerabilities in a software component using text mining. Text mining is applied to the software components taking into account the terms and their frequencies. The software components are sometimes affected with vulnerabilities due to which viruses, trap doors enter into them. Hence the software components are tested for vulnerabilities using text mining. The source code of the components is used for text mining. This process is further enhanced by terms, term’s frequencies along with character frequency. The result says that the software component is said to be vulnerable based on precision and recall values. © Research India Publications.","Prediction model; Text mining; Vulnerabilities","","Article","Final","","Scopus","2-s2.0-84937867879"
"Janiszewski M.; Felkner A.; Olszak J.","Janiszewski, Marek (57191745404); Felkner, Anna (34976397200); Olszak, Jakub (57195611205)","57191745404; 34976397200; 57195611205","Trust and Risk Assessment Model of Popular Software Based on Known Vulnerabilities","2017","International Journal of Electronics and Telecommunications","63","3","","329","336","7","","10.1515/eletel-2017-0044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029231745&doi=10.1515%2feletel-2017-0044&partnerID=40&md5=67ffc7c378ff581fde9da06af51f86c6","This paper presents a new concept of an approach to risk assessment which can be done on the basis of publicly available information about vulnerabilities. The presented approach uses also the notion of trust and implements many concepts used in so called trust and reputation management systems (which are widely used in WSN, MANET or P2P networks, but also in ecommerce platforms). The article shows first outcomes obtained from the presented model. The outcomes demonstrate that the model can be implemented in real system to make software management more quantified and objective process, which can have real and beneficial impact on institutional security. In article, however the emphasis was set not on the model itself (which can be easily changed) but on the possibility of finding useful information about vulnerabilities. © 2017 by Marek Janiszewski.","0-day vulnerabilities forecast; prediction model; risk assessment; risk of information systems; software management; software vulnerabilities; trust and reputation management models","Information management; Mobile ad hoc networks; Peer to peer networks; Risks; P2P network; Prediction model; Real systems; Risk assessment models; Software management; Software vulnerabilities; Trust and reputation managements; Risk assessment","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85029231745"
"Acosta J.C.; Padilla E.; Homer J.","Acosta, Jaime C. (7102205632); Padilla, Edgar (39962218900); Homer, John (25651499600)","7102205632; 39962218900; 25651499600","Augmenting attack graphs to represent data link and network layer vulnerabilities","2016","Proceedings - IEEE Military Communications Conference MILCOM","","","7795462","1010","1015","5","","10.1109/MILCOM.2016.7795462","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011802379&doi=10.1109%2fMILCOM.2016.7795462&partnerID=40&md5=c59cd586dd3565ab34908af83551efe2","Attack graphs enable system stakeholders to understand the stepping stones or exploitation procedures that an adversary could potentially execute to impact the confidentiality, integrity, and availability of a network system. These graphs are used to assess risk and to determine components that, when hardened, contribute most to risk reduction. While these graphs are powerful and widely used in enterprise network systems they focus on application vulnerabilities; they currently do not incorporate weaknesses in the network backbone (e.g., routing) that could lead to traffic hijacking, spoofing, eavesdropping, and several others. In this paper, we describe our work in augmenting the MulVAL attack graph software to incorporate network layer misconfigurations. Through a case study, we show how our modular data pipeline, leveraging previous work in network layer attack impact prediction, can aid system stakeholders in identifying risk and deciding on risk reduction strategies. © 2016 IEEE.","Ad hoc networks; Computer security; Logic gates; Ports (Computers); Routing protocols; Servers","Ad hoc networks; Graphic methods; Logic gates; Military communications; Network protocols; Network routing; Network security; Risk assessment; Routing protocols; Security of data; Security systems; Servers; Enterprise networks; Misconfigurations; Modular data; Network backbones; Network systems; Ports (Computers); Risk reductions; Stepping stone; Network layers","Conference paper","Final","","Scopus","2-s2.0-85011802379"
"Pang Y.; Xue X.; Namin A.S.","Pang, Yulei (56131822700); Xue, Xiaozhen (37666341100); Namin, Akbar Siami (13805425800)","56131822700; 37666341100; 13805425800","Early identification of vulnerable software components via ensemble learning","2017","Proceedings - 2016 15th IEEE International Conference on Machine Learning and Applications, ICMLA 2016","","","7838188","476","481","5","","10.1109/ICMLA.2016.83","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015387749&doi=10.1109%2fICMLA.2016.83&partnerID=40&md5=bd6201ce9d9700913b05f9a1f8b60a57","Software components, which are vulnerable to being exploited, need to be identified and patched. Employing any prevention techniques designed for the purpose of detecting vulnerable software components in early stages can reduce the expenses associated with the software testing process significantly and thus help building a more reliable and robust software system. Although previous studies have demonstrated the effectiveness of adapting prediction techniques in vulnerability detection, the feasibility of those techniques is limited mainly because of insufficient training data sets. This paper proposes a prediction technique targeting at early identification of potentially vulnerable software components. In the proposed scheme, the potentially vulnerable components are viewed as mislabeled data that may contain true but not yet observed vulnerabilities. The proposed hybrid technique combines the supports vector machine algorithm and ensemble learning strategy to better identify potential vulnerable components. The proposed vulnerability detection scheme is evaluated using some Java Android applications. The results demonstrated that the proposed hybrid technique could identify potentially vulnerable classes with high precision and relatively acceptable accuracy and recall.","Ensemble learning; Mislabeled data; Support vector machine; Vulnerability","Artificial intelligence; Learning systems; Software testing; Support vector machines; Android applications; Ensemble learning; Mislabeled data; Prediction techniques; Prevention techniques; Software component; Vulnerability; Vulnerability detection; Machine components","Conference paper","Final","","Scopus","2-s2.0-85015387749"
"Bansal A.; Malhotra R.; Raje K.","Bansal, Ankita (55998430000); Malhotra, Ruchika (15758058000); Raje, Kimaya (57191037967)","55998430000; 15758058000; 57191037967","Analyzing and assessing the security-related defects","2016","2016 1st International Conference on Innovation and Challenges in Cyber Security, ICICCS 2016","","","7542332","21","25","4","","10.1109/ICICCS.2016.7542332","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985902350&doi=10.1109%2fICICCS.2016.7542332&partnerID=40&md5=7ab72e315ad95805fcfd3e12cba88920","The use of the Internet has become an integral part of everyone's life. Due to this, the introduction of virus and other malicious crackers is increasing everyday. This in turn leads to the introduction of defects which adversely affect the security. Thus, protecting vital information in this cyber world is not an easy task. We need to deal with security related defects to ensure failure free and smooth functioning of the software. Thus, in this paper, we intend to study and analyze various aspects of security-related defects by analyzing the defect reports available in various open-source software repositories. Besides this, prediction models can also be constructed which can be used by researchers and practitioners to predict various aspects of security-related defects. Such prediction models are especially beneficial for large-scale systems, where testing experts need to focus their attention and resources to the problem areas of the system under development. Thus, application of software prediction models in the early phases of the software life cycle contributes to efficient defect removal and results in delivering more reliable and better quality software products. Empirical studies lack the use of proper research methodology and thus result in reporting inconsistent results. This study will review the sequence of steps followed in the research process for carrying empirical and replicated studies. The steps include a) literature survey and definition of variables b) data collection c) report findings using statistical and machine learning techniques d) analyzing performance measures for evaluating the performance of the predicted models and e) interpretation of the obtained results for developing a software prediction model. These steps are explained with the help of experimental public domain data set. In addition, the paper provides an overview of repositories for mining software engineering data, tools for analyzing this data and various categories of machine learning methods. It also discusses existing research avenues and provides future research directions in this area. © 2016 IEEE.","Empirical Validation; Machine Learning; Security Vulnerabilities; Security-Related Defects; Statistical Methods","Application programs; Artificial intelligence; Computer software; Defects; Forecasting; Large scale systems; Learning systems; Life cycle; Open source software; Open systems; Software engineering; Statistical methods; Testing; Viruses; Empirical validation; Future research directions; Machine learning methods; Machine learning techniques; Mining software engineering datum; Research methodologies; Security vulnerabilities; Software prediction models; C (programming language)","Conference paper","Final","","Scopus","2-s2.0-84985902350"
"Jain B.; Tsai C.-C.; Porter D.E.","Jain, Bhushan (36632196100); Tsai, Chia-Che (56156799300); Porter, Donald E. (22836456100)","36632196100; 56156799300; 22836456100","A Clairvoyant Approach to Evaluating Software (In)Security","2017","Proceedings of the Workshop on Hot Topics in Operating Systems - HOTOS","Part F129307","","","62","68","6","","10.1145/3102980.3102991","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027992631&doi=10.1145%2f3102980.3102991&partnerID=40&md5=f4b28ecb1dcc115a5f61544e92457e0f","Nearly all modern software has security flaws - -either known or unknown by the users. However, metrics for evaluating software security (or lack thereof) are noisy at best. Common evaluation methods include counting the past vulnerabilities of the program, or comparing the size of the Trusted Computing Base (TCB), measured in lines of code (LoC) or binary size. Other than deleting large swaths of code from project, it is difficult to assess whether a code change decreased the likelihood of a future security vulnerability. Developers need a practical, constructive way of evaluating security. This position paper argues that we actually have all the tools needed to design a better, empirical method of security evaluation. We discuss related work that estimates the severity and vulnerability of certain attack vectors based on code properties that can be determined via static analysis. This paper proposes a grand, unified model that can predict the risk and severity of vulnerabilities in a program. Our prediction model uses machine learning to correlate these code features of open-source applications with the history of vulnerabilities reported in the CVE (Common Vulnerabilities and Exposures) database. Based on this model, one can incorporate an analysis into the standard development cycle that predicts whether the code is becoming more or less prone to vulnerabilities. © 2017 ACM.","","Codes (symbols); Learning systems; Open source software; Open systems; Common vulnerabilities and exposures; Evaluating software; Evaluation methods; Open source application; Security evaluation; Security vulnerabilities; Standard development; Trusted computing base; Static analysis","Conference paper","Final","","Scopus","2-s2.0-85027992631"
"Zhang Y.; Lo D.; Xia X.; Xu B.; Sun J.; Li S.","Zhang, Yun (57267209400); Lo, David (35269388000); Xia, Xin (54586248800); Xu, Bowen (57189036787); Sun, Jianling (8324494900); Li, Shanping (35275218400)","57267209400; 35269388000; 54586248800; 57189036787; 8324494900; 35275218400","Combining Software Metrics and Text Features for Vulnerable File Prediction","2016","Proceedings of the IEEE International Conference on Engineering of Complex Computer Systems, ICECCS","2016-January","","7384228","40","49","9","","10.1109/ICECCS.2015.15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964876127&doi=10.1109%2fICECCS.2015.15&partnerID=40&md5=876b0adf14cf1d7334257c88869d8db3","In recent years, to help developers reduce time and effort required to build highly secure software, a number of prediction models which are built on different kinds of features have been proposed to identify vulnerable source code files. In this paper, we propose a novel approach VULPREDICTOR to predict vulnerable files, it analyzes software metrics and text mining together to build a composite prediction model. VULPREDICTOR first builds 6 underlying classifiers on a training set of vulnerable and non-vulnerable files represented by their software metrics and text features, and then constructs a meta classifier to process the outputs of the 6 underlying classifiers. We evaluate our solution on datasets from three web applications including Drupal, PHPMyAdmin and Moodle which contain a total of 3,466 files and 223 vulnerabilities. The experiment results show that VULPREDICTOR can achieve F1 and EffectivenessRatio@20% scores of up to 0.683 and 75%, respectively. On average across the 3 projects, VULPREDICTOR improves the F1 and EffectivenessRatio@20% scores of the best performing state-of-the-art approaches proposed by Walden et al. by 46.53% and 14.93%, respectively. © 2015 IEEE.","Machine Learning; Text Mining; Vulnerable File","Classification (of information); Data mining; Forecasting; Machine-learning; Prediction modelling; Reduce time; Secure software; Software metrics; Source codes; Text feature; Text-mining; Training sets; Vulnerable file; Learning systems","Conference paper","Final","","Scopus","2-s2.0-84964876127"
"Rahman A.; Pradhan P.; Partho A.; Williams L.","Rahman, Akond (57188647874); Pradhan, Priysha (57195313926); Partho, Asif (57195313386); Williams, Laurie (35565101900)","57188647874; 57195313926; 57195313386; 35565101900","Predicting Android Application Security and Privacy Risk with Static Code Metrics","2017","Proceedings - 2017 IEEE/ACM 4th International Conference on Mobile Software Engineering and Systems, MOBILESoft 2017","","","7972729","149","153","4","","10.1109/MOBILESoft.2017.14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027057092&doi=10.1109%2fMOBILESoft.2017.14&partnerID=40&md5=8355bc8e66eaf229f2eb6fb401ee0954","Android applications pose security and privacy risks for end-users. These risks are often quantified by performing dynamic analysis and permission analysis of the Android applications after release. Prediction of security and privacy risks associated with Android applications at early stages of application development, e.g. when the developer (s) are writing the code of the application, might help Android application developers in releasing applications to end-users that have less security and privacy risk. The goal of this paper is to aid Android application developers in assessing the security and privacy risk associated with Android applications by using static code metrics as predictors. In our paper, we consider security and privacy risk of Android application as how susceptible the application is to leaking private information of end-users and to releasing vulnerabilities. We investigate how effectively static code metrics that are extracted from the source code of Android applications, can be used to predict security and privacy risk of Android applications. We collected 21 static code metrics of 1,407 Android applications, and use the collected static code metrics to predict security and privacy risk of the applications. As the oracle of security and privacy risk, we used Androrisk, a tool that quantifies the amount of security and privacy risk of an Android application using analysis of Android permissions and dynamic analysis. To accomplish our goal, we used statistical learners such as, radial-based support vector machine (r-SVM). For r-SVM, we observe a precision of 0.83. Findings from our paper suggest that with proper selection of static code metrics, r-SVM can be used effectively to predict security and privacy risk of Android applications. © 2017 IEEE.","Android application; code metrics; prediction; security and privacy risk","Android (operating system); Codes (symbols); Forecasting; Risk assessment; Risks; Software engineering; Support vector machines; Android applications; Application development; Code metrics; End users; Private information; Security and privacy; Source codes; Static code metrics; Mobile security","Conference paper","Final","","Scopus","2-s2.0-85027057092"
"Sankararaman S.; Roychoudhury I.; Zhang X.; Goebel K.","Sankararaman, Shankar (36437515000); Roychoudhury, Indranil (22951903000); Zhang, Xiaoge (57194857713); Goebel, Kai (7005329259)","36437515000; 22951903000; 57194857713; 7005329259","Preliminary investigation of impact of technological impairment on trajectory-based operations","2017","17th AIAA Aviation Technology, Integration, and Operations Conference, 2017","","","","","","14","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023636262&partnerID=40&md5=810133f5a9ad3cccac38b2b0da31cf59","The Next Generation Air Transportation System (NextGen) incorporates collaborative air traffic management and Trajectory-Based Operations (TBO) in order to significantly increase the capacity, efficiency, and predictability of operations in the National Airspace System (NAS), without decreasing safety. This is enabled by airspace users and service providers sharing knowledge about operations that allows prediction of the complete 4 D flight trajectory with as little uncertainty as possible. Additionally, new software and hardware technology is critical to reaching NextGen goals, especially with regard to TBO. What if the technologies that are critical for TBO were to be impaired or fail completely? Should there be a malfunction of a piece of the technology, it must be ensured that the whole system does not break down completely or suffer severe impairment. Instead, operations need to be maintained proportionally to the problem and safety needs to be ensured (graceful degradation). This paper proposes a systematic framework to investigate the vulnerability of TBO to technology disruption, and determine the impact of technological impairment on TBO. Two representative technologies are chosen for detailed investigation and the impact of their impairment on the degradation of TBO is illustrated using a weather-related scenario. © 2017, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.","","Air traffic control; Air transportation; Air Traffic Management; Flight trajectory; Graceful degradation; National airspace system; Next-generation air transportation systems; Sharing knowledge; Software and hardwares; Systematic framework; Trajectories","Conference paper","Final","","Scopus","2-s2.0-85023636262"
"Ben Othmane L.; Chehrazi G.; Bodden E.; Tsalovski P.; Brucker A.D.","Ben Othmane, Lotfi (36699233100); Chehrazi, Golriz (55119945300); Bodden, Eric (14041233500); Tsalovski, Petar (56938983400); Brucker, Achim D. (8868852700)","36699233100; 55119945300; 14041233500; 56938983400; 8868852700","Time for Addressing Software Security Issues: Prediction Models and Impacting Factors","2017","Data Science and Engineering","2","2","","107","124","17","","10.1007/s41019-016-0019-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052717508&doi=10.1007%2fs41019-016-0019-8&partnerID=40&md5=2ce528894be816b0090ce02f69b9e19b","Finding and fixing software vulnerabilities have become a major struggle for most software development companies. While generally without alternative, such fixing efforts are a major cost factor, which is why companies have a vital interest in focusing their secure software development activities such that they obtain an optimal return on this investment. We investigate, in this paper, quantitatively the major factors that impact the time it takes to fix a given security issue based on data collected automatically within SAP’s secure development process, and we show how the issue fix time could be used to monitor the fixing process. We use three machine learning methods and evaluate their predictive power in predicting the time to fix issues. Interestingly, the models indicate that vulnerability type has less dominant impact on issue fix time than previously believed. The time it takes to fix an issue instead seems much more related to the component in which the potential vulnerability resides, the project related to the issue, the development groups that address the issue, and the closeness of the software release date. This indicates that the software structure, the fixing processes, and the development groups are the dominant factors that impact the time spent to address security issues. SAP can use the models to implement a continuous improvement of its secure software development process and to measure the impact of individual improvements. The development teams at SAP develop different types of software, adopt different internal development processes, use different programming languages and platforms, and are located in different cities and countries. Other organizations, may use the results—with precaution—and be learning organizations. © 2016, The Author(s).","Human factors; Issue fix time; Secure software","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85052717508"
"Yang L.; Li X.; Yu Y.","Yang, Limin (57189896185); Li, Xiangxue (13605796200); Yu, Yu (57192375191)","57189896185; 13605796200; 57192375191","VulDigger: A Just-in-Time and Cost-Aware Tool for Digging Vulnerability-Contributing Changes","2017","2017 IEEE Global Communications Conference, GLOBECOM 2017 - Proceedings","2018-January","","8254428","1","7","6","","10.1109/GLOCOM.2017.8254428","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046369310&doi=10.1109%2fGLOCOM.2017.8254428&partnerID=40&md5=cfb2b63951aee82cb3c3a49d0ca4df6a","It has been widely adopted to minimize the maintenance cost by predicting potential vulnerabilities before code audits in academia and industry. Most previous research dedicated to file/component level vulnerability prediction models is coarse- grained and may suffer from cost-prohibitive and impractical security testing activities. In this paper, we focus on a cost- aware vulnerability prediction model and present a just-in-time change-level code review tool called VulDigger to dig suspicious ones from a sea of code changes. Our contributions benefit from the case study of Mozilla Firefox by constructing a large-scale vulnerability-contributing changes (VCCs) dataset in a semi-automatic fashion. We then further manifest a classification tool with a mixture of established and new metrics derived from both software defect prediction and vulnerability prediction. Consequently, the precision of such tool is extremely promising (i.e., 92%) for an effort-aware software team. We also examine the return on investment by training a regression model to locate most skeptical changes with fewer lines to inspect. Our findings suggest that such model is capable of pinpointing 31% of all VCCs with only 20% of the effort it would take to audit all changes (i.e., 55% better than random predictor). Our outputs can assist as an early step of continuous security inspections as it provides immediate feedback once developers submit changes to their code base. © 2017 IEEE.","","Codes (symbols); Just in time production; Large dataset; Regression analysis; Classification tool; Cost prohibitive; Immediate feedbacks; Maintenance cost; Prediction model; Regression model; Security testing; Software defect prediction; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85046369310"
"Vijayan A.; Koneru A.; Ebrahimit M.; Chakrabarty K.; Tahoori M.B.","Vijayan, Arunkumar (56875301500); Koneru, Abhishek (56875474500); Ebrahimit, Mojtaba (57189659811); Chakrabarty, Krishnendu (57203198425); Tahoori, Mehdi B. (6603381884)","56875301500; 56875474500; 57189659811; 57203198425; 6603381884","Online soft-error vulnerability estimation for memory arrays","2016","Proceedings of the IEEE VLSI Test Symposium","2016-May","","7477301","","","","","10.1109/VTS.2016.7477301","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973866705&doi=10.1109%2fVTS.2016.7477301&partnerID=40&md5=336d024a31741a9c79aaec0d8a4702ba","Radiation-induced soft errors are a major reliability concern in circuits fabricated at advanced technology nodes. Online soft-error vulnerability estimation offers the flexibility of exploiting dynamic fault-tolerant mechanisms for cost-effective reliability enhancement. We propose a generic run-time method with low area and power overhead to predict the soft-error vulnerability of on-chip memory arrays. The vulnerability prediction is based on signal probabilities (SPs) of a small set of flip-flops, chosen at design time, by studying the correlation between the soft-error vulnerability and the flip-flop SPs for representative workloads. We exploit machine learning to develop a predictive model that can be deployed in the system in software form. Simulation results on two processor designs show that the proposed technique can accurately estimate the soft-error vulnerability of on-chip memory arrays that constitute the instruction cache, the data cache, and the register file. © 2016 IEEE.","","Artificial intelligence; Cost effectiveness; Error correction; Errors; Flip flop circuits; Learning systems; Memory architecture; Radiation hardening; Advanced technology; Instruction caches; Power overhead; Predictive modeling; Radiation-induced; Reliability enhancement; Signal probability; Two processors; Cache memory","Conference paper","Final","","Scopus","2-s2.0-84973866705"
"Pandey P.; Snekkenes E.A.","Pandey, Pankaj (56452372200); Snekkenes, Einar Arthur (55929053700)","56452372200; 55929053700","Design and performance aspects of information security prediction markets for risk management","2015","SECRYPT 2015 - 12th International Conference on Security and Cryptography, Proceedings; Part of 12th International Joint Conference on e-Business and Telecommunications, ICETE 2015","","","","273","284","11","","10.5220/0005547502730284","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964957442&doi=10.5220%2f0005547502730284&partnerID=40&md5=e7eddd394b1e026f92f3bcc0e572a77d","Prediction Markets are the markets designed and operated to mine and aggregate the information scattered among the traders. Recently, some researchers have started exploring the application of prediction markets in the information security domain. The information security prediction market will facilitate trading of contracts to hedge the financial impact of the risks associated with the underlying information security events, such as discovery of a vulnerability in a piece of software. However, prediction markets differ in their objectives and requirements, and therefore information security prediction markets need to be carefully engineered to meet the specific requirements. The contribution of this paper is the identification of a set of design requirements for an information security prediction market, and associated performance criteria. We present five categories of design requirements: Contracts, Trading Process, Participants and Incentives, Clearing House, and Market Management for the information security prediction market. Furthermore, we present six performance measures: Information Elicitation, Transparency, Efficiency, Transaction Cost, Liquidity, and Manipulation Resistance for the performance assessment of information security prediction market. © Copyright 2015 SCITEPRESS - Science and Technology Publications. All rights reserved.","Information security; Prediction market; Security economics; Security risk management","Cryptography; Forecasting; Risk management; Security of data; Financial impacts; Performance aspects; Performance assessment; Performance criterion; Performance measure; Prediction markets; Security Economics; Security risk managements; Commerce","Conference paper","Final","","Scopus","2-s2.0-84964957442"
"Chatzipoulidis A.; Michalopoulos D.; Mavridis I.","Chatzipoulidis, Aristeidis (36781729600); Michalopoulos, Dimitrios (58416619500); Mavridis, Ioannis (56276761700)","36781729600; 58416619500; 56276761700","Information infrastructure risk prediction through platform vulnerability analysis","2015","Journal of Systems and Software","106","","9500","28","41","13","","10.1016/j.jss.2015.04.062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930801342&doi=10.1016%2fj.jss.2015.04.062&partnerID=40&md5=81c7611654e1e1d3501e4805f3da8bb8","Abstract The protection of information infrastructures is important for the function of other infrastructure sectors. As vital parts for the information infrastructure operation, software-based platforms, face a series of vulnerabilities and threats. This paper aims to provide a complementary approach to existing vulnerability prediction solutions and launch the measurement of zero-day risk by introducing a risk prediction methodology for an information infrastructure. The proposed methodology consists of four steps and utilizes the outcomes of a proper analysis of security measurements provided by specifications from the Security Content Automation Protocol. First, we identify software platform assets that support an information infrastructure and second we measure the historical rate of vulnerability occurrences. Third, we use a distribution fitting procedure to estimate the statistical correlation between empirical and reference probability distributions and verify the statistical significance of the distribution fitting results with the Kolmogorov - Smirnov test. Fourth, we develop conditional probability tables that constitute a Bayesian Belief Network topology as means to enable risk prediction and estimation on security properties. The practicality of the risk prediction methodology is demonstrated with an implementation example from the electronic banking sector. The contribution of the proposed methodology is to provide auditors with a proactive approach about zero-day risks. © 2015 Elsevier Inc.","Bayesian belief network (BBN); Kolmogorov-Smirnov test; Zero-day risk","Bayesian networks; Computational complexity; Forecasting; Network security; Probability distributions; Risk perception; Statistical tests; Conditional probability tables; Distribution fitting; Information infrastructures; Infrastructure sector; Kolmogorov-Smirnov test; Statistical correlation; Statistical significance; Vulnerability analysis; Risk assessment","Article","Final","","Scopus","2-s2.0-84930801342"
"Toma-Danila D.; Zulfikar C.; Manea E.F.; Cioflan C.O.","Toma-Danila, D. (55595611000); Zulfikar, C. (24472180100); Manea, E.F. (56548519000); Cioflan, C.O. (6507607481)","55595611000; 24472180100; 56548519000; 6507607481","Improved seismic risk estimation for Bucharest, based on multiple hazard scenarios and analytical methods","2015","Soil Dynamics and Earthquake Engineering","73","","","1","16","15","","10.1016/j.soildyn.2015.02.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924692660&doi=10.1016%2fj.soildyn.2015.02.013&partnerID=40&md5=646cdc82ac09423017ad5e59f02431b1","Bucharest, capital of Romania, is one of the most exposed big cities in Europe to seismic damage, due to the intermediate-depth earthquakes in the Vrancea region, to the vulnerable building stock and local soil conditions. This paper tries to answer very important questions related to the seismic risk at city scale that were not yet adequately answered. First, we analyze and highlight the bottlenecks of previous risk-related studies. Based on new researches in the hazard of Bucharest (recent microzonation map and ground-motion prediction equations, reprocessed real recorded data) and in vulnerability assessment (analytical methods, earthquake loss estimation software like SELENA and ELER, the recently implemented Near Real-Time System for Estimating the Seismic Damage in Romania) we provide an improved estimation of the number of buildings and population that could be affected, for different earthquake scenarios. A new method for enhancing the spatial resolution of the building stock data is used successfully. © 2015 Elsevier Ltd.","Bucharest seismic risk; ELER; Loss estimation software; SELENA; Vrancea earthquakes","Bucharest [Bucharest (ADS)]; Bucharest [Romania]; Carpathians; Romania; Vrancea; Damage detection; Earthquake resistance; Equations of motion; Hazards; Interactive computer systems; Population statistics; Real time systems; Risk perception; Risks; Seismology; Structural analysis; ELER; Loss estimation; Seismic risk; SELENA; Vrancea; analytical method; earthquake event; estimation method; hazard management; risk assessment; seismic hazard; software; spatial resolution; Earthquakes","Article","Final","","Scopus","2-s2.0-84924692660"
"Davari M.; Zulkernine M.","Davari, Maryam (57194219510); Zulkernine, Mohammad (6506457317)","57194219510; 6506457317","Analysing vulnerability reproducibility for Firefox browser","2016","2016 14th Annual Conference on Privacy, Security and Trust, PST 2016","","","7906955","674","681","7","","10.1109/PST.2016.7906955","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019230172&doi=10.1109%2fPST.2016.7906955&partnerID=40&md5=020824adb9467d5a5e646691af6b6a4d","Fixing some security failures are difficult because they cannot be easily reproduced. To address Hardly Reproducible Vulnerabilities (HRVs), security experts spend a significant amount of time, effort, and budget. Sometimes they do not succeed in the reproduction step and ignore some security failures. The exploitation of a vulnerability due to its irreproducibility may cause severe consequences. An efficient solution is to explore the behaviour of both hardly and easily reproducible security issues at the code level. We use linear regression techniques to build models based on the classical software complexity metrics and a set of attributes related to the environment of the system. The results show that the considered metrics and the vulnerability types do not have significant linear correlations with each other. Also, predicting the HRV-prone parts of large systems is a great help for security experts to focus their effort on the top-ranked vulnerable files. After identifying the suitable indicators based on linear regression, different machine learning techniques such as Random Forest, Logistic Regression, C4.5 Decision Tree, and Naive Bayes are employed to build HRV prediction models. The Random Forest technique achieves the precision of 82% and recall of 84% to classify vulnerable files into HRV-prone or non HRV-prone files. We believe that the results encourage the use of software metrics for vulnerability prediction in some projects. © 2016 IEEE.","Hardly Reproducible Vulnerability (HRV); machine learning; Random Forest; security failure; software metrics; Vulnerability","Artificial intelligence; Budget control; Decision trees; Forecasting; Regression analysis; Hardly Reproducible Vulnerability (HRV); Random forests; Security failure; Software metrics; Vulnerability; Learning systems","Conference paper","Final","","Scopus","2-s2.0-85019230172"
"Alves H.; Fonseca B.; Antunes N.","Alves, Henrique (57193347712); Fonseca, Baldoino (36175426900); Antunes, Nuno (57217858593)","57193347712; 36175426900; 57217858593","Experimenting machine learning techniques to predict vulnerabilities","2016","Proceedings - 7th Latin-American Symposium on Dependable Computing, LADC 2016","","","7781850","151","156","5","","10.1109/LADC.2016.32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013197112&doi=10.1109%2fLADC.2016.32&partnerID=40&md5=d7595ce93b684ce3fb56d7e7c9684b49","Software metrics can be used as a indicator of the presence of software vulnerabilities. These metrics have been used with machine learning to predict source code prone to contain vulnerabilities. Although it is not possible to find the exact location of the flaws, the models can show which components require more attention during inspections and testing. Each new technique uses his own evaluation dataset, which many times has limited size and representativeness. In this experience report, we use a large and representative dataset to evaluate several state of the art vulnerability prediction techniques. This dataset was built with information of 2186 vulnerabilities from five widely used open source projects. Results show that the dataset can be used to distinguish which are the best techniques. It is also shown that some of the techniques can predict nearly all of the vulnerabilities present in the dataset, although with very low precisions. Finally, accuracy, precision and recall are not the most effective to characterize the effectiveness of this tools. © 2016 IEEE.","Machine Learning; Software Metrics; Software Security; Vulnerabilities","Artificial intelligence; Forecasting; Open source software; Machine learning techniques; Open source projects; Precision and recall; Prediction techniques; Software metrics; Software security; Software vulnerabilities; Vulnerabilities; Learning systems","Conference paper","Final","","Scopus","2-s2.0-85013197112"
"Yang J.; Ryu D.; Baik J.","Yang, Joonseok (57188999015); Ryu, Duksan (55317946600); Baik, Jongmoon (14036858600)","57188999015; 55317946600; 14036858600","Improving vulnerability prediction accuracy with Secure Coding Standard violation measures","2016","2016 International Conference on Big Data and Smart Computing, BigComp 2016","","","7425809","115","122","7","","10.1109/BIGCOMP.2016.7425809","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964669821&doi=10.1109%2fBIGCOMP.2016.7425809&partnerID=40&md5=c4a74461a501f93597080c508b142d43","As the need of software has been increasing, the danger of malicious attacks against software has been worse. In order to fortify software systems against adversaries, researchers have devoted significant efforts on mitigating software vulnerabilities. To eliminate security vulnerabilities from software with lower inspection effort, vulnerability prediction approaches have been emerged. By allocating human and time resource on the potentially vulnerable subset, development organization could eliminate vulnerabilities in a cost effective manner. In the vulnerability prediction approaches, a vulnerability prediction model is constructed based on various software attributes. However, vulnerability prediction models based on the traditional software attributes have provided poor prediction accuracy or low cost effectiveness since the traditional software attributes are unable to reflect vulnerability characteristics sufficiently. In this paper, we propose a novel vulnerability prediction approach based on the CERT-C Secure Coding Standard. To evaluate the efficacy of the proposed approach, the prediction results of the suggested prediction models and other traditional models were assessed in terms of prediction accuracy and cost effectiveness. The results show that the proposed method can improve the vulnerability prediction accuracy. © 2016 IEEE.","CERT-C Secure Coding Standard; Security; Software Engineering; Vulnerability; Vulnerability Prediction","C (programming language); Codes (symbols); Cost effectiveness; Costs; Forecasting; Network security; Software engineering; Prediction accuracy; Secure coding; Security; Security vulnerabilities; Software systems; Software vulnerabilities; Traditional models; Vulnerability; Big data","Conference paper","Final","","Scopus","2-s2.0-84964669821"
"Jimenez M.; Papadakis M.; Traon Y.L.","Jimenez, Matthieu (57191959513); Papadakis, Mike (57197295611); Traon, Yves Le (55884641800)","57191959513; 57197295611; 55884641800","An empirical analysis of vulnerabilities in OpenSSL and the linux kernel","2016","Proceedings - Asia-Pacific Software Engineering Conference, APSEC","0","","7890577","105","112","7","","10.1109/APSEC.2016.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018482858&doi=10.1109%2fAPSEC.2016.025&partnerID=40&md5=2d2495124b2109960baab8c182028506","Vulnerabilities are one of the main concerns faced by practitioners when working with security critical applications. Unfortunately, developers and security teams, even experienced ones, fail to identify many of them with severe consequences. Vulnerabilities are hard to discover since they appear in various forms, caused by many different issues and their identification requires an attacker's mindset. In this paper, we aim at increasing the understanding of vulnerabilities by investigating their characteristics on two major open-source software systems, i.e., the Linux kernel and OpenSSL. In particular, we seek to analyse and build a profile for vulnerable code, which can ultimately help researchers in building automated approaches like vulnerability prediction models. Thus, we examine the location, criticality and category of vulnerable code along with its relation with software metrics. To do so, we collect more than 2,200 vulnerable files accounting for 863 vulnerabilities and compute more than 35 software metrics. Our results indicate that while 9 Common Weakness Enumeration (CWE) types of vulnerabilities are prevalent, only 3 of them are critical in OpenSSL and 2 of them in the Linux kernel. They also indicate that different types of vulnerabilities have different characteristics, i.e., metric profiles, and that vulnerabilities of the same type have different profiles in the two projects we examined. We also found that the file structure of the projects can provide useful information related to the vulnerabilities. Overall, our results demonstrate the need for making project specific approaches that focus on specific types of vulnerabilities. © 2016 IEEE.","Common Vulnerability Exposures; Software Metrics; Software Security; Vulnerabilities","Linux; Open source software; Open systems; Automated approach; Common Vulnerability Exposures; Empirical analysis; Open source software systems; Security critical applications; Software metrics; Software security; Vulnerabilities; Security of data","Conference paper","Final","","Scopus","2-s2.0-85018482858"
"AlEroud A.; Alsmadi I.","AlEroud, Ahmed (55053484000); Alsmadi, Izzat (17433667400)","55053484000; 17433667400","Identifying cyber-attacks on software defined networks: An inference-based intrusion detection approach","2017","Journal of Network and Computer Applications","80","","","152","164","12","","10.1016/j.jnca.2016.12.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007380974&doi=10.1016%2fj.jnca.2016.12.024&partnerID=40&md5=685117c04ea7de1811ce4922fef913b4","Software Defined Networking is an emerging architecture which focuses on the role of software to manage computer networks. Software Defined Networks (SDNs) introduce several mechanisms to detect specific types of attacks such as Denial of Service (DoS). Nevertheless, they are vulnerable to similar attacks that occur in traditional networks, such as the attacks that target control and data plane. Several techniques are proposed to handle the security vulnerabilities in SDNs. However, it is fairly challenging to create attack signatures, scenarios, or even intrusion detection rules that are applicable to dynamic environments such SDNs. This paper introduces a new approach to identify attacks on SDNs that uses: (1) similarity with existing attacks that target traditional networks, (2) an inference mechanism to avoid false positives and negatives during the prediction process, and (3) a packet aggregation technique which aims at creating attack signatures and use them to predict attacks on SDNs. We validated our approach on two datasets and showed that it yields promising results. © 2016 Elsevier Ltd","Graph mining, Denial of service attacks; Information security; Intrusion detection; Security architecture; Software defined networks","Computer crime; Data mining; Denial-of-service attack; Intrusion detection; Mercury (metal); Network architecture; Security of data; Software defined networking; Dynamic environments; Emerging architectures; Graph mining; Inference mechanism; Intrusion detection approaches; Packet aggregation; Security Architecture; Security vulnerabilities; Network security","Article","Final","","Scopus","2-s2.0-85007380974"
"Zeng L.; Lin Y.-R.; Cui Y.; Mannan M.S.","Zeng, Lihan (57195338630); Lin, Yan-Ru (56587031000); Cui, Yan (57195337152); Mannan, M. Sam (7103014437)","57195338630; 56587031000; 57195337152; 7103014437","An integrated prediction model for H2S/CO2 corrosion in the pipe of refinery","2017","Global Congress on Process Safety 2017 - Topical Conference at the 2017 AIChE Spring Meeting and 13th Global Congress on Process Safety","3","","","1984","2009","25","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027286771&partnerID=40&md5=e3f96df27d1493c4da6dbfae2e3a1fdc","Corrosion of equipment is one of the most vital factors that results in serious process safety incidents. Though various types of equipment are subject to corrosion issues to different extents depending on the process conditions, the pipe and the piping network connecting units and equipment are relatively more vulnerable to corrosion. The vulnerability of a pipe to internal corrosion is predominantly due to the process conditions. Among the factors contributing to the internal corrosion, substances creating corrosive conditions in the pipe, such as CO2 and H2S, are the most common factors, followed by the flow condition of processes (flow rate and temperature). In this paper, a single-phase integrated prediction model for H2S/CO2 corrosion is developed to study a holistic effect of the most important variables. The model investigates the electrochemical kinetics of corrosive substances, the scale formation conditions, and the flow conditions that have an impact on the mass transport of corrosive species. The COMSOL software based on the finite element method is used to simulate all these processes and the prediction results will be compared with the previously published model and the field data.","90° elbow; CO<sub>2</sub>; Corrosion; Electrochemical kinetics; H<sub>2</sub>S; Mass transport; Pipe; Turbulent flow; Wall shear stress","Carbon dioxide; Corrosion; Equipment; Forecasting; Mass transfer; Pipe; Shear flow; Shear stress; Turbulent flow; Corrosive conditions; Corrosive species; Electrochemical kinetics; Integrated prediction models; Internal corrosion; Process condition; Process safety incidents; Wall shear stress; Finite element method","Conference paper","Final","","Scopus","2-s2.0-85027286771"
"Song J.-H.; Park J.-P.; Jun M.-S.","Song, Jun-Ho (56963002300); Park, Jae-Pyo (8922667100); Jun, Moon-Seog (7006544871)","56963002300; 8922667100; 7006544871","A study of vulnerability assessment using fuzzing data suite and data flow analysis in software","2016","Advanced Science Letters","22","9","","2592","2597","5","","10.1166/asl.2016.7809","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019980738&doi=10.1166%2fasl.2016.7809&partnerID=40&md5=ecc1edd193b598e6cc779bec5a0d263e","Recently, there are many studies about infringement protection such as analysis study of potential abuse of weak point and researches for weak point of software. This study provide with Exploitable assessment method based on the data flow analysis and software clash analysis for the exploit prediction. This method analyze the limitation of existing clash analysis and the characteristics of existing data flow analysis, and improve the traceability of access violation clash weak point based on the relativity of functional unit by analyzing unit area of clash analysis and data flow analysis. In order to achieve this, I extracted the fuzzing data format, mapped data format to the knot of Dependency structure matrices based data flow analysis, and conducted the examination if the reading access violation from functions connected to clash was related to the clash value. The proposed method adapted functional flow analysis of software to weak point analysis method to overcome the excessive exhaustion of resources in overall flow analysis of software, related the dynamic-based analysis result such as fuzzing and taint analysis, which finally is able to extract the base data for assessment Exploitable. © 2016 American Scientific Publishers. All rights reserved.","Dependency structure matrices; Exploitable assessment; Software crash analysis; Taint analysis; Vulnerability","","Article","Final","","Scopus","2-s2.0-85019980738"
"Subramani S.; Vouk M.; Williams L.","Subramani, Shweta (58426661400); Vouk, Mladen (7004021808); Williams, Laurie (35565101900)","58426661400; 7004021808; 35565101900","Non-operational testing of software for security issues","2013","2013 IEEE International Symposium on Software Reliability Engineering Workshops, ISSREW 2013","","","6688857","21","22","1","","10.1109/ISSREW.2013.6688857","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893222181&doi=10.1109%2fISSREW.2013.6688857&partnerID=40&md5=fd2db80cb22ff3243c93c7be56e37923","We are studying extension of the classical Software Reliability Engineering (SRE) methodology into the security space. We combine 'classical' reliability modeling, when applied to reported vulnerabilities found under 'normal' operational profile conditions, with safety oriented fault management processes. We illustrate with open source Fedora software. © 2013 IEEE.","detection; non-operational testing; prediction; security faults; vulnerabilities","Error detection; Forecasting; Open systems; Software reliability; Software testing; Technical presentations; Fault management; Operational profile; Reliability model; Security faults; Security issues; Security space; Software reliability engineering; vulnerabilities; Security of data","Conference paper","Final","","Scopus","2-s2.0-84893222181"
"Shamal P.K.; Rahamathulla K.; Akbar A.","Shamal, P.K. (57201879553); Rahamathulla, K. (57191586794); Akbar, Ali (57209596959)","57201879553; 57191586794; 57209596959","A study on software vulnerability prediction model","2017","Proceedings of the 2017 International Conference on Wireless Communications, Signal Processing and Networking, WiSPNET 2017","2018-January","","","703","706","3","","10.1109/WiSPNET.2017.8299852","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046337751&doi=10.1109%2fWiSPNET.2017.8299852&partnerID=40&md5=d6ca4f0c7a58594bbafbd431ef24ca8c","Developing a secure software is time consuming and a complex activity. The main source of insecurity is vulnerabilities in the software. So the prediction of software vulnerability plays important role in software engineering, especially in web application development. A software vulnerability prediction model forecasts whether a software component is vulnerable or not. This paper describes various software vulnerability prediction models. Mainly two types of software vulnerability models are used to predict the vulnerability component in software. In software metrics based prediction model, different software metrics are used as an indicator of software vulnerability. In text analysis based method, source code of the software is used as input to the prediction model. Source code is converted into tokens and frequencies. These are used to predict the vulnerability. © 2017 IEEE.","machine learning; software metrics; software vulnerability prediction; text mining; Web application","Application programs; Machine learning; Machine-learning; Prediction modelling; Secure software; Software metrics; Software vulnerabilities; Software vulnerability prediction; Source codes; Text-mining; WEB application; Web applications; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85046337751"
"Gupta M.K.; Govil M.C.; Singh G.","Gupta, Mukesh Kumar (56405866500); Govil, Mahesh Chandra (35812070200); Singh, Girdhari (57224916905)","56405866500; 35812070200; 57224916905","Predicting Cross-Site Scripting (XSS) security vulnerabilities in web applications","2015","Proceedings of the 2015 12th International Joint Conference on Computer Science and Software Engineering, JCSSE 2015","","","7219789","162","167","5","","10.1109/JCSSE.2015.7219789","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945944571&doi=10.1109%2fJCSSE.2015.7219789&partnerID=40&md5=4915b7101ceb30ee9753d0cf644c29e0","Recently, machine-learning based vulnerability prediction models are gaining popularity in web security space, as these models provide a simple and efficient way to handle web application security issues. Existing state-of-art Cross-Site Scripting (XSS) vulnerability prediction approaches do not consider the context of the user-input in output-statement, which is very important to identify context-sensitive security vulnerabilities. In this paper, we propose a novel feature extraction algorithm to extract basic and context features from the source code of web applications. Our approach uses these features to build various machine-learning models for predicting context-sensitive Cross-Site Scripting (XSS) security vulnerabilities. Experimental results show that the proposed features based prediction models can discriminate vulnerable code from non-vulnerable code at a very low false rate. © 2015 IEEE.","context-sensitive; cross-site scripting vulnerability; input validation; machine learning; web application security","Codes (symbols); Forecasting; Learning systems; Machine learning; Predictive analytics; Software engineering; Context sensitive; Cross site scripting; Feature extraction algorithms; Input validation; Machine learning models; Prediction model; Security vulnerabilities; Web application security; Network security","Conference paper","Final","","Scopus","2-s2.0-84945944571"
"Biswas B.; Pal S.; Mukhopadhyay A.","Biswas, Baidyanath (7103207977); Pal, Shounak (57191161009); Mukhopadhyay, Arunabha (7201817059)","7103207977; 57191161009; 7201817059","AVICS-Eco framework: An approach to attack prediction and vulnerability assessment in a cyber ecosystem","2016","AMCIS 2016: Surfing the IT Innovation Wave - 22nd Americas Conference on Information Systems","","","","","","","","10.2139/ssrn.2792074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088757145&doi=10.2139%2fssrn.2792074&partnerID=40&md5=f175ead0798d882db09f32882402e993","In the light of recent cyber-attacks, it has become imperative for organizations to predict breaches in an accurate and comprehensive manner. In this study, we assess the impact of the external environment as well as factors internal to the organization. We propose the AVICS-Eco Framework to (i) predict cyberattacks in organizations, (ii) assess critical vulnerabilities, (iii) aid IS managers to plan security investments, and, (iv) decide what to patch and when to patch. We validated our model using Partial Least Square Structural Equation Modelling. We have used CSI-FBI, Ponemon and Checkpoint Survey data from 1997 to 2015. As a recommendation, CTOs should be cautious with the vulnerable software of specific categories. We derived that software vendors need to prioritize patches on Networks before Operating Systems. Firewalls were found to be superior in comparison to anti-viruses. Finally, we found limited support for cybersecurity legal provisions as attack inhibitors in the United States.","Cyber security; Ecosystem; PLS; SEM; Software vulnerability","Ecosystems; Information systems; Information use; Investments; Network security; Scanning electron microscopy; Viruses; Attack prediction; Cyber security; External environments; Partial least square (PLS); Software vendors; Software vulnerabilities; Structural equation modelling; Vulnerability assessments; Forecasting","Conference paper","Final","","Scopus","2-s2.0-85088757145"
"Sultana K.Z.; Williams B.J.","Sultana, Kazi Zakia (23494078600); Williams, Byron J. (24478168500)","23494078600; 24478168500","Evaluating micro patterns and software metrics in vulnerability prediction","2017","SoftwareMining 2017 - Proceedings of the 2017 6th IEEE/ACM International Workshop on Software Mining, co-located with ASE 2017","","","8100852","40","47","7","","10.1109/SOFTWAREMINING.2017.8100852","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040775009&doi=10.1109%2fSOFTWAREMINING.2017.8100852&partnerID=40&md5=2173122b00542bd2a069baa831a6ca4f","Software security is an important aspect of ensuring software quality. Early detection of vulnerable code during development is essential for the developers to make cost and time effective software testing. The traditional software metrics are used for early detection of software vulnerability, but they are not directly related to code constructs and do not specify any particular granularity level. The goal of this study is to help developers evaluate software security using class-level traceable patterns called micro patterns to reduce security risks. The concept of micro patterns is similar to design patterns, but they can be automatically recognized and mined from source code. If micro patterns can better predict vulnerable classes compared to traditional software metrics, they can be used in developing a vulnerability prediction model. This study explores the performance of class-level patterns in vulnerability prediction and compares them with traditional class-level software metrics. We studied security vulnerabilities as reported for one major release of Apache Tomcat, Apache Camel and three stand-alone Java web applications. We used machine learning techniques for predicting vulnerabilities using micro patterns and class-level metrics as features. We found that micro patterns have higher recall in detecting vulnerable classes than the software metrics. © 2017 IEEE.","","Codes (symbols); Computer software selection and evaluation; Forecasting; Learning systems; Granularity levels; Machine learning techniques; Prediction model; Security vulnerabilities; Software metrics; Software Quality; Software security; Software vulnerabilities; Software testing","Conference paper","Final","","Scopus","2-s2.0-85040775009"
"Cornel D.; Konev A.; Sadransky B.; Horváth Z.; Gröller E.; Waser J.","Cornel, D. (56410768100); Konev, A. (56275679300); Sadransky, B. (55390075500); Horváth, Z. (56275349200); Gröller, E. (6701726115); Waser, J. (35182238100)","56410768100; 56275679300; 55390075500; 56275349200; 6701726115; 35182238100","Visualization of Object-Centered Vulnerability to Possible Flood Hazards","2015","Computer Graphics Forum","34","3","","331","340","9","","10.1111/cgf.12645","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937951853&doi=10.1111%2fcgf.12645&partnerID=40&md5=e1d2047b173220a0c70c665f9bf8545a","(Figure Presented). As flood events tend to happen more frequently, there is a growing demand for understanding the vulnerability of infrastructure to flood-related hazards. Such demand exists both for flood management personnel and the general public. Modern software tools are capable of generating uncertainty-aware flood predictions. However, the information addressing individual objects is incomplete, scattered, and hard to extract. In this paper, we address vulnerability to flood-related hazards focusing on a specific building. Our approach is based on the automatic extraction of relevant information from a large collection of pre-simulated flooding events, called a scenario pool. From this pool, we generate uncertainty-aware visualizations conveying the vulnerability of the building of interest to different kinds of flooding events. On the one hand, we display the adverse effects of the disaster on a detailed level, ranging from damage inflicted on the building facades or cellars to the accessibility of the important infrastructure in the vicinity. On the other hand, we provide visual indications of the events to which the building of interest is vulnerable in particular. Our visual encodings are displayed in the context of urban 3D renderings to establish an intuitive relation between geospatial and abstract information. We combine all the visualizations in a lightweight interface that enables the user to study the impacts and vulnerabilities of interest and explore the scenarios of choice. We evaluate our solution with experts involved in flood management and public communication. © 2015 The Author(s) Computer Graphics Forum © 2015 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.","","Flood control; Hazards; Human resource management; Visualization; Automatic extraction; Building facades; Flood management; Flood prediction; General publics; Individual objects; Public communications; Visual encodings; Floods","Article","Final","","Scopus","2-s2.0-84937951853"
"Sariman G.; Kucuksille E.U.","Sariman, Guncel (36163813000); Kucuksille, Ecir Ugur (18037773100)","36163813000; 18037773100","A novel approach to determine software security level using Bayes Classifier via static code metrics","2016","Elektronika ir Elektrotechnika","22","2","","73","80","7","","10.5755/j01.eie.22.2.12177","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963836298&doi=10.5755%2fj01.eie.22.2.12177&partnerID=40&md5=f44d9079b3f994589851972f20c406e6","Technological developments are increasing day by day and software products are growing in an uncontrolled way. This leads to the development of applications which do not comply with principles of design. Software which has not passed security testing may put the end user into danger. During the processes of error detection and verification of developed software, static and dynamic analysis may be used. Static code analysis provides analysis in different categories while coding without code compile. Source code metrics are also within these categories. Code metrics evaluate software quality, level of risk, and interchangeability by analysing software based on those metrics. In this study, we will describe our web-based application which is developed to determine the level of security in software. In this scope, software's metric calculation method will be explained. The scoring system we used to determine the security level calculation will be explained, taking into account metric thresholds that are acceptable in the literature. Bayes Classifier Method, distinguishing risks in the project files with the analysis of uploaded sample software files, will be described. Finally, objectives of this analysis method and planned activities will be explained.","Bayes methods; Information security; Software metrics; Software safety; Vulnerability prediction","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-84963836298"
"Stuckman J.; Walden J.; Scandariato R.","Stuckman, Jeffrey (35303887700); Walden, James (22036834700); Scandariato, Riccardo (23095243000)","35303887700; 22036834700; 23095243000","The effect of dimensionality reduction on software vulnerability prediction models","2017","IEEE Transactions on Reliability","66","1","7779151","17","37","20","","10.1109/TR.2016.2630503","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003666894&doi=10.1109%2fTR.2016.2630503&partnerID=40&md5=7ca99b2101257fe9f6a9d4e6c8419a6c","Statistical prediction models can be an effective technique to identify vulnerable components in large software projects. Two aspects of vulnerability prediction models have a profound impact on their performance: 1) the features (i.e., the characteristics of the software) that are used as predictors and 2) the way those features are used in the setup of the statistical learning machinery. In a previous work, we compared models based on two different types of features: software metrics and term frequencies (text mining features). In this paper, we broaden the set of models we compare by investigating an array of techniques for the manipulation of said features. These techniques fall under the umbrella of dimensionality reduction and have the potential to improve the ability of a prediction model to localize vulnerabilities. We explore the role of dimensionality reduction through a series of cross-validation and cross-project prediction experiments. Our results show that in the case of software metrics, a dimensionality reduction technique based on confirmatory factor analysis provided an advantage when performing cross-project prediction, yielding the best F-measure for the predictions in five out of six cases. In the case of text mining, feature selection can make the prediction computationally faster, but no dimensionality reduction technique provided any other notable advantage. © 1963-2012 IEEE.","Computer security; machine learning; software metrics; text mining","Data mining; Factor analysis; Machinery; Confirmatory factor analysis; Cross validation; Dimensionality reduction; Dimensionality reduction techniques; Software metrics; Software vulnerabilities; Statistical learning; Statistical prediction model; Forecasting","Article","Final","","Scopus","2-s2.0-85003666894"
"Aciiçmez O.; Schindler W.","Aciiçmez, Onur (14033907600); Schindler, Werner (57197914387)","14033907600; 57197914387","A vulnerability in RSA implementations due to instruction cache analysis and its demonstration on OpenSSL","2008","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","4964 LNCS","","","256","273","17","","10.1007/978-3-540-79263-5_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149120482&doi=10.1007%2f978-3-540-79263-5_16&partnerID=40&md5=20de6196f0fc0494c05b72480ac9cfea","MicroArchitectural Analysis (MA) techniques, more specifically Simple Branch Prediction Analysis (SBPA) and Instruction Cache Analysis, have the potential of disclosing the entire execution flow of a software-implemented cryptosystem ([5,2]). In this paper we will show that one can completely break RSA in the original unpatched OpenSSL version (v.0.9.8e) even if the most secure configuration is in place, including all countermeasures against side-channel and MicroArchitectural analysis (in particular, base blinding). We also discuss (known) countermeasures that prevent this attack. In a first step we apply an instruction cache attack to reveal which Montgomery operations require extra reductions. To exploit this information we model the timing behavior of the modular exponentiation algorithm by a stochastic process. Its analysis provides the optimal guessing strategy, which reveals the secret key (modp 1) and finally the factorization of the RSA modulus n∈=∈p 1 p 2. For the instruction cache attack we applied a spy process that was embedded in the target process (OpenSSL), which clearly facilitates the experimental part. This simplification yet does not nullify our results since in cache attacks empirical results from embedded spy processes and (suitably implemented) stand-alone spy processes are very close to each other [16] and, moreover, our guessing strategy is fault-tolerant. Interestingly, the second step of our attack is related to that of a particular combined power and timing attack on smart cards [23] (see also [27,22]). Before we published our result [1] we informed the OpenSSL development team who included a patch into the stable branch of v.0.9.7e ([31,32]) and CERT which informed software vendors ([33,34,35]). In particular, this countermeasure is included in the current version 0.9.8f. We have only analyzed OpenSSL, thus we currently do not know the strength of other cryptographic libraries. © 2008 Springer-Verlag Berlin Heidelberg.","Instruction-cache attack; MicroArchitectural analysis; Montgomery multiplication; RSA; Side channel analysis; Stochastic process","Buffer storage; Computer crime; Random processes; Security of data; Software architecture; Instruction-cache attack; MicroArchitectural analysis; Montgomery multiplication; RSA; Side channel analysis; Cryptography","Conference paper","Final","","Scopus","2-s2.0-43149120482"
"Aung M.T.; Milanović J.V.; Simmons P.A.","Aung, M.T. (7003440210); Milanović, J.V. (35508147600); Simmons, P.A. (57197900255)","7003440210; 35508147600; 57197900255","Automated comprehensive assessment and visualization of voltage sag performance","2004","2004 11th International Conference on Harmonics and Quality of Power","","","","191","198","7","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-19644383887&partnerID=40&md5=20b0b75f03c9c426b37e45eff5fe0fd6","This paper describes modular software for the automated assessment and visualization of voltage sag performance. The software allows in-depth analysis of voltage sag performance of the individual buses, the performance of the entire network at different voltage levels and inside the industry facility. The prediction and characterization of voltage sag, identifying the area of vulnerability and the area affected by the fault, and the propagation of voltage sags can be done automatically taking into account different fault statistics for symmetrical and asymmetrical faults, and different fault distributions. The module also considers the protection system and effects of its failure on the duration of voltage sags. The software capabilities are demonstrated on a generic distribution network, and the results of the module are presented in the graphical and tabular form using specially developed graphical user interface (GUI). © 2004 IEEE.","Graphical User Interface; Power Quality; Power System Protection; Stochastic Assessment; Transformer Connection; Voltage Sags","Automation; Computer programming languages; Database systems; Electric potential; Electric transformers; Graphical user interfaces; Random processes; Software engineering; Visualization; Power quality; Power system protection; Stochastic assessment; Transformer connection; Voltage sags; Electric power systems","Conference paper","Final","","Scopus","2-s2.0-19644383887"
"Santos J.C.M.; Fei Y.","Santos, Juan Carlos Martinez (26325154200); Fei, Yunsi (7103059457)","26325154200; 7103059457","Leveraging speculative architectures for runtime program validation","2013","Transactions on Embedded Computing Systems","13","1","3","","","","","10.1145/2512456","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883857670&doi=10.1145%2f2512456&partnerID=40&md5=64152a241d75d7cc5bec7937d4c39862","Program execution can be tampered with by malicious attackers through exploiting software vulnerabilities. Changing the program behavior by compromising control data and decision data has become the most serious threat in computer system security. Although several hardware approaches have been presented to validate program execution, they either incur great hardware overhead or introduce false alarms. We propose a new hardware-based approach by leveraging the existing speculative architectures for runtime program validation. The on-chip branch target buffer (BTB) is utilized as a cache of the legitimate control flow transfers stored in a secure memory region. In addition, the BTB is extended to store the correct program path information. At each indirect branch site, the BTB is used to validate the decision history of previous conditional branches and monitor the following execution path at runtime. Implementation of this approach is transparent to the upper operating system and programs. Thus, it is applicable to legacy code. Because of good code locality of the executable programs and effectiveness of branch prediction, the frequency of control-flow validations against the secure off-chip memory is low. Our experimental results show a negligible performance penalty and small storage overhead. © 2013 ACM.","Control flow validation; Program validation; Security attacks","Digital storage; Branch target buffers; Computer system security; Control flows; Hardware-based approach; Performance penalties; Program validation; Security attacks; Software vulnerabilities; Hardware","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-84883857670"
"Rodríguez R.J.; Trubiani C.; Merseguer J.","Rodríguez, Ricardo J. (56794828000); Trubiani, Catia (24781277400); Merseguer, José (6602569693)","56794828000; 24781277400; 6602569693","Fault-tolerant techniques and security mechanisms for model-based performance prediction of critical systems","2012","ISARCS'12 - Proceedings of the 3rd International ACM SIGSOFT Symposium on Architecting Critical Systems","","","","21","30","9","","10.1145/2304656.2304660","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864069777&doi=10.1145%2f2304656.2304660&partnerID=40&md5=3c679096c8d9312eb7c7d86cf4734f70","Security attacks aim to system vulnerabilities that may lead to operational failures. In order to react to attacks software designers use to introduce Fault-Tolerant Techniques (FTTs), such as recovery procedures, and/or Security Mechanisms (SMs), such as encryption of data. FTTs and SMs inevitably consume system resources, hence they influence the system performance, even affecting its full operability. The goal of this paper is to provide a model-based methodology able to quantitatively estimate the performance degradation due to the introduction of FTTs and/or SMs aimed at protecting critical systems. Such a methodology is able to inform software designers about the performance degradation the system may incur, thus supporting them to find appropriate security strategies while meeting performance requirements. This approach has been applied to a case study in the E-commerce domain, whose experimental results demonstrate its effectiveness. Copyright © 2012 ACM.","Critical systems; Fault-tolerant techniques; Model-based performance prediction; Security mechanisms","Degradation; Critical systems; E-commerce domains; Fault-tolerant; Operational failures; Performance degradation; Performance prediction; Performance requirements; Recovery procedure; Security attacks; Security mechanism; Security strategies; Software designers; System resources; System vulnerability; Security of data","Conference paper","Final","","Scopus","2-s2.0-84864069777"
"Shin Y.; Williams L.","Shin, Yonghee (24492397200); Williams, Laurie (35565101900)","24492397200; 35565101900","An initial study on the use of execution complexity metrics as indicators of software vulnerabilities","2011","Proceedings - International Conference on Software Engineering","","","","1","7","6","","10.1145/1988630.1988632","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959548762&doi=10.1145%2f1988630.1988632&partnerID=40&md5=18f117101516f70e98ce7a92de9706e1","Allocating code inspection and testing resources to the most problematic code areas is important to reduce development time and cost. While complexity metrics collected statically from software artifacts are known to be helpful in finding vulnerable code locations, some complex code is rarely executed in practice and has less chance of its vulnerabilities being detected. To augment the use of static complexity metrics, this study examines execution complexity metrics that are collected during code execution as indicators of vulnerable code locations. We conducted case studies on two large size, widely-used open source projects, the Mozilla Firefox web browser and the Wireshark network protocol analyzer. Our results indicate that execution complexity metrics are better indicators of vulnerable code locations than the most commonly-used static complexity metric, lines of source code. The ability of execution complexity metrics to discriminate vulnerable code locations from neutral code locations and to predict vulnerable code locations vary depending on projects. However, the vulnerability prediction models using execution complexity metrics are superior to the models using static complexity metrics in reducing inspection effort. © 2011 ACM.","Complexity metrics; Execution metrics; Software security; Software vulnerability prediction","Computational complexity; Forecasting; Internet protocols; Mathematical models; Network protocols; Security of data; Software engineering; Software testing; User interfaces; Web browsers; Code execution; Code inspections; Complex codes; Complexity metrics; Development time; Execution metrics; Firefox web browser; Large sizes; Mozilla; Open source projects; Prediction model; Protocol analyzers; Software artifacts; Software security; Software vulnerabilities; Software vulnerability prediction; Source codes; Wireshark; Cobalt compounds","Conference paper","Final","","Scopus","2-s2.0-79959548762"
"Nie C.; Zhao X.; Chen K.; Han Z.","Nie, Chujiang (36606636600); Zhao, Xianfeng (55623697900); Chen, Kai (57051675000); Han, Zhengqing (57220946053)","36606636600; 55623697900; 57051675000; 57220946053","An software vulnerability number prediction model based on micro-parameters","2011","Jisuanji Yanjiu yu Fazhan/Computer Research and Development","48","7","","1279","1287","8","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051575318&partnerID=40&md5=ebfb56684b0efa1ed2a414c916690510","As the cost caused by software vulnerabilities keeps increasing, people pay more and more attention to the researches on the vulnerability. Although discovering vulnerability is difficult because of the defect of vulnerability analysis, to predict the number of vulnerabilities is very useful in some domain, such as information security assessment. At present, the main methods to estimate the density of the vulnerabilities focus on the macro level, but they can not reflect the essential of vulnerability. A prediction model based on micro-parameter is proposed to predict the number of vulnerability with the micro-parameters of software, and it extracts the typical micro-parameters from some software series for the purpose of discovering the relationship between the vulnerability number and micro-parameters. With the hypothesis of vulnerability inheriting, the prediction model abstracts the micro-parameters from software and tries to find a linear relationship between the vulnerability number and some micro-parameters. This model also gives a method to predict the vulnerability number of software with its micro-parameters and the vulnerability number of its previous versions. This method is verified with 7 software series, and the results show the prediction model is effective.","History vulnerability; Inherited vulnerability; Microscopic parameters; Software analysis; Vulnerability predicts","Mathematical models; Security of data; Inherited vulnerability; Linear relationships; Microscopic parameters; Prediction model; Software analysis; Software vulnerabilities; Vulnerability analysis; Vulnerability predicts; Forecasting","Article","Final","","Scopus","2-s2.0-80051575318"
"Sharma V.S.; Trivedi K.S.","Sharma, Vibhu Saujanya (55463051100); Trivedi, Kishor S. (7102364865)","55463051100; 7102364865","Architecture based analysis of performance, reliability and security of software systems","2005","Proceedings of the Fifth International Workshop on Software and Performance, WOSP'05","","","","217","227","10","","10.1145/1071021.1071046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749038912&doi=10.1145%2f1071021.1071046&partnerID=40&md5=3ec55cd387f80d7f72e5eff19380343c","With software systems becoming more complex, and handling diverse and critical applications, the need for their thorough evaluation has become ever more important at each phase of software development. With the prevalent use of component-based design, the software architecture as well as the behavior of the individual components of the system needs to be taken into account when evaluating it. In recent past a number of studies have focused on architecture based reliability estimation. But areas such as security and cache behavior still lack such an approach. In this paper we propose an architecture based unified hierarchical model for software reliability, performance, security and cache behavior prediction. We define a metric called the vulnerability index of a software component for quantifying its (in)security. We provide expressions for predicting the overall behavior of the system based on the characteristics of individual components, which also takes into account second order architectural effects for providing an accurate prediction. This approach also facilitates the identification of reliability, performance, security and cache performance bottlenecks. In addition we illustrate how the approach could be applied to software systems by case studies and also provide expressions to perform sensitivity analysis. Copyright 2005 ACM.","Performance; Reliability; Security","Buffer storage; Computer architecture; Computer software; Reliability; Security systems; Software engineering; Cache behavior; Critical applications; Performance assesment; Reliability estimation; Systems analysis","Conference paper","Final","","Scopus","2-s2.0-33749038912"
"Fabbri A.G.; Chung C.-J.F.; Jang D.-H.","Fabbri, A.G. (7102285974); Chung, C.-J.F. (57213356844); Jang, D.-H. (26426175500)","7102285974; 57213356844; 26426175500","A software approach to spatial predictions of natural hazards and consequent risks","2004","Management Information Systems","9","","","289","305","16","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-11844274740&partnerID=40&md5=1793781afb6e30ed88866be10e574bf6","The development of a computer system for spatial analysis for the predictive mapping of natural hazards was discussed. A three stage strategy was developed that consisted of construction of a hazard predictive map using spatial databases for time partitioned distributions of future hazardous events. Risk maps were generated with the introduction of socio-economic factors representing assumed or established vulnerability levels. An application from a study area affected by mass movements in Korea was used to demonstrate the analytical structutre and modeling power implied in the software approach as a fundamental tool for decision making.","Analytical strategy; Cross-validation; Landslide hazard; Mathematical models; Prediction; Risk; Software","Computer systems; Database systems; Geographic information systems; Hazards; Image analysis; Landslides; Mapping; Mathematical models; Risk management; Analytical strategy; Cross-validation; Prediction; Risk; Computer software","Conference paper","Final","","Scopus","2-s2.0-11844274740"
"Nguyen V.H.; Tran L.M.S.","Nguyen, Viet Hung (57199220026); Tran, Le Minh Sang (36642760300)","57199220026; 36642760300","Predicting vulnerable software components with dependency graphs","2010","6th International Workshop on Security Measurements and Metrics, MetriSec 2010","","","1853923","","","","","10.1145/1853919.1853923","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649393441&doi=10.1145%2f1853919.1853923&partnerID=40&md5=8565d373696e49b66a6fd56f3206b6fd","Security metrics and vulnerability prediction for software have gained a lot of interests from the community. Many software security metrics have been proposed e.g., complexity metrics, cohesion and coupling metrics. In this paper, we propose a novel code metric based on dependency graphs to predict vulnerable components. To validate the efficiency of the proposed metric, we conduct a prediction model which targets the JavaScript Engine of Firefox. In this experiment, our prediction model has obtained a very good result in term of accuracy and recall rates. This empirical result is a good evidence showing dependency graphs are also a good option for early indicating vulnerability. © 2010 ACM.","prediction; vulnerability","Forecasting; Mathematical models; Code metrics; Complexity metrics; Dependency graphs; Empirical results; Firefox; Javascript; prediction; Prediction model; Recall rate; Security metrics; Software component; Software security; vulnerability; Security of data","Conference paper","Final","","Scopus","2-s2.0-78649393441"
"Akbar T.A.; Lin H.; DeGroote J.","Akbar, Tahir Ali (36665816400); Lin, Henry (57192807291); DeGroote, John (23027165900)","36665816400; 57192807291; 23027165900","Development and evaluation of GIS-based ArcPRZM-3 system for spatial modeling of groundwater vulnerability to pesticide contamination","2011","Computers and Geosciences","37","7","","822","830","8","","10.1016/j.cageo.2011.01.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958724676&doi=10.1016%2fj.cageo.2011.01.011&partnerID=40&md5=9c59be1f55fe8b367ebb15e464e4b313","The objectives of this study were to develop and evaluate a GIS-based modeling system called ArcPRZM-3 for spatial modeling of pesticide leaching potential from soil surface towards groundwater. The ArcPRZM-3 was developed by coupling a commonly used FORTRAN-based Pesticide Root Zone Model version 3 (PRZM-3) with user-friendly input and output interfaces through links to GIS using customized programming. A Visual PRZM-3 interface simplifies the entry of model inputs and links to the databases of crops, soils, and pesticides. The ArcPRZM-3 produces user-friendly outputs from the PRZM-3 batch simulations in the form of tables, charts, and maps. The Visual PRZM-3 can be used to run a single simulation for site-specific studies or simultaneous multiple simulations for spatial distributed modeling. The ArcPRZM-3 was applied to simulate maximum dissolved bentazon concentration at 0.75. m soil depth for a period of 2 years. These simulation results were used to develop a health risk map for Woodruff County, Arkansas, based on the U.S. Environmental Protection Agency's Lifetime Health Advisory Level (USEPA-LHA) of bentazon in drinking water. The ArcPRZM-3 was evaluated by comparing the bentazon detection data from monitoring wells from the same area with the predicted bentazon health risk map. The results showed that 100% of the wells where bentazon was detected were within the high risk category based on the ArcPRZM-3 predictions. However, uncertainty in the ArcPRZM-3 model and the timing of groundwater well monitoring could both complicate the interpretations of the ArcPRZM-3 simulation results. © 2011 Elsevier Ltd.","ArcPRZM-3; GIS; Groundwater; Groundwater spatial modeling; Pesticide; PRZM-3; Vulnerability assessment","Arkansas; United States; Woodruff County; Air pollution control; Computer simulation; Environmental Protection Agency; Geologic models; Groundwater; Groundwater pollution; Health; Leaching; Pesticides; Potable water; Rating; Soils; Wells; ArcPRZM-3; Distributed modeling; Groundwater vulnerability; Groundwater wells; High-risk categories; Input and outputs; Model inputs; Modeling systems; Monitoring wells; Pesticide leaching; PRZM-3; Root zone; Simulation result; Site-specific; Soil depth; Soil surfaces; Spatial modeling; U.S. Environmental Protection Agency; Vulnerability assessments; computer simulation; concentration (composition); database; GIS; groundwater; leaching; numerical model; pesticide; pollutant transport; software; soil depth; spatial analysis; Health risks","Article","Final","","Scopus","2-s2.0-79958724676"
"Zheng H.; Wang H.; Azuaje F.","Zheng, Huiru (8982328500); Wang, Haiying (55914042800); Azuaje, Francisco (7003522661)","8982328500; 55914042800; 7003522661","ENelator: A simulation system for large-scale vulnerability analysis of species-, disease- and process-specific protein networks","2010","Journal of Computational Science","1","4","","197","205","8","","10.1016/j.jocs.2010.08.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649526011&doi=10.1016%2fj.jocs.2010.08.002&partnerID=40&md5=f39e97648518cf3a2076908ed9566dae","The identification of vulnerabilities in protein networks is a promising approach to predicting potential therapeutic targets. Different methods have been applied to domain-specific applications, with an emphasis on single-node deletions. There is a need to further assess significant associations between vulnerability, functional essentiality and topological features across species, processes and diseases. This requires the development of open, user-friendly systems to generate and test existing hypotheses about the vulnerability of networks in the face of dysfunctional components. We implemented methodologies to estimate the vulnerability of different networks to the dysfunction of different combinations of components, under random and directed attack scenarios. To demonstrate the relevance of our approaches and software, published protein-protein interaction (PPI) networks from Saccharomyces cerevisiae, Escherichia coli and Homo sapiens were analyzed. A PPI network implicated in the development of human heart failure, and signaling networks relevant to Caspase3 and P53 regulation were also investigated. Known essential proteins (individually or in groups) have no detectable effects on network stability. Some of the most vulnerable proteins are neither essential nor hubs. Known diagnostic biomarkers have little effect on the communication efficiency of the disease network. Predictions made on the signaling networks are consistent with recent experimental evidence. Our system, which integrates other quantitative measures, can assist in the identification of potential drug targets and systems-level properties. The system for large-scale analysis of random and directed attacks is freely available, as a Cytoscape plugin, on request from the authors. © 2010 Elsevier B.V.","Network robustness; Network vulnerability analysis; Protein networks; Shortest path length; Simulation of biological systems","Bioinformatics; Diagnosis; Escherichia coli; Proteins; Signaling; Yeast; Communication efficiency; Domain-specific application; Network robustness; Network vulnerability analysis; Protein network; Protein-protein interaction networks; Shortest path; Vulnerability analysis; Network security","Article","Final","","Scopus","2-s2.0-78649526011"
"Woon I.M.Y.; Kankanhalli A.","Woon, Irene M.Y. (6508177096); Kankanhalli, Atreyi (6508127144)","6508177096; 6508127144","Investigation of IS professionals' intention to practise secure development of applications","2007","International Journal of Human Computer Studies","65","1","","29","41","12","","10.1016/j.ijhcs.2006.08.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751182668&doi=10.1016%2fj.ijhcs.2006.08.003&partnerID=40&md5=c5f1ba6448c5c9f162d1e9acd0356194","It is well known that software errors may lead to information security vulnerabilities, the breach of which can have considerable negative impacts for organizations. Studies have found that a large percentage of security defects in e-business applications are due to design-related flaws, which could be detected and corrected during applications development. Traditional methods of managing software application vulnerabilities have often been ad hoc and inadequate. A recent approach that promises to be more effective is to incorporate security requirements as part of the application development cycle. However, there is limited practice of secure development of applications (SDA) and lack of research investigating the phenomenon. Motivated by such concerns, the goal of this research is to investigate the factors that may influence the intention of information systems (IS) professionals to practise SDA, i.e., incorporate security as part of the application development lifecycle. This study develops two models based on the widely used theory of planned behaviour (TPB) and theory of reasoned action (TRA) to explain the phenomenon. Following model operationalization, a field survey of 184 IS professionals was conducted to empirically compare the explanatory power of the TPB-based model versus the TRA-based model. Consistent with TPB and TRA predictions, attitude and subjective norm were found to significantly impact intention to practise SDA for the overall survey sample. Attitude was in turn determined by product usefulness and career usefulness of SDA, while subjective norm was determined by interpersonal influence, but not by external influence. Contrary to TPB predictions, perceived behavioural controls, conceptualized in terms of self-efficacy and facilitating conditions, had no significant effect on intention to practise SDA. Thus, a modified TRA-based model was found to offer the best explanation of behavioural intention to practise SDA. Implications for research and information security practice are suggested. © 2006 Elsevier Ltd. All rights reserved.","Information security; Secure development of applications; Theory of planned behaviour; Theory of reasoned action","Behavioral research; Computer software; Electronic commerce; Error correction; Industrial engineering; Network protocols; Societies and institutions; E-business; Secure development of applications (SDA); Theory of planned behaviour (TPB); Theory of reasoned action (TRA); Security of data","Article","Final","","Scopus","2-s2.0-33751182668"
"Conant J.A.","Conant, John A. (7003481302)","7003481302","Modeling of spectral emission images from fully 3D gaseous combustion plumes","2001","Proceedings of SPIE - The International Society for Optical Engineering","4448","","","8","15","7","","10.1117/12.449369","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035760689&doi=10.1117%2f12.449369&partnerID=40&md5=ed324a39e460379d5e760728d2e7a8d7","The prediction of infrared emissions from gaseous plumes is an important tool for remote sensing, heat transfer, and vulnerability assessment. We have developed a software model called ""RAD3D"" that generates hyperspectral images of 3D gaseous plumes. The model directly ray-traces through gas volumes input in the PLOT3D format. In addition to single and arrays of lines of sight, the model will automatically generate additional lines of sight to resolve image structure. Outputs of the model include hyperspectral and in-band radiances and transmittances, along with range-to-plume images. Opaque body regions may be embedded within the gas volume.","Band; Gas plume; Hyperspectral; Image; Infrared; Model; PLOT3D; Spectral","Computer simulation; Gas emissions; Heat transfer; Ray tracing; Remote sensing; Thermal plumes; Thermodynamic stability; Gaseous infrared radiation transport; Spectral emission images; Three dimensional gaseous combustion plumes; Combustion","Conference paper","Final","","Scopus","2-s2.0-0035760689"
"Dolce M.; Cardone D.; Pardi L.","Dolce, M. (6603674721); Cardone, D. (6603941256); Pardi, L. (23668629300)","6603674721; 6603941256; 23668629300","Seismic risk management of highway bridges","2006","Proceedings of the 3rd International Conference on Bridge Maintenance, Safety and Management - Bridge Maintenance, Safety, Management, Life-Cycle Performance and Cost","","","","319","320","1","","10.1201/b18175-121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-56749179936&doi=10.1201%2fb18175-121&partnerID=40&md5=3edd3ac3c2d7f4ca1d5174250cb39d19","The Italian research project S.A.G.G.I. (Advanced systems for the global management of infrastructures) is aimed at developing an integrated system for the effective management of the transport infrastructures. Bridges and viaducts can be considered as the critical elements of a road network, due to their own characteristics and the considerable consequences, in terms of both repair costs and circulation problems, implied by their degradation and damage. Besides the progressive decay of the structure, a sudden and heavy damage can occur due to an earthquake. Earthquakes of medium-high intensity have high probability of occurrence during the lifetime of a bridge in many regions, such as in Italy, where a new seismic zonation in 2003 has emphasised the seismic inadequacies of most bridges and viaducts. Moreover, the slow degradation of their structural materials can significantly change their strength and ductility and, then, increase their seismic vulnerability. The above considerations justify the needs for setting up criteria and models to evaluate the seismic vulnerability and risk of bridges and define economically competitive maintenance strategies. That is why S.A.G.G.I. aims also at developing prediction models of the degradation and of the maintenance needs of bridges and viaducts in seismic areas. The research program will be accomplished in three steps: I. Critically appraising the state of the art on the seismic vulnerability of bridges and viaducts and of the techniques for its reduction; II. Setting up innovative models to evaluate the seismic resistance of the most common structural types, also validated by an extensive experimental investigation on new and deteriorated models; III. Setting up operative models for the dynamic evaluation of the seismic vulnerability and of the associated seismic risk of highway bridges and viaducts and implementation of an algorithm exploiting the data base of the Autostrade per l'Italia Company. The final objective is to develop a tool which is able to evaluate the seismic risk associated to the structure (i) as built, (ii) taking account of the current degradation state, (iii) taking account of the natural evolution of the decay process and the programmed maintenance and/or seismic upgrading interventions. The second step of S.A.G.G.I., in particular, is related to the set up of an innovative model for the evaluation of the seismic resistance of the most frequent types of highway bridges and viaducts. Such a model shall permit to predict the expected damage for different intensities of the reference earthquake, as described in terms of peak ground acceleration and response spectrum. The model will necessarily make reference to the structural types and to the geometrical characteristics which are more frequently found in the Italian highway network. These will be selected on the basis of the information contained in the database of Autostrade per l'Italia. Moreover, in the model the typical design situations of the selected structural types, especially concerning percent ratios and arrangement of steel reinforcement in reinforced concrete members, will be taken into account. These data will be drawn through the critical appraisal of design documents of real bridges and viaducts belonging to the selected types. The model for the evaluation of the seismic resistance will permit to evaluate the performance levels of the structure under seismic actions of different intensities, on the basis of the available data on the specific structure and of the typical structural characteristics. The model will also be able to account for eventual decay situations and for its time evolution, thus evaluating the consequent performance reduction (increase of vulnerability) of the structure under seismic actions. The numerical results provided by the model will be compared to the results of experimental tests that will be carried out on large scale structural models, at the Laboratory of Structures of the University of Basilicata. The experimental models will reproduce structural pier-deck sub-assemblages in 1/4 scale. For some of them the decay conditions will be reproduced with equivalent decay conditions, obtained by suitably reducing the strength characteristics of the materials or the local reinforcement section. They will be subjected to cyclic tests, with increasing amplitude, to evaluate their lateral strength and ductility capacity. Also seismic simulation tests using the pseudodynamic method will be carried out on some of them, to evaluate their actual seismic resistance. The deck will be treated as a rigid mass concentrated at the top of the pier, connected to the pier through bearing devices. The design of the pier of the models will be referred to the design documents of the pre-selected real structures. The structural models will be also tested in the retrofitted configuration, obtained by introducing different seismic isolation devices, instead of the bearings, between the pier and the deck. The third step of S.A.G.G.I., in particular, is related to the set up of an operative method for seismic vulnerability and risk evaluations. Such a method will be developed with reference to the most common bridge and viaduct types, previously selected. The method shall take account of both the structural characteristics of the bridge and of fhe modifications of strength and ductility due to the decay of the materials and/or rehabilitation interventions and/or seismic retrofit interventions. The method will estimate the damage produced by earthquakes of given intensity, described in terms of peak ground accelerations and response spectra, on the bridge under different hypotheses of decay progress, starting from the no decay condition. Therefore it will be possible to examine different situations, in relation to the time evolution of the state of degradation, namely; (i) seismic risk associated to the current state of the structure, (ii) seismic risk associated to the evolution of the decay according to the predicted progress, (iii) seismic risk associated to the evolution of the decay and to programmed rehabilitation interventions, (iv) seismic risk associated to the previous conditions with additional seismic retrofit interventions. Also the algorithm of evaluation of the seismic risk will be purposely developed for the most frequent previously selected types of highway bridges and viaducts. Once the algorithm is translated into a software, the seismic risk evaluation algorithm will permit to make predictions of the expected damage, in one year or for a given time interval, of a bridge or viaduct in a given site. It will account for the actual state of degradation of the structure and its probable evolution, as well as of eventaal rehabilitation and/or seismic retrofit interventions. Therefore it will permit to define an ""optimal"" maintenance and upgrading strategy with respect to the basic and decay-induced seismic vulnerability problems. The algorithm and the software will be referred to the database of Autostrade per l'Italia. In the paper, the first results of the research project, related to the state on the seismic performances, assessment and retrofit of existing bridges, as well as the main aspects of the research developments expected in the research project are described. © 2006 Taylor & Francis Group.","","Costs; Decay (organic); Ductility; Earthquake engineering; Forecasting; Highway administration; Highway bridges; Highway planning; Life cycle; Maintenance; Motor transportation; Piers; Reinforced concrete; Retrofitting; Risk management; Risks; Safety engineering; Strength of materials; Experimental investigations; Geometrical characteristics; Implementation of an algorithm; Large-scale structural models; Reinforced concrete member; Seismic isolation devices; Structural characteristics; Transport infrastructure; Earthquakes","Conference paper","Final","","Scopus","2-s2.0-56749179936"
"Gupta Aaron Das","Gupta, Aaron Das (55491953100)","55491953100","Evaluation of drag loading models in overturning response codes","1990","","","","","57","65","8","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025598765&partnerID=40&md5=ff6ee7434cfd4cd8952c1a214b66c73d","The dynamic overturning response of Army vehicles with shelters and trailers as well as overturn susceptible structures has been of considerable interest to the defense community since critical structures and internal equipments can be damaged resulting in system malfunction and reduction of vehicle performance jeopardizing the primary mission objective. To overcome vulnerability of vehicles and associated equipments to overturning, the Army is actively engaged in a hardening program which will result in overturn mitigation and increased survivability. Flexible multibody dynamics programs can be used to predict overturning response of structures due to transient overpressure loading and facilitate evaluation of mitigation devices such as outriggers, cables and guy wires. However, accuracy of overturning prediction is dominated by the loading model during the drag phase when the structure becomes unstable. To assess validity of the loading model, accuracy of drag coefficients as a function of the roll angle and flow velocity should be evaluated. The current investigation is devoted to a comparative evaluation of drag coefficients used in loading models in overturning response codes as well as any experimental data that may be available.","","Aerodynamics - Drag; Computer Software; Dynamics - Computer Applications; Mathematical Models; Structural Analysis; Army Vehicles; Drag Loading Models; Dynamic Overturning Response; Flexible Multibody Dynamics Programs; Overturning Response Codes; Software Package Minitruck; Military Vehicles","Conference paper","Final","","Scopus","2-s2.0-0025598765"
"Woo S.-W.; Joh H.; Alhazmi O.H.; Malaiya Y.K.","Woo, Sung-Whan (23053591400); Joh, Hyunchul (26648448600); Alhazmi, Omar H. (9239998700); Malaiya, Yashwant K. (35619571700)","23053591400; 26648448600; 9239998700; 35619571700","Modeling vulnerability discovery process in Apache and IIS HTTP servers","2011","Computers and Security","30","1","","50","62","12","","10.1016/j.cose.2010.10.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251599901&doi=10.1016%2fj.cose.2010.10.007&partnerID=40&md5=32238f7737c862e899725889cf3de8d2","Vulnerability discovery models allow prediction of the number of vulnerabilities that are likely to be discovered in the future. Hence, they allow the vendors and the end users to manage risk by optimizing resource allocation. Most vulnerability discovery models proposed use the time as an independent variable. Effort-based modeling has also been proposed, which requires the use of market share data. Here, the feasibility of characterizing the vulnerability discovery process in the two major HTTP servers, Apache and IIS, is quantitatively examined using both time and effort-based vulnerability discovery models, using data spanning more than a decade. The data used incorporates the effect of software evolution for both servers. In addition to aggregate vulnerabilities, different groups of vulnerabilities classified using both the error types and severity levels are also examined. Results show that the selected vulnerability discovery models of both types can fit the data of the two HTTP servers very well. Results also suggest that separate modeling for an individual class of vulnerabilities can be done. In addition to the model fitting, predictive capabilities of the two models are also examined. The results demonstrate the applicability of quantitative methods to widely-used products, which have undergone evolution. © 2010 Elsevier Ltd. All rights reserved.","Quantitative modeling; Risk evaluation; Security; Vulnerability discovery model (VDM); Web server","Competition; Security of data; Servers; Web services; Quantitative modeling; Risk evaluation; Security; Vulnerability discovery; Web servers; HTTP","Article","Final","","Scopus","2-s2.0-79251599901"
"Saeidi A.; Deck O.; Verdel T.","Saeidi, Ali (55584791825); Deck, Olivier (6506168525); Verdel, Thierry (8551924000)","55584791825; 6506168525; 8551924000","Comparison of Building Damage Assessment Methods for Risk Analysis in Mining Subsidence Regions","2013","Geotechnical and Geological Engineering","31","4","","1073","1088","15","","10.1007/s10706-013-9633-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880139677&doi=10.1007%2fs10706-013-9633-7&partnerID=40&md5=10b4f8bd1713480d06ebd650fa6ecf63","The occurrence of subsidence phenomena in urban regions may induce small to severe damage to buildings. Many methods are provided in the literature to assess buildings damage. Most of these methods are empirical and use the horizontal ground strain as a subsidence intensity in the vicinity of a building. Application and comparison of these methods with a case study is the main objective of this paper. This comparison requires some harmonization of the existing methods and the development of a software, which combines the subsidence hazard prediction, the damage evaluation methods and a database of buildings with structural parameters as well as the geographical coordinates of the buildings An additional results is the development of a method for the prediction of the horizontal ground strain in the vicinity of each building. Results are given as a map of damaged buildings for the case study and the different existing methods with some statistical calculations such as the mean and the standard deviation of damage in the city. Comparison of these results allows identification of the ""safer"" method that give the higher mean of damage. The comparison of the calculated results and observed damage in Lorrain region show that, the Dzegeniuk et al. methods is more realistic in comparison of the other empirical methods. © 2013 Springer Science+Business Media Dordrecht.","Building vulnerability; Damage; Horizontal ground strain; Mining subsidence","France; Lorraine; Cutting machines (mining); Damage detection; Subsidence; Building vulnerabilities; Damage; Geographical coordinates; Mining subsidence; Standard deviation; Statistical calculations; Structural parameter; Subsidence hazards; building; comparative study; damage; hazard assessment; mining; parameterization; prediction; strain; subsidence; urban area; vulnerability; Buildings","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84880139677"
"Shar L.K.; Tan H.B.K.","Shar, Lwin Khin (36802571200); Tan, Hee Beng Kuan (7403011293)","36802571200; 7403011293","Predicting common web application vulnerabilities from input validation and sanitization code patterns","2012","2012 27th IEEE/ACM International Conference on Automated Software Engineering, ASE 2012 - Proceedings","","","","310","313","3","","10.1145/2351676.2351733","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866920422&doi=10.1145%2f2351676.2351733&partnerID=40&md5=19124a9a4c9dd3934e195481877d4673","Software defect prediction studies have shown that defect predictors built from static code attributes are useful and effective. On the other hand, to mitigate the threats posed by common web application vulnerabilities, many vulnerability detection approaches have been proposed. However, finding alternative solutions to address these risks remains an important research problem. As web applications generally adopt input validation and sanitization routines to prevent web security risks, in this paper, we propose a set of static code attributes that represent the characteristics of these routines for predicting the two most common web application vulnerabilities-SQL injection and cross site scripting. In our experiments, vulnerability predictors built from the proposed attributes detected more than 80% of the vulnerabilities in the test subjects at low false alarm rates. Copyright 2012 ACM.","Defect prediction; Empirical study; Input validation and sanitization; Static code attributes; Web application vulnerabilities","Defects; Forecasting; Network security; Software engineering; Defect prediction; Empirical studies; Sanitization; Static codes; Web application vulnerability; World Wide Web","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84866920422"
"Gao Z.-W.; Yao Y.; Rao F.; Liu Y.-Z.; Luo P.","Gao, Zhi-Wei (58597935000); Yao, Yao (57199732805); Rao, Fei (57215746607); Liu, Yan-Zhao (55184994100); Luo, Ping (55705278900)","58597935000; 57199732805; 57215746607; 55184994100; 55705278900","Predicting model of vulnerabilities based on the type of vulnerability severity","2013","Tien Tzu Hsueh Pao/Acta Electronica Sinica","41","9","","1784","1787","3","","10.3969/j.issn.0372-2112.2013.09.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888599781&doi=10.3969%2fj.issn.0372-2112.2013.09.018&partnerID=40&md5=5d4b8a407e6f11b14e619e05c66585eb","There are many kinds of software vulnerability prediction models which are capable of predicting the total number and the time interval of occurrence of vulnerabilities in the software. But none of them can predict the severity of software vulnerabilities. However, in some cases, such as software credibility, we have to consider the total number of software vulnerabilities and time interval as well as the vulnerability severity affecting the trustworthiness of software. Considering the impact of the vulnerability severity, the application and risk control of software is also very important in the traditional software security. Based on the traditional Markov model, we classified the severity of software vulnerabilities occurrence, proposed a new software vulnerability prediction mathematical model. The model can not only predict the total number of software vulnerability and the time interval, but also the total number vulnerabilities of each class as well as the type of the vulnerabilities. Our tests showed that it has better accuracy, and the type of information that other prediction models can not offer.","Markov chain; Predicting model of vulnerabilities; Prediction with classificaiton; Third party prediction of vulnerabilities","Forecasting; Markov processes; Mathematical models; Network security; Markov model; Predicting models; Prediction model; Risk controls; Software security; Software vulnerabilities; Third parties; Time interval; Application programs","Article","Final","","Scopus","2-s2.0-84888599781"
"Ahmed M.S.; Al-Shaer E.; Taibah M.; Khan L.","Ahmed, Mohammad Salim (55463594500); Al-Shaer, Ehab (6602187244); Taibah, Mohamed (24722340700); Khan, Latifur (26643247800)","55463594500; 6602187244; 24722340700; 26643247800","Objective risk evaluation for automated security management","2011","Journal of Network and Systems Management","19","3","","343","366","23","","10.1007/s10922-010-9177-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052638739&doi=10.1007%2fs10922-010-9177-6&partnerID=40&md5=1643400c3add2aefb3720e375d46cdfe","Network security depends on a number of factors. And a common characteristic of these factors is that they are dynamic in nature. Such factors include new vulnerabilities and threats, the network policy structure and traffic. These factors can be divided into two broad categories. Network risk and service risk. As the name implies, the former one corresponds to risk associated with the network policy whereas the later one depends on the services and software running on the system. Therefore, evaluating security from both the service and policy perspective can allow the management system to make decisions regarding how a system should be changed to enhance security as par the management objective. Such decision making includes choosing between alternative security architectures, designing security countermeasures, and to systematically modify security configurations to improve security. As there may be real time changes to the network threat, this evaluation must be done dynamically to handle such changes. In this paper, we provide a security metric framework that quantifies objectively the most significant security risk factors, which include existing vulnerabilities, historical trend of vulnerabilities of the remotely accessible services, prediction of potential vulnerabilities for these services and their estimated severity, unused address space and finally propagation of an attack within the network. These factors cover both the service aspect and the network aspect of risk toward a system. We have implemented this framework as a user-friendly tool called Risk based prOactive seCurity cOnfiguration maNAger (ROCONA) and showed how this tool simplifies security configuration management of services and policies in a system using risk measurement and mitigation. We also combine all the components into one single metric and present validation experiments using real-life vulnerability data from National Vulnerability Database (NVD) and show comparison with two existing risk measurement tools. © Springer Science+Business Media, LLC 2010.","Attack immunity; Attack propagation; Quality of protection; Risk prediction; Security evaluation; Vulnerability measure","Computer crime; Management; Measurements; Risk assessment; Risk perception; Attack immunity; Attack propagation; Quality of protections; Risk prediction; Security evaluation; Vulnerability measure; Network security","Article","Final","","Scopus","2-s2.0-80052638739"
"Arena P.; Patanè L.; Caruso S.; Anastasi M.; Cannata A.","Arena, P. (23003603300); Patanè, L. (6603946091); Caruso, S. (57195351037); Anastasi, M. (55350235000); Cannata, A. (57197647735)","23003603300; 6603946091; 57195351037; 55350235000; 57197647735","A software framework for the generation of dynamic vulnerability maps for risk assessment","2009","WIT Transactions on the Built Environment","110","","","369","379","10","","10.2495/DMAN090321","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865802079&doi=10.2495%2fDMAN090321&partnerID=40&md5=bfdbe2de154a4165dfa48456771e46ce","In this paper a software framework that is able to generate dynamic vulnerability maps for risk assessment is proposed. A series of models for different risk scenarios have been considered and described together with their implementation in the developed software framework. Finally the visualization of the acquired data and simulation results can be used as a support for real-time emergency planning. © 2009 WIT Press.","Disaster management; Prediction models","Computer programming; Data visualization; Disaster prevention; Disasters; Risk assessment; Risk perception; Visualization; Disaster management; Emergency planning; Prediction model; Risk scenarios; Software frameworks; Vulnerability maps; Health risks","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-84865802079"
"Crawford J.E.; Malvar L.J.; Wesevich J.W.; Valancius J.; Reynolds A.D.","Crawford, John E. (55722752400); Malvar, L. Javier (7004242728); Wesevich, James W. (6506359770); Valancius, Joseph (6508133648); Reynolds, Aaron D. (7201478307)","55722752400; 7004242728; 6506359770; 6508133648; 7201478307","Retrofit of reinforced concrete structures to resist blast effects","1997","ACI Structural Journal","94","4","","371","377","6","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031186557&partnerID=40&md5=71f2e38af18351e20a2b84f0776dace5","Analyses were conducted to demonstrate the effectiveness of jacketing columns of existing reinforced concrete multistory buildings to improve their survivability to attacks by explosives. Different standoff distances, charge sizes, and steel and composite jackets were considered. Two building designs were analyzed: one in which the building members were designed primarily for gravity loads (UBC seismic zone 1) and one in which the members were designed to resist seismic loads (UBC seismic zone 4). Structural response predictions were performed with the three-dimensional Lagrangian finite element code DYNA3D, using a concrete material model especially designed to predict nonlinear concrete responses to explosive loads. The results indicate that jacketing can be an effective means to retrofit an existing facility to lessen its vulnerability to blast loads.","Blast effects; Composites; FRP; Reinforced concrete; Steel jacket","Blast resistance; Columns (structural); Computer software; Earthquake resistance; Finite element method; Gravitational effects; Mathematical models; Retrofitting; Structural analysis; Software package DYNA3D; Steel jackets; Reinforced concrete","Article","Final","","Scopus","2-s2.0-0031186557"
"Shar L.K.; Tan H.B.K.","Shar, Lwin Khin (36802571200); Tan, Hee Beng Kuan (7403011293)","36802571200; 7403011293","Predicting SQL injection and cross site scripting vulnerabilities through mining input sanitization patterns","2013","Information and Software Technology","55","10","","1767","1780","13","","10.1016/j.infsof.2013.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880843062&doi=10.1016%2fj.infsof.2013.04.002&partnerID=40&md5=077d790d98cf2de91099555f65577ced","Context SQL injection (SQLI) and cross site scripting (XSS) are the two most common and serious web application vulnerabilities for the past decade. To mitigate these two security threats, many vulnerability detection approaches based on static and dynamic taint analysis techniques have been proposed. Alternatively, there are also vulnerability prediction approaches based on machine learning techniques, which showed that static code attributes such as code complexity measures are cheap and useful predictors. However, current prediction approaches target general vulnerabilities. And most of these approaches locate vulnerable code only at software component or file levels. Some approaches also involve process attributes that are often difficult to measure. Objective This paper aims to provide an alternative or complementary solution to existing taint analyzers by proposing static code attributes that can be used to predict specific program statements, rather than software components, which are likely to be vulnerable to SQLI or XSS. Method From the observations of input sanitization code that are commonly implemented in web applications to avoid SQLI and XSS vulnerabilities, in this paper, we propose a set of static code attributes that characterize such code patterns. We then build vulnerability prediction models from the historical information that reflect proposed static attributes and known vulnerability data to predict SQLI and XSS vulnerabilities. Results We developed a prototype tool called PhpMinerI for data collection and used it to evaluate our models on eight open source web applications. Our best model achieved an averaged result of 93% recall and 11% false alarm rate in predicting SQLI vulnerabilities, and 78% recall and 6% false alarm rate in predicting XSS vulnerabilities. Conclusion The experiment results show that our proposed vulnerability predictors are useful and effective at predicting SQLI and XSS vulnerabilities. © 2013 Elsevier B.V. All rights reserved.","Data mining; Empirical study; Input sanitization; Static code attributes; Vulnerability prediction; Web application vulnerability","Applications; Data mining; Errors; Java programming language; Learning systems; World Wide Web; Cross-site scripting; Dynamic Taint Analysis; Empirical studies; Historical information; Sanitization; Static codes; Vulnerability detection; Web application vulnerability; Forecasting","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84880843062"
"Miller K.W.","Miller, Keith W. (57806255300)","57806255300","Technology, unemployment, and power","2013","IT Professional","15","6","6674034","10","11","1","","10.1109/MITP.2013.99","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890056024&doi=10.1109%2fMITP.2013.99&partnerID=40&md5=4a4f5f5f4b013d489da28446ec1ca172","Business organizations and people are making predictions and investments based on a recognition that smart machines are starting to make dramatic changes in the world of work and will likely make even more dramatic changes in the near future. Some people see this as an opportunity, while others see it as vulnerability, but many people see it as inevitable. © 1999-2012 IEEE.","Ethics; Information technology; IT professionals; Smart machines; Workforce","Computer applications; Software engineering; Business organizations; Ethics; IT professional; Smart machines; Workforce; Information technology","Article","Final","","Scopus","2-s2.0-84890056024"
"Bergstrom C.; Chuprun S.; Gifford S.; Maalouli G.","Bergstrom, Chad (7004918483); Chuprun, Scott (6603040972); Gifford, Steve (7005526830); Maalouli, Ghassan (7801432580)","7004918483; 6603040972; 7005526830; 7801432580","Software defined radio (SDR) special military applications","2002","Proceedings - IEEE Military Communications Conference MILCOM","1","","","383","388","5","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037003515&partnerID=40&md5=cd832ec6a43f93b4225d4996b176c0b5","Emerging conflict scenarios are highlighting US vulnerabilities to terrorist engagements and espionage. Warfighters and intelligence agencies must leverage the electromagnetic spectrum in new ways to ""put steel on target"" and collect information on adversaries. Emerging software defined radio systems (SDR) can be used well beyond their intended JTRS communications mission to locate threats in a ""stand-in"" signal collection mode, and coordinate offensive resources and other JTRS systems to neutralize the threat. In this manner, the programmable communication system is transformed into a flexible signal collection resource that locates hostile signals both in spectrum and geography, becoming an intelligence and combat multiplier. Ultimately, this approach will leverage the ubiquitous presence of SDR platforms on the near-future battlespace to significantly enhance warfighter survivability while presenting new opportunities for distributed signal collection and analysis.","","Algorithms; Fast Fourier transforms; Interference suppression; Matrix algebra; Military applications; Military electronic countermeasures; Military geology; Mobile radio systems; Radio transmitters; Signal filtering and prediction; Software engineering; Timing jitter; Digital modular radio; Espionage; Noncooperative geolocation; Signal collection; Software defined radio; Time difference of arrival; Military communications","Conference paper","Final","","Scopus","2-s2.0-0037003515"
"Antunes J.; Neves N.F.; Verissimo P.","Antunes, João (8397511600); Neves, Nuno Ferreira (7004622224); Verissimo, Paulo (7003870476)","8397511600; 7004622224; 7003870476","Detection and prediction of resource-exhaustion vulnerabilities","2008","Proceedings - International Symposium on Software Reliability Engineering, ISSRE","","","4700313","87","96","9","","10.1109/ISSRE.2008.47","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67249137066&doi=10.1109%2fISSRE.2008.47&partnerID=40&md5=139c6d0e4f624f743de255ad185c31e2","Systems connected to the Internet are highly susceptible to denial-of-service attacks that can compromise service availability, causing damage to customers and providers. Due to errors in the design or coding phases, particular client-server interactions can be made to consume much more resources than necessary easing the success of this kind of attack. To address this issue we propose a new methodology for the detection and identification of local resource-exhaustion vulnerabilities. The methodology also gives a prediction on the necessary effort to exploit a specific vulnerability, useful to support decisions regarding the configuration of a system, in order to sustain a certain attack magnitude. The methodology was implemented in a tool called PREDATOR that is able to automatically generate malicious traffic and to perform post-processing analysis to build accurate resource usage projections on a given target server. The validity of the approach was demonstrated with several synthetic programs and well-known DNS servers. © 2008 IEEE.","","Coding errors; Computer crime; Computer software selection and evaluation; Servers; Client server; Coding phasis; Denial of service attacks; DNS server; Malicious traffic; Post processing; Resource usage; Service availability; Software reliability","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-67249137066"
"Ding S.; Tan H.B.K.; Shar L.K.; Padmanabhuni B.M.","Ding, Sun (36805563500); Tan, Hee Beng Kuan (7403011293); Shar, Lwin Khin (36802571200); Padmanabhuni, Bindu Madhavi (54395923600)","36805563500; 7403011293; 36802571200; 54395923600","Towards a hybrid framework for detecting input manipulation vulnerabilities","2013","Proceedings - Asia-Pacific Software Engineering Conference, APSEC","1","","6805427","363","370","7","","10.1109/APSEC.2013.56","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936877880&doi=10.1109%2fAPSEC.2013.56&partnerID=40&md5=9b2f5fb957f580ab6fee966650af6e78","Input manipulation vulnerabilities such as SQL Injection, Cross-site scripting, Buffer Overflow vulnerabilities are highly prevalent and pose critical security risks. As a result, many methods have been proposed to apply static analysis, dynamic analysis or a combination of them, to detect such security vulnerabilities. Most of the existing methods classify vulnerabilities into safe and unsafe. They have both false-positive and false-negative cases. In general, security vulnerability can be classified into three cases: (1) provable safe; (2) provable unsafe; (3) unsure. In this paper, we propose a hybrid framework-Detecting Input Manipulation Vulnerabilities (DIMV), to verify the adequacy of security vulnerability defenses for input manipulation vulnerabilities by integrating formal verification with vulnerability prediction in a seamless way. The verification part takes into account sink predicates and effect of domain and custom specifications for detecting input manipulation vulnerabilities. Proving from specification is used as far as possible. Cases that cannot be proved are then predicted from the signatures mined. Our evaluation shows the practicality of the proposed framework. © 2013 IEEE.","Data mining; Formal verification; Framework; Input manipulation vulnerabilities; Input validation; Prediction; Specification; Verification; Vulnerability detection","Data mining; Forecasting; Formal verification; Mobile security; Network security; Software engineering; Specifications; Static analysis; Verification; Cross site scripting; False positive and false negatives; Framework; Hybrid framework; Input manipulation vulnerabilities; Input validation; Security vulnerabilities; Vulnerability detection; Security of data","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84936877880"
"Alhazmi O.H.; Malaiya Y.K.","Alhazmi, Omar H. (9239998700); Malaiya, Yashwant K. (35619571700)","9239998700; 35619571700","Prediction capabilities of vulnerability discovery models","2006","Proceedings - Annual Reliability and Maintainability Symposium","","","1677355","86","91","5","","10.1109/RAMS.2006.1677355","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250212150&doi=10.1109%2fRAMS.2006.1677355&partnerID=40&md5=dac6454c0ca08818346eea0b659cbbe6","Quantitative approaches for software security are needed for effective testing, maintenance and risk assessment of software systems. Vulnerabilities that are present in an operating system after its release represent a great risk. Vulnerability Discovery Models (VDMs) have been proposed to model vulnerability discovery and have has been fitted to vulnerability data against calendar time. The models have been shown to fit very well. In this paper, we investigate the prediction capabilities that these models offer by evaluating accuracy of predictions made with partial data. We examine both the recently proposed logistic model and a new linear model. In addition to VDMs, we consider static approaches to estimating some of the major attributes of the vulnerability discovery process, presenting a static approach to estimating the initial values of one of the VDM's parameters. We also suggest the use of constraints for parameter estimation during curve-fitting. Here we develop computational approaches for early applications of the models and examine the predictive capability of the models. We use data from Windows 98, Windows 2000 and Red Hat Linux 7.1. We examine the impact of using a specific constraint when the parameters of the logistic model are estimated Plots for the prediction error are given. The results demonstrate that the prediction error is significantly less when a constraint based on past observations is added. It is observed that the linear model may yield acceptable projections for systems for which vulnerability discovery has not yet reached saturation. The results also suggest that it may be possible to improve the prediction capability by combining static and dynamic approaches, or by combing different models. © 2006 IEEE.","Intrusions; Operating systems; Quantitative models; Security holes; Vulnerability","Computational methods; Computer operating systems; Computer software; Parameter estimation; Predictive control systems; Risk assessment; Intrusions; Quantitative models; Security holes; Vulnerability; Vulnerability Discovery Models (VDM); Mathematical models","Conference paper","Final","","Scopus","2-s2.0-34250212150"
"Gegick M.; Williams L.","Gegick, Michael (15831948600); Williams, Laurie (35565101900)","15831948600; 35565101900","Toward the use of automated static analysis alerts for early identification of vulnerability- and attack-prone components","2007","Second International Conference on Internet Monitoring and Protection, ICIMP 2007","","","4271764","18","","","","10.1109/ICIMP.2007.46","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35348918737&doi=10.1109%2fICIMP.2007.46&partnerID=40&md5=a547c03443ac23ce32a2548754ee4bf9","Extensive research has shown that software metrics can be used to identify fault- and failure-prone components. These metrics can also give early indications of overall software quality. We seek to parallel the identification and prediction of fault- and failure-prone components in the reliability context with vulnerability- and attack-prone components in the security context. Our research will correlate the quantity and severity of alerts generated by source code static analyzers to vulnerabilities discovered by manual analyses and testing. A strong correlation may indicate that automated static analyzers (ASA), a potentially early technique for vulnerability identification in the development phase, can identify high risk areas in the software system. Based on the alerts, we may be able to predict the presence of more complex and abstract vulnerabilities involved with the design and operation of the software system. An early knowledge of vulnerability can allow software engineers to make informed risk management decisions and prioritize redesign, inspection, and testing efforts. This paper presents our research objective and methodology. © 2007 IEEE.","","Automation; Computer crime; Identification (control systems); Risk analysis; Software design; Static analysis; Automated static analyzers (ASA); Software engineers; Software metrics; Software quality; Fault tolerant computer systems","Conference paper","Final","","Scopus","2-s2.0-35348918737"
"Gore B.F.; Jarvis P.","Gore, Brian F. (7004119833); Jarvis, Peter (9332564800)","7004119833; 9332564800","Modeling the complexities of human performance","2005","Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics","2","","","1604","1609","5","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-27944494762&partnerID=40&md5=9687cfbf8dbb003b01dfe3631fe7d023","The Man-machine Integrated Design and Analysis System (MIDAS) is an integrated human performance modeling software tool that symbolically represents a human's perceptual, cognitive and motor systems in an integrated fashion to produce emergent, high level behavioral predictions characteristic of actual human performance. MIDAS has been augmented in a number of significant ways to simulate even more realistic simulations of human behavior in various aerospace operational contexts. The effort undertaken in the current project culminated in an agent-based sub model (e.g. slots) that can be used by a variety of models to predict and recreate short- and long-term effects of stressors (fatigue, stress, time pressure, inadequate situation awareness, etc.) on performance in aerospace accidents/incidents causation. A computational simulation demonstrated performance influences brought to task performance by fatigue (as characterized by the Yerkes-Dodson theoretical threshold model) that is incurred while undertaking activities required to complete a goal behavior, and the impact of performance-influencing factors (PIFs) on human performance output by combining this with a primitive based action error vulnerability. This paper discusses the computational development effort undertaken in creating the conceptual relationship and PIF implementation in MIDAS. ©2005 IEEE.","Error modeling; Fatigue modeling; Human performance modeling; MIDAS; Performance influencing factors","Behavioral research; Cognitive systems; Computation theory; Human computer interaction; Man machine systems; Error modeling; Fatigue modeling; Human performance modeling; MIDAS; Performance influencing factors; Performance","Conference paper","Final","","Scopus","2-s2.0-27944494762"
"Neuhaus S.; Zimmermann T.; Holler C.; Zeller A.","Neuhaus, Stephan (35303613700); Zimmermann, Thomas (16308551800); Holler, Christian (36052675500); Zeller, Andreas (7007015864)","35303613700; 16308551800; 36052675500; 7007015864","Predicting vulnerable software components","2007","Proceedings of the ACM Conference on Computer and Communications Security","","","","529","540","11","","10.1145/1315245.1315311","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67249126825&doi=10.1145%2f1315245.1315311&partnerID=40&md5=2c73ccaac529a852d31e74347842b8b7","Where do most vulnerabilities occur in software? Our Vulture tool automatically mines existing vulnerability databases and version archives to map past vulnerabilities to components. The resulting ranking of the most vulnerable components is a perfect base for further investigations on what makes components vulnerable. In an investigation of the Mozilla vulnerability history, we surprisingly found that components that had a single vulnerability in the past were generally not likely to have further vulnerabilities. However, components that had similar imports or function calls were likely to be vulnerable. Based on this observation, we were able to extend Vulture by a simple predictor that correctly predicts about half of all vulnerable components, and about two thirds of all predictions are correct. This allows developers and project managers to focus their their efforts where it is needed most: ""We should look at nsXPInstallManager because it is likely to contain yet unknown vulnerabilities."". Copyright 2007 ACM.","Prediction; Software security","Forecasting; International trade; Mining; Project management; Function calls; Mozilla; Perfect base; Project managers; Software component; Software security; Computer software","Conference paper","Final","","Scopus","2-s2.0-67249126825"
"Hovsepyan A.; Scandariato R.; Joosen W.; Walden J.","Hovsepyan, Aram (14056192200); Scandariato, Riccardo (23095243000); Joosen, Wouter (57202521929); Walden, James (22036834700)","14056192200; 23095243000; 57202521929; 22036834700","Software vulnerability prediction using text analysis techniques","2012","MetriSec'12 - Proceedings of the 4th International Workshop on Security Measurements and Metrics","","","","7","9","2","","10.1145/2372225.2372230","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867544938&doi=10.1145%2f2372225.2372230&partnerID=40&md5=76e86c3ad169b8937fb74408f18b9673","Early identification of software vulnerabilities is essential in software engineering and can help reduce not only costs, but also prevent loss of reputation and damaging litigations for a software firm. Techniques and tools for software vulnerability prediction are thus invaluable. Most of the existing techniques rely on using component characteristic(s) (like code complexity, code churn) for the vulnerability prediction. In this position paper, we present a novel approach for vulnerability prediction that leverages on the analysis of raw source code as text, instead of using ""cooked"" features. Our initial results seem to be very promising as the prediction model achieves an average accuracy of 0.87, precision of 0.85 and recall of 0.88 on 18 versions of a large mobile application. Copyright 2012 ACM.","","Software engineering; Code complexity; Component characteristics; Mobile applications; Position papers; Prediction model; Software firms; Software vulnerabilities; Source codes; Text analysis; Forecasting","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84867544938"
"Puławski L.","Puławski, Łukasz (37862004300)","37862004300","Software defect prediction based on source code metrics time series","2011","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","6499 LNCS","","","104","120","16","","10.1007/978-3-642-18302-7_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79956326087&doi=10.1007%2f978-3-642-18302-7_7&partnerID=40&md5=bc40b0e9b177171f62acce808e40ab64","Source code metrics have been proved to be reliable indicators of the vulnerability of the source code to defects. Typically, a source code unit with high value of a certain metric is considered to be badly structured and thus error-prone. However, analysis of source code change history shows that there are cases when source files with low values of metrics still turn out to be defective. Instead of introducing new metrics for such cases, I investigate the possibility of estimating the vulnerability of source code units to defects on the basis of the history of the values of selected well-known metrics. The experiments show that we can efficiently identify bad source code units just by looking at the history of metrics, coming from only a few revisions that precede the actual resolution of the defect. © 2011 Springer-Verlag Berlin Heidelberg.","Classification; Software defect prediction; Source code metrics","Rough set theory; Time series; Classification; Error prones; Software defect prediction; Source code changes; Source codes; Source files; Defects","Conference paper","Final","","Scopus","2-s2.0-79956326087"
"Ma Z.S.; Krings A.W.","Ma, Zhanshan Sam (57959779300); Krings, Axel W. (7003345545)","57959779300; 7003345545","Dynamic hybrid fault modeling and extended evolutionary game theory for reliability, survivability and fault tolerance analyses","2011","IEEE Transactions on Reliability","60","1","5704533","180","196","16","","10.1109/TR.2011.2104997","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952189469&doi=10.1109%2fTR.2011.2104997&partnerID=40&md5=b661c45d8f10c8d3610532347fef1156","We introduce a new layered modeling architecture consisting of dynamic hybrid fault modeling and extended evolutionary game theory for reliability, survivability, and fault tolerance analyses. The architecture extends traditional hybrid fault models and their relevant constraints in the Agreement algorithms with survival analysis, and evolutionary game theory. The dynamic hybrid fault modeling (i) transforms hybrid fault models into time- and covariate-dependent models; (ii) makes real-time prediction of reliability more realistic, and allows for real-time prediction of fault-tolerance; (iii) sets the foundation for integrating hybrid fault models with reliability and survivability analyses by integrating them with evolutionary game modeling; and (iv) extends evolutionary game theory by stochastically modeling the survival (or fitness) and behavior of game players. To analyse survivability, we extend dynamic hybrid fault modeling with a third-layer, operational level modeling, to develop the three-layer survivability analysis approach (dynamic hybrid fault modeling constitutes the tactical and strategic levels). From the perspective of evolutionary game modeling, the two mathematical fields, i.e., survival analysis and agreement algorithms, which we applied for developing dynamic hybrid fault modeling, can also be utilized to extend the power of evolutionary game theory in modeling complex engineering, biological (ecological), and social systems. Indeed, a common property of the areas where our extensions to evolutionary game theory can be advantageous is that the risk analysis and management are a core issue. Survival analysis (including competing risks analysis, and multivariate survival analysis) offers powerful modeling tools to analyse time-, space-, and/or covariate-dependent uncertainty, vulnerability, and/or frailty which game players may experience. The agreement algorithms, which are not limited to the agreement algorithms from distributed computing, when applied to extend evolutionary game modeling, can be any problem (game system) specific rules (algorithms or models) that can be utilized to dynamically check the consensus among game players. We expect that the modeling architecture and approaches discussed in the study should be implemented as a software environment to deal with the necessary sophistication. Evolutionary computing should be particularly convenient to serve as the core optimization engine, and should simplify the implementation. Accordingly, a brief discussion on the software architecture is presented. © 2010 IEEE.","Agreement algorithms; Byzantine generals problem; dynamic hybrid fault models; extended evolutionary game theory modeling; fault; reliability; survivability; survival analysis; tolerance; wireless sensor networks","Bioinformatics; Distributed computer systems; Dynamic models; Ecology; Evolutionary algorithms; Fault tolerance; Fits and tolerances; Game theory; Multivariant analysis; Network architecture; Reliability analysis; Reliability theory; Risk analysis; Risk assessment; Sensors; Software architecture; Uncertainty analysis; Wireless sensor networks; Agreement algorithms; Byzantine generals problem; Evolutionary game theory; fault; survivability; survival analysis; tolerance; Quality assurance","Article","Final","","Scopus","2-s2.0-79952189469"
"Hancilar U.; Tuzun C.; Yenidogan C.; Erdik M.","Hancilar, U. (35791846800); Tuzun, C. (35977149500); Yenidogan, C. (35977207600); Erdik, M. (7003962489)","35791846800; 35977149500; 35977207600; 7003962489","Urban earthquake loss assessment by ELER","2010","COST ACTION C26: Urban Habitat Constructions under Catastrophic Events - Proceedings of the Final Conference","","","","987","992","5","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861964973&partnerID=40&md5=dfbf71fb68e6fb14552e42f1a7c59233","Rapid loss estimation after potentially damaging earthquakes is critical for effective emergency response and public information. A methodology and software package, Earthquake Loss Estimation Routine- ELER, for rapid estimation of earthquake shaking and losses throughout the Euro-Mediterranean region (Erdik et al. 2008, 2010a, Hancilar et al. 2009, Sesetyan et al. 2009) was developed under the Joint Research Activity-3 (JRA3) of the EC FP6 Project entitled ""Network of Research Infrastructures for European Seismology-NERIES (www.neries-eu.org)"". Recently, a new version (v2.0) of ELER software has been released. The multi-level methodology developed is capable of incorporating regional variability and sources of uncertainty stemming from ground motion predictions, fault finiteness, site modifications, inventory of physical and social elements subjected to earthquake hazard and the associated vulnerability relationships. Although primarily intended for almost real-time estimation of earthquake shaking and losses, the routine is also equally capable of incorporating scenario based earthquake loss assessments. This paper introduces the urban earthquake loss assessment module, Level 2 module, of ELER software. Spectral capacity based loss assessment methodology and its vital ingredients are presented. © 2010 Taylor & Francis Group, London.","","Estimation; Rating; Assessment methodologies; Earthquake hazard; Earthquake loss estimation; Earthquake shaking; Emergency response; Ground motion prediction; Level 2; Loss estimation; Public information; Rapid estimation; Real-time estimation; Regional variability; Research infrastructure; Social elements; Sources of uncertainty; Earthquakes","Conference paper","Final","","Scopus","2-s2.0-84861964973"
"Atkison T.","Atkison, Travis (24604912600)","24604912600","Aiding prediction algorithms in detecting high-dimensional malicious applications using a randomized projection technique","2010","Proceedings of the Annual Southeast Conference","","","80","","","","","10.1145/1900008.1900117","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951838068&doi=10.1145%2f1900008.1900117&partnerID=40&md5=a59cbd0e405dc84e0b308eecf9538266","This research paper describes an on-going effort to design, develop and improve upon malicious application detection algorithms. This work looks specifically at improving a cosine similarity, information retrieval technique to enhance detection of known and variances of known malicious applications by applying the feature extraction technique known as randomized projection. Document similarity techniques, such as cosine similarity, have been used with great success in several document retrieval applications. By following a standard information retrieval methodology, software, in machine readable format, can be regarded as documents in the corpus. These ""documents"" may or may not have a known malicious functionality. The query is software, again in machine readable format, which contains a certain type of malicious software. This methodology provides an ability to search the corpus with a query and retrieve/identify potentially malicious software as well as other instances of the same type of vulnerability. Retrieval is based on the similarity of the query to a given document in the corpus. There have been several efforts to overcome what is known as 'the curse of dimensionality' that can occur with the use of this type of information retrieval technique including mutual information and randomized projections. Randomized projections are used to create a low-order embedding of the high dimensional data. Results from experimentation have shown promise over previously published efforts. Copyright ©2010 ACM.","Cosine similarity; Information retrieval; Malicious software detection; Randomized projections; Rc-gram analysis","Clustering algorithms; Computer crime; Feature extraction; Cosine similarity; Curse of dimensionality; Detection algorithm; Document Retrieval; Document similarity; Feature extraction techniques; High dimensional data; High-dimensional; Machine-readable format; Malicious software; Mutual informations; Order embedding; Prediction algorithms; Projection techniques; Randomized projections; Rc-gram analysis; Research papers; Standard information; Information retrieval","Conference paper","Final","","Scopus","2-s2.0-79951838068"
"Gore B.F.","Gore, B.F. (7004119833)","7004119833","An emergent behavior model of complex human-system performance: An aviation surface-related application","2002","VDI Berichte","","1675","","313","328+517","","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-69349107314&partnerID=40&md5=c49f5f2a23da3ffe37f4e0d7d7d95f2b","Advanced technologies can have powerful mitigating effects on human error. However, if the user is not adequately considered during the technological development, unforeseen effects on human performance and system vulnerabilities emerge. It is important therefore to be able to quantify and predict precursor events that impact system safety. One enabling methodology for human-system quantification is termed the integrated human performance model. This paper will demonstrate that performance modifiers shown by human-in-the-loop (HITL) simulation data were used as the basis for the predictive, emergent human performance modeling tool Air Man-machine integrated Design and Analysis System (Air MIDAS) in generating human-system performance predictions. The Air MIDAS software tool demonstrated an agent s sensitivity to a simulated environment through successfully creating emergent behavior.","","Accident prevention; Automation; Cognitive systems; Computer simulation; Computer software; Error analysis; Human form models; Sensitivity analysis; Technological forecasting; Human errors; Precursors; Surface operations; System safety; Human engineering","Conference paper","Final","","Scopus","2-s2.0-69349107314"
"Park Y.-J.; Lee G.","Park, Yong-Joon (55494363300); Lee, Gyungho (7404852683)","55494363300; 7404852683","Repairing return address stack for buffer overflow protection","2004","2004 Computing Frontiers Conference","","","","335","342","7","","10.1145/977091.977139","https://www.scopus.com/inward/record.uri?eid=2-s2.0-4143130038&doi=10.1145%2f977091.977139&partnerID=40&md5=e955023ef72f2d3910151c3ef7e08971","Although many defense mechanisms against buffer overflow attacks have been proposed, buffer overflow vulnerability in software is still one of the most prevalent vulnerabilities exploited. This paper proposes a micro-architecture based defense mechanism against buffer overflow attacks. As buffer overflow attack leads to a compromised return address, our approach is to provide a software transparent micro-architectural support for return address integrity checking. By keeping an uncompromised copy of the return address separate from the activation record in run-time stack, the return address compromised by a buffer overflow attack can be detected at run time. Since extra copies of return addresses are already found in the return address stack (RAS) for return address prediction in most high-performance microprocessors, this paper considers augmenting the RAS in speculative superscalar processors for return address integrity checking. The new mechanism provides 100% accurate return address prediction as well as integrity checking for return addresses. Hence, it enhances system performance in addition to preventing a buffer overflow attack.","Buffer overflow; Computer architecture; Computer security; Intrusion tolerance","Computer architecture; Information analysis; Microprocessor chips; Problem solving; Recursive functions; Statistical methods; Buffer overflow; Integrity checking; Intrusion tolerance; Invasive software; Security of data","Conference paper","Final","","Scopus","2-s2.0-4143130038"
"Henderson William; Kendall David; Robson Adrian","Henderson, William (7202529809); Kendall, David (7101670962); Robson, Adrian (7102089907)","7202529809; 7101670962; 7102089907","Accounting for clock frequency variation in the analysis of distributed factory control systems","2000","IEEE International Workshop on Factory Communication Systems, WPCS, Proceedings,","","","","51","58","7","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034480054&partnerID=40&md5=e00cfc13bb52883a1a1f466bdccb7e20","Micro-controllers are now widely deployed as components in distributed systems. Temporal predictability is often important for such embedded systems - i.e., software must execute within specified time bounds to maintain the safe operation of the system as a whole. Micro-controller clocks exhibit random variation in resonant frequency from one component to another, a temperature sensitivity and gradual changes with time. Thus, the execution speed of software will vary when implemented on different processors of the same class. We highlight the vulnerability of standard scheduling analysis in the presence of clock frequency uncertainty when applied to the performance prediction of distributed embedded systems. We propose a modified scheduling analysis to account for the inevitable range of processor clock rates in embedded distributed systems and confirm our analysis by an empirical study using CAN for inter-processor communication.","","Control system analysis; Control system synthesis; Distributed parameter control systems; Embedded systems; Microcontrollers; Predictive control systems; Random processes; Clock frequency variations; Distributed embedded systems; Distributed factory control systems; SCADA systems","Conference paper","Final","","Scopus","2-s2.0-0034480054"
"Gegick M.; Rotella P.; Williams L.","Gegick, Michael (15831948600); Rotella, Pete (36963806300); Williams, Laurie (35565101900)","15831948600; 36963806300; 35565101900","Toward Non-security failures as a predictor of security faults and failures","2009","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","5429 LNCS","","","135","149","14","","10.1007/978-3-642-00199-4_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350630478&doi=10.1007%2f978-3-642-00199-4_12&partnerID=40&md5=4ea504a5ab354a2dab4b43c56a7f46a5","In the search for metrics that can predict the presence of vulnerabilities early in the software life cycle, there may be some benefit to choosing metrics from the non-security realm. We analyzed non-security and security failure data reported for the year 2007 of a Cisco software system. We used non-security failure reports as input variables into a classification and regression tree (CART) model to determine the probability that a component will have at least one vulnerability. Using CART, we ranked all of the system components in descending order of their probabilities and found that 57% of the vulnerable components were in the top nine percent of the total component ranking, but with a 48% false positive rate. The results indicate that non-security failures can be used as one of the input variables for security-related prediction models. © 2009 Springer Berlin Heidelberg.","Attack-prone; Classification and regression tree","Computer software; Mathematical models; Probability; Regression analysis; Safety engineering; Attack-prone; Classification and regression tree; Classification and regression tree models; Component ranking; False positive rates; Input variables; Prediction model; Security failure; Software life cycles; Software systems; System components; Security of data","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-70350630478"
"Töyssy S.; Helenius M.","Töyssy, Sampo (15048854500); Helenius, Marko (14060304200)","15048854500; 14060304200","About malicious software in smartphones","2006","Journal in Computer Virology","2","2","","109","119","10","","10.1007/s11416-006-0022-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750436929&doi=10.1007%2fs11416-006-0022-0&partnerID=40&md5=8785d38b8230b16d8f0d47c3628a631c","Phones with some of the capabilities of modern computers also have the same kind of drawbacks. These phones are commonly referred to as smartphones. They have both phone and personal digital assistant (PDA) functionality. Typical to these devices is to have a wide selection of different connectivity options from general packet radio service (GPRS) data transfer to multi media messages (MMS) and wireless local area network (WLAN) capabilities. They also have standardized operating systems, which makes smartphones a viable platform for malware writers. Since the design of the operating systems is recent, many common security holes and vulnerabilities have been taken into account during the design. However, these precautions have not fully protected these devices. Even now, when smartphones are not that common, there is a handful of viruses for them. In this paper we will discuss some of the most typical viruses in the mobile environment and propose guidelines and predictions for the future. © Springer-Verlag France 2006.","","","Article","Final","","Scopus","2-s2.0-33750436929"
"Reddy T.A.; Snyder S.; Bem J.; Bahnfleth W.","Reddy, T. Agami (7102586329); Snyder, Steven (55548371400); Bem, Justin (33467537700); Bahnfleth, William (7004444149)","7102586329; 55548371400; 33467537700; 7004444149","Analysis tools and guidance documents for evaluating and reducing vulnerability of buildings to airborne threats - Part 1: Literature review","2011","ASHRAE Transactions","117","PART 1","","817","825","8","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-82055190020&partnerID=40&md5=c53117a7dca3c141d3fb544f0f9086fb","This paper classifies and describes analysis methods, tools, and simulation programs that allow prediction of airborne chemical/biological dispersal and transport dynamics in indoor environments subject to different risk scenarios. These are the building blocks for related analytical treatment of the overall problem involving risk assessment, risk management, and identifying cost-effective mitigation measures. These methods are distinguished by the level of mathematical and scientific rigor in modeling the phenomena, in the spatial and temporal resolution in solving the modeling equations, and in the types of boundary conditions and the numerical parameters that appear in the model. The paper also describes various general guidance documents and vulnerability assessment protocols and software available in the open-source literature to assess and reduce vulnerability in buildings due to airborne threats and risks. ©2011 ASHRAE.","","Numerical methods; Risk management; Risk perception; Airborne chemicals; Analysis method; Analysis tools; Analytical treatment; Building blockes; Guidance document; In-buildings; Indoor environment; Literature reviews; Mitigation measures; Modeling equations; Numerical parameters; Open-source; Risk scenarios; Simulation program; Temporal resolution; Transport dynamics; Vulnerability assessments; Risk assessment","Conference paper","Final","","Scopus","2-s2.0-82055190020"
"Smith B.; Williams L.","Smith, Ben (55476072900); Williams, Laurie (35565101900)","55476072900; 35565101900","Using SQL hotspots in a prioritization heuristic for detecting all types of web application vulnerabilities","2011","Proceedings - 4th IEEE International Conference on Software Testing, Verification, and Validation, ICST 2011","","","5770611","220","229","9","","10.1109/ICST.2011.15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958756538&doi=10.1109%2fICST.2011.15&partnerID=40&md5=be7d8e67bb1e6487547bf5ba05909a8c","Development organizations often do not have time to perform security fortification on every file in a product before release. One way of prioritizing security efforts is to use metrics to identify core business logic that could contain vulnerabilities, such as database interaction code. Database code is a source of SQL injection vulnerabilities, but importantly may be home to unrelated vulnerabilities. The goal of this research is to improve the prioritization of security fortification efforts by investigating the ability of SQL hotspots to be used as the basis for a heuristic for prediction of all vulnerability types. We performed empirical case studies of 15 releases of two open source PHP web applications: Word Press, a blogging application, and WikkaWiki, a wiki management engine. Using statistical analysis, we show that the more SQL hotspots a file contains per line of code, the higher the probability that file will contain any type of vulnerability. © 2011 IEEE.","empirical; hotspots; prioritization; sql; sql injection; wikkawiki; wordpress","Computer software selection and evaluation; User interfaces; Verification; World Wide Web; empirical; Hotspots; Prioritization; sql; SQL injection; wikkawiki; wordpress; Software testing","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-79958756538"
"Lidskii E.A.; Mikhaleva U.A.","Lidskii, E.A. (57199570626); Mikhaleva, U.A. (55537463900)","57199570626; 55537463900","Principles of prediction of information protection in the communication network","2012","Telecommunications and Radio Engineering (English translation of Elektrosvyaz and Radiotekhnika)","71","18","","1651","1663","12","","10.1615/TelecomRadEng.v71.i18.40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871604243&doi=10.1615%2fTelecomRadEng.v71.i18.40&partnerID=40&md5=ca0cef01b6c45814b638e67403ee01bd","An active method for combating unauthorized intrusion into the communication network operation is discussed. Preventive measures are planned based on known data on vulnerability of software products. An open NVD database is used. The most dangerous threats are predicted in the form of a game between two partners: the attacker and the defender. The game results in recommendations on information protection for the studied program system. © 2012 by Begell House, Inc.","Attack; Database; Efficiency; Estimation; Information security; Payoff matrix; Protection; Software; Threat; Vulnerability","Computer software; Database systems; Efficiency; Electrical engineering; Estimation; Attack; Payoff matrix; Protection; Threat; Vulnerability; Security of data","Article","Final","","Scopus","2-s2.0-84871604243"
"Gegick M.; Williams L.; Osborne J.; Vouk M.","Gegick, Michael (15831948600); Williams, Laurie (35565101900); Osborne, Jason (7203032439); Vouk, Mladen (7004021808)","15831948600; 35565101900; 7203032439; 7004021808","Prioritizing software security fortification through code-level metrics","2008","Proceedings of the ACM Conference on Computer and Communications Security","","","","31","37","6","","10.1145/1456362.1456370","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349244714&doi=10.1145%2f1456362.1456370&partnerID=40&md5=2edddfb5e4879d15d2757336a6b91f47","Limited resources preclude software engineers from finding and fixing all vulnerabilities in a software system. We create predictive models to identify which components are likely to have the most security risk. Software engineers can use these models to make measurement-based risk management decisions and to prioritize software security fortification efforts, such as redesign and additional inspection and testing. We mined and analyzed data from a large commercial telecommunications software system containing over one million lines of code that had been deployed to the field for two years. Using recursive partitioning, we built attack-prone prediction models with the following code-level metrics: static analysis tool alert density, code churn, and count of source lines of code. One model identified 100% of the attack-prone components (40% of the total number of components) with an 8% false positive rate. As such, the model could be used to prioritize fortification efforts in the system. Copyright 2008 ACM.","Attack-prone; Vulnerability-prone","Mathematical models; Military engineering; Regression analysis; Risk analysis; Risk management; Software engineering; Attack-prone; False positive rates; Lines of code; Measurement-based; Number of components; Prediction model; Predictive models; Recursive Partitioning; Security risks; Software engineers; Software security; Software systems; Vulnerability-prone; Computer software selection and evaluation","Conference paper","Final","","Scopus","2-s2.0-70349244714"
"Holm H.; Sommestad T.; Franke U.; Ekstedt M.","Holm, Hannes (36680084500); Sommestad, Teodor (24476642500); Franke, Ulrik (26421623400); Ekstedt, Mathias (6602241139)","36680084500; 24476642500; 26421623400; 6602241139","Success rate of remote code execution attacks expert assessments and observations","2012","Journal of Universal Computer Science","18","6","","732","749","17","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862696351&partnerID=40&md5=0f475f7ab089df44b999848cdeea3a5f","This paper describes a study on how cyber security experts assess the importance of three variables related to the probability of successful remote code execution attacks: (i) nonexecutable memory, (ii) access and (iii) exploits for High or Medium vulnerabilities as defined by the Common Vulnerability Scoring System. The rest of the relevant variables were fixed by the environment of a cyber defense exercise where the respondents participated. The questionnaire was fully completed by fifteen experts. These experts perceived access as the most important variable and availability of exploits for High vulnerabilities as more important than Medium vulnerabilities. Non-executable memory was not seen as significant. Estimates by the experts are compared to observations of actual attacks carried out during the cyber defense exercise. These comparisons show that experts' in general provide fairly inaccurate advice on an abstraction level such as in the present study. However, results also show a prediction model constructed through expert judgment likely is of better quality if the experts' estimates are weighted according to their expertise. © J.UCS.","Cyber security; Remote code execution; Software vulnerabilities","","Article","Final","","Scopus","2-s2.0-84862696351"
"Zhang S.; Caragea D.; Ou X.","Zhang, Su (42263160500); Caragea, Doina (35615386800); Ou, Xinming (25651998800)","42263160500; 35615386800; 25651998800","An empirical study on using the national vulnerability database to predict software vulnerabilities","2011","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","6860 LNCS","PART 1","","217","231","14","","10.1007/978-3-642-23088-2_15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052810370&doi=10.1007%2f978-3-642-23088-2_15&partnerID=40&md5=ca4f74d793455149b1ea4703e3cb9d56","Software vulnerabilities represent a major cause of cyber-security problems. The National Vulnerability Database (NVD) is a public data source that maintains standardized information about reported software vulnerabilities. Since its inception in 1997, NVD has published information about more than 43,000 software vulnerabilities affecting more than 17,000 software applications. This information is potentially valuable in understanding trends and patterns in software vulnerabilities, so that one can better manage the security of computer systems that are pestered by the ubiquitous software security flaws. In particular, one would like to be able to predict the likelihood that a piece of software contains a yet-to-be-discovered vulnerability, which must be taken into account in security management due to the increasing trend in zero-day attacks. We conducted an empirical study on applying data-mining techniques on NVD data with the objective of predicting the time to next vulnerability for a given software application. We experimented with various features constructed using the information available in NVD, and applied various machine learning algorithms to examine the predictive power of the data. Our results show that the data in NVD generally have poor prediction capability, with the exception of a few vendors and software applications. By doing a large number of experiments and observing the data, we suggest several reasons for why the NVD data have not produced a reasonable prediction model for time to next vulnerability with our current approach. © 2011 Springer-Verlag Berlin Heidelberg.","cyber-security; data mining; vulnerability prediction","Data mining; Database systems; Expert systems; Learning algorithms; Mathematical models; Network security; Cyber security; Empirical studies; National vulnerability database; Prediction capability; Prediction model; Predictive power; Public data source; Security management; Software applications; Software security; Software vulnerabilities; Zero day attack; Forecasting","Conference paper","Final","","Scopus","2-s2.0-80052810370"
"Zhou Z.; Bai X.","Zhou, Ziguan (22959308500); Bai, Xiaomin (7201893222)","22959308500; 7201893222","A new integrated fault diagnosis and analysis system for large-scale power grid","2008","IFAC Proceedings Volumes (IFAC-PapersOnline)","17","1 PART 1","","","","","","10.3182/20080706-5-KR-1001.1908","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79961019818&doi=10.3182%2f20080706-5-KR-1001.1908&partnerID=40&md5=c6f5c7484cce66595ab23d19700b77ff","This paper describes an efficient methodology that can be used in an integrated fault diagnosis and analysis system (IFDAS) for large-scale power grid. The IFDAS extended from some software tools used in contemporary power grid consists of four main parts: fault information processing, fault diagnosis, power gird vulnerability analysis and prediction contingency analysis. It also has some innovative characteristics. First, the large-scale power system IFDAS in this paper is based on slow successive tripping procedure of cascading failure. Second, the concept of coordination is brought into the cascading failure analysis and defence scheme of IFDAS. Third, it is an on-line cascading failure analysis method which can trace real-time operation conditions. And last it makes full use of data acquisition channels and monitor system dynamic behavior. According to the slow successive tripping procedure character of the cascading failure and blackouts, the concept of a large-scale power system IFDAS is presented. And the features, structure, flow and responding mechanism of the IFDAS are all detailed discussed. Finally, case analysis shows that the IFDAS is a system good at fast identifying cause during cascading failure, supplying optimum dispatching strategy, and avoiding blackouts effectively. Copyright © 2007 International Federation of Automatic Control All Rights Reserved.","Constraint and security monitoring and control; Intelligent control of power systems; Real time simulation and dispatching","Data processing; Electric power transmission networks; Intelligent control; Power generation; Power transmission; Quality assurance; Real time systems; Safety factor; Trace analysis; Transients; Analysis system; Cascading failure analysis; Cascading failures; Case analysis; Constraint and security monitoring and control; Contingency analysis; Data acquisition channels; Fault diagnosis; Information processing; Large-scale power systems; Monitor system; Power grids; Real time simulation and dispatching; Real-time operation; Software tool; Vulnerability analysis; Failure analysis","Conference paper","Final","","Scopus","2-s2.0-79961019818"
"Shin Y.; Meneely A.; Williams L.; Osborne J.A.","Shin, Yonghee (24492397200); Meneely, Andrew (23135193600); Williams, Laurie (35565101900); Osborne, Jason A. (7203032439)","24492397200; 23135193600; 35565101900; 7203032439","Evaluating complexity, code churn, and developer activity metrics as indicators of software vulnerabilities","2011","IEEE Transactions on Software Engineering","37","6","5560680","772","787","15","","10.1109/TSE.2010.81","https://www.scopus.com/inward/record.uri?eid=2-s2.0-83555172467&doi=10.1109%2fTSE.2010.81&partnerID=40&md5=b39f789b6324673027851b17ec841a6b","Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The models using all three types of metrics together predicted over 80 percent of the known vulnerable files with less than 25 percent false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects. © 2006 IEEE.","Fault prediction; software metrics; software security; vulnerability prediction","Codes (symbols); Computer operating systems; Forecasting; Inspection; Software testing; Web browsers; Development history; Empirical case studies; False positive; Fault prediction; Firefox web browser; Lines of code; Linux kernel; Mozilla; Open source projects; Random selection; Red hats; Security experts; software metrics; software security; Software vulnerabilities; Source codes; Testing and inspection; Testing effort; Security of data","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-83555172467"
"Matulewski K.V.; McBride W.","Matulewski, K.V. (6506396787); McBride, W. (7201796742)","6506396787; 7201796742","Day/night underwater object detection from an airborne sensor using NOVAS (non-acoustical optical vulnerability assessment software)","2005","Proceedings of MTS/IEEE OCEANS, 2005","2005","","1640104","2274","2278","4","","10.1109/OCEANS.2005.1640104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947135893&doi=10.1109%2fOCEANS.2005.1640104&partnerID=40&md5=6c1322f714f9670a563c0d00e3ab6edc","The majority of underwater object detections, from airborne platforms, are done with the unaided eye or a video camera during daytime or nighttime searches. An understanding of the optical environment in which detection is performed is, therefore, crucial in predicting the depth at which the underwater object can be detected day or night. The motivation behind NOVAS (Non-acoustical Optical Vulnerability Assessment Software) is to provide the Navy with a round-the-clock (24/7), PC-based assessment tool of the optical environment in which the warfighter operates. In order to achieve this goal, NOVAS has two types of predictions that are divided up according to the time of day: vertical visibility depth of an underwater target for the daytime, and bioluminescence signature of underwater man made objects as they navigate at different speeds in the water column at night. Both types of predictions are made by treating the optical environment as realistically as possible: a dynamic sea surface topography overlying a water column with depth-dependent optical properties. The topography of the sea surface depends on wind speed and far-off swell generation. The optical properties of the water column consist of absorption, scattering, and a mean scattering angle. The amount of bioluminescent activity generated by a moving source depends on both the biological organism concentration through which it is moving and its speed relative to the local currents. NOVAS also contains data maps that either display the location of the different experimental survey stations taken during a particular NAVOCEANO cruise, or are satellite generated. NOVAS models a low-light-level airborne camera with an intensified CCD. The loss of resolution in the intensification process is also modeled with the help of the Modulation Transfer Function. NOVAS predictions are displayed synoptically with regional maps and simulated images, as well as in one spot in the ocean, using in situ data. Images are simulated as if the viewer or system is looking down from an airborne platform hovering over an underwater object.","","Bioluminescence; Charge coupled devices; Computer software; Surface topography; Video cameras; Airborne platform; Non-acoustical Optical Vulnerability Assessment Software; Underwater object; Underwater target; Sensors","Conference paper","Final","","Scopus","2-s2.0-33947135893"
"Freeman S.M.; Clune T.L.; Burns III R.W.","Freeman, Shawn M. (36975118500); Clune, Thomas L. (6603552777); Burns III, Robert W. (56217620100)","36975118500; 6603552777; 56217620100","Latent risks and dangers in the state of climate model software development","2010","Proceedings of the FSE/SDP Workshop on the Future of Software Engineering Research, FoSER 2010","","","","111","114","3","","10.1145/1882362.1882386","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951624848&doi=10.1145%2f1882362.1882386&partnerID=40&md5=7cbe38644002cd1000f330309a57707d","As the importance of climate modeling dramatically increases due to concerns about global climate change, the quality of model software will come under ever more intense scrutiny. Software defects that alter model predictions could negatively impact important climate policy decisions, and even if detected in time could reduce policy makers' confidence in the science. Effective protection against software defense against errors requires the anticipation of vulnerabilities for every facet of this important work. Unfortunately, existing climate model implementations and associated software engineering practices are inadequate to defend properly against some concerns over defects and untested parameter regimes. Organizations that wish to avoid scrutiny, whether deserved or not, should ensure that appropriate software engineering practices are established with all due haste. In this paper we examine some of the inadequacies of existing software development methodologies and suggest strategies for fundamentally changing the established culture. Copyright 2010 ACM.","Climate change; Climate model; Software verification","Climate change; Decision making; Defects; Engineering research; Software design; Verification; Associated softwares; Climate modeling; Climate policy; Global climate changes; Model implementation; Model prediction; Parameter regimes; Policy makers; Software defects; Software defense; Software development; Software development methodologies; Software engineering practices; Software verification; Climate models","Conference paper","Final","","Scopus","2-s2.0-79951624848"
"Atkison T.","Atkison, Travis (24604912600)","24604912600","Applying randomized projection to aid prediction algorithms in detecting high-dimensional rogue applications","2009","Proceedings of the 47th Annual Southeast Regional Conference, ACM-SE 47","","","1566477","","","","","10.1145/1566445.1566477","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449727238&doi=10.1145%2f1566445.1566477&partnerID=40&md5=8b9a7dee1d88381cf0bc5acd7bac51b9","This paper describes a research effort to improve the use of the cosine similarity information retrieval technique to detect unknown, known or variances of known rogue software by applying the feature extraction technique of randomized projection. Document similarity techniques, such as cosine similarity, have been used with great success in several document retrieval applications. By following a standard information retrieval methodology, software, in machine readable format, can be regarded as documents in the corpus. These ""documents"" may or may not have a known rogue functionality. The query is software, again in machine readable format, which contains a certain type of rogue software. This methodology provides an ability to search the corpus with a query and retrieve/identify potentially rogue software as well as other instances of the same type of vulnerability. This retrieval is based on the similarity of the query to a given document in the corpus. To overcome what is known as the 'the curse of dimensionality' that can occur with the use of this type of information retrieval technique, randomized projections are used to create a low-order embedding of the high-dimensional data. For our experiment, we obtain Microsoft Windows applications, infect a subset of them with several common Trojans and apply our dimensionality and prediction methodology. Preliminary results show promise when applying randomized projections to cosine similarity in both speed of prediction and efficiency of required space when compared with using only cosine similarity. ©2009 ACM.","Cosine similarity; Information retrieval; N-gram analysis; Randomized projections; Rogue software detection","Algorithms; Computer software; Feature extraction; Information retrieval; Information services; Security of data; Spontaneous emission; Cosine similarity; Curse of dimensionality; Document Retrieval; Document similarity; Feature extraction techniques; High dimensional data; High-dimensional; Machine-readable format; Microsoft windows; Order embedding; Prediction algorithms; Prediction methodology; Research efforts; Rogue software; Standard information; Trojans; Information use","Conference paper","Final","","Scopus","2-s2.0-70449727238"
"Granados N.F.; Kauffman R.J.; King B.","Granados, Nelson F. (23034517700); Kauffman, Robert J. (35083571300); King, Bradley (24778289400)","23034517700; 35083571300; 24778289400","The emerging role of vertical search engines in travel distribution: A newly-vulnerable electronic markets perspective","2008","Proceedings of the Annual Hawaii International Conference on System Sciences","","","4439094","","","","","10.1109/HICSS.2008.435","https://www.scopus.com/inward/record.uri?eid=2-s2.0-51449090579&doi=10.1109%2fHICSS.2008.435&partnerID=40&md5=f8555cf28d4e9a5bd182a0366ca50dae","Information technology (IT) advances often create turmoil and disturb existing industry structures. We analyze the impact of the Internet and e-commerce technologies on digital intermediaries in the travel distribution sector. We suggest that the emerging role of meta-search agents, which act as shopbots for online travel, lead to the increasing vulnerability of existing intermediaries in the industry, especially the information infrastructure- providing global distribution systems (GDSs) and the distribution-based online travel agencies (OTAs). We base our analysis on the theories of newly-vulnerable markets and intermediation. This supports our examination of the characteristics of existing electronic distribution players in the industry that make them vulnerable to new search engine technologies. The main findings that we report are: (1) technological advances change the defensibility of e-market intermediaries to new value-adding secondary intermediaries, and (2) the value-added opportunities come from providing consumers with enhanced transparency-based price and feature search, as well as comparison capabilities. Our analysis allows us to make bold predictions about the future of the industry, and to apply our findings to other industry contexts. © 2008 IEEE.","Digital intermediation; Electronic markets; Meta-search travel agents; Newly-vulnerable markets; Secondary intermediaries; Technology adapters; Travel industry","Computer networks; Computer software; Electronic commerce; Information retrieval; Information services; Search engines; World Wide Web; Digital intermediation; Electronic markets; Meta-search travel agents; Newly-vulnerable markets; Secondary intermediaries; Technology adapters; Travel industry; Online systems","Conference paper","Final","","Scopus","2-s2.0-51449090579"
"Shar L.K.; Beng Kuan Tan H.; Briand L.C.","Shar, Lwin Khin (36802571200); Beng Kuan Tan, Hee (55901799400); Briand, Lionel C. (7006613079)","36802571200; 55901799400; 7006613079","Mining SQL injection and cross site scripting vulnerabilities using hybrid program analysis","2013","Proceedings - International Conference on Software Engineering","","","6606610","642","651","9","","10.1109/ICSE.2013.6606610","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886430853&doi=10.1109%2fICSE.2013.6606610&partnerID=40&md5=665501358e2a108182da7e9151c75b1f","In previous work, we proposed a set of static attributes that characterize input validation and input sanitization code patterns. We showed that some of the proposed static attributes are significant predictors of SQL injection and cross site scripting vulnerabilities. Static attributes have the advantage of reflecting general properties of a program. Yet, dynamic attributes collected from execution traces may reflect more specific code characteristics that are complementary to static attributes. Hence, to improve our initial work, in this paper, we propose the use of dynamic attributes to complement static attributes in vulnerability prediction. Furthermore, since existing work relies on supervised learning, it is dependent on the availability of training data labeled with known vulnerabilities. This paper presents prediction models that are based on both classification and clustering in order to predict vulnerabilities, working in the presence or absence of labeled training data, respectively. In our experiments across six applications, our new supervised vulnerability predictors based on hybrid (static and dynamic) attributes achieved, on average, 90% recall and 85% precision, that is a sharp increase in recall when compared to static analysis-based predictions. Though not nearly as accurate, our unsupervised predictors based on clustering achieved, on average, 76% recall and 39% precision, thus suggesting they can be useful in the absence of labeled training data. © 2013 IEEE.","Defect prediction; empirical study; input validation and sanitization; static and dynamic analysis; vulnerability","Forecasting; Static analysis; Defect prediction; Empirical studies; Sanitization; Static and dynamic analysis; vulnerability; Software engineering","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84886430853"
"Al-Shaer E.S.; Hamed H.H.","Al-Shaer, Ehab S. (6602187244); Hamed, Hazem H. (7004904848)","6602187244; 7004904848","Discovery of policy anomalies in distributed firewalls","2004","Proceedings - IEEE INFOCOM","4","","","2605","2616","11","","10.1109/INFCOM.2004.1354680","https://www.scopus.com/inward/record.uri?eid=2-s2.0-8344229907&doi=10.1109%2fINFCOM.2004.1354680&partnerID=40&md5=28ca3f38656048e36d3d0c85e5fa7d7b","Firewalls are core elements in network security. However, managing firewall rules, particularly in multi-firewall enterprise networks, has become a complex and error-prone task. Firewall filtering rules have to be written, ordered and distributed carefully in order to avoid firewall policy anomalies that might cause network vulnerability. Therefore, inserting or modifying filtering rules in any firewall requires thorough intra- and inter-firewall analysis to determine the proper rule placement and ordering in the firewalls. In this paper, we identify all anomalies that could exist in a single- or multi-firewall environment. We also present a set of techniques and algorithms to automatically discover policy anomalies in centralized and distributed legacy firewalls. These techniques are implemented in a software tool called the ""Firewall Policy Advisor"" that simplifies the management of filtering rules and maintains the security of next-generation firewalls.","","Algorithms; Computational complexity; Computer crime; Distributed computer systems; High level languages; Internet; Security of data; Signal filtering and prediction; Intra-firewall anomalies; Multi-firewall enterprise networks; Network security; Computer system firewalls","Conference paper","Final","","Scopus","2-s2.0-8344229907"
"Albeanu G.; Duda G.; Stancu-Mara N.","Albeanu, Grigore (6602656209); Duda, Gheorghe (55913354400); Stancu-Mara, Niculae (55913079200)","6602656209; 55913354400; 55913079200","On designing highly secure embedded systems","2010","Proceedings - 16th ISSAT International Conference on Reliability and Quality in Design","","","","174","178","4","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886880586&partnerID=40&md5=81885552e3e7f39cf41d2d78ec5d08ee","Embedded systems are specialized electronic equipments base on both hardware and software components. Any particular embedded system has to be secure and to satisfy strictly the requirement on functionality, reliability, cost, power consumption, volume, and in some cases special constraints related to catastrophic events. This paper describes the most relevant aspects concerning the secure software design for embedded systems and asserts that the quality of the embedded systems can be increased using appropriate secure software techniques and a multi-step strategy based on prevention and vulnerability discovery, resistance to attacks, security holes removing, and behavior prediction along the system life-cycle.","Embedded systems; Quality assurance; Secure programming; Security assurance; Software vulnerability","Quality assurance; Reliability; Behavior prediction; Catastrophic event; Hardware and software components; Secure programming; Secure software; Security assurance; Software vulnerabilities; Vulnerability discovery; Embedded systems","Conference paper","Final","","Scopus","2-s2.0-84886880586"
"Chen K.; Feng D.-G.; Su P.-R.; Nie C.-J.; Zhang X.-F.","Chen, Kai (57051675000); Feng, Deng-Guo (7401981154); Su, Pu-Rui (8941785000); Nie, Chu-Jiang (36606636600); Zhang, Xiao-Fei (55919504200)","57051675000; 7401981154; 8941785000; 36606636600; 55919504200","Multi-cycle vulnerability discovery model for prediction","2010","Ruan Jian Xue Bao/Journal of Software","21","9","","2367","2375","8","","10.3724/SP.J.1001.2010.03626","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957962023&doi=10.3724%2fSP.J.1001.2010.03626&partnerID=40&md5=b1cda28a707f478b3b5f0ddb5f06b1a7","This paper presents a multi-cycle vulnerability discovery model which shows the vulnerability discovery process and the relationship between the number of vulnerabilities and their release time. The model makes use of a cycle, which expands the scope of old models. A method is proposed to compute the parameters of this model to fit the discover process of the target software. Different rules are also given to find the initial values. Experiments are made on eight Windows operating systems. The results show that this model is more effective and more accurate than current models. © by Institute of Software, the Chinese Academy of Sciences. All rights reserved.","CSF; Model; Multi-cycle; Predict; Vulnerability","Windows operating system; CSF; Current models; Initial values; Multi-cycle; Predict; Release time; Vulnerability; Vulnerability discovery; Forecasting","Article","Final","","Scopus","2-s2.0-77957962023"
"Sesetyan K.; Demircioglu M.B.; Zulfikar C.; Kamer Y.; Erdik M.","Sesetyan, K. (6505940026); Demircioglu, M.B. (6602557071); Zulfikar, C. (24472180100); Kamer, Y. (36026706300); Erdik, M. (7003962489)","6505940026; 6602557071; 24472180100; 36026706300; 7003962489","Regional earthquake shaking and loss assessment","2010","COST ACTION C26: Urban Habitat Constructions under Catastrophic Events - Proceedings of the Final Conference","","","","1037","1042","5","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861964889&partnerID=40&md5=82b4965157653beba27276487d501522","The Joint Research Activity 3 of EC FP6 NERIES Project has developed a methodology and software, ELER® - Earthquake Loss Estimation Routine, for rapid estimation of earthquake shaking and losses throughout the Euro-Mediterranean region. Recently, a new version (v2.0) of the software has been released. The multi-level methodology developed together with researchers from Imperial College, NORSAR and ETHZurich is capable of incorporating regional variability and sources of uncertainty stemming from ground motion predictions, fault finiteness, site modifications, inventory of physical and social elements and the associated vulnerability relationships. Loss estimation (damage, casualty and economic) is performed at different levels of sophistication (Level 0, 1 and 2) that are commensurate with the availability of inventory of human built environment (Loss Mapping). This paper elaborates on ground motion estimation and Level 0 and Level 1 loss estimation methodologies used in ELER®, also presenting examples from the validation studies conducted. © 2010 Taylor & Francis Group, London.","","Earthquakes; Motion estimation; Earthquake loss estimation; Earthquake shaking; Ground motion prediction; Human built environment; Imperial College; Level-1; Loss estimation; Rapid estimation; Regional variability; Research activities; Social elements; Sources of uncertainty; Validation study; Estimation","Conference paper","Final","","Scopus","2-s2.0-84861964889"
"Grandhi R.V.; Chandu S.V.L.; Rajagopalan H.; Fautheree D.","Grandhi, R.V. (7005161957); Chandu, S.V.L. (6602834943); Rajagopalan, H. (6603313410); Fautheree, D. (57208923689)","7005161957; 6602834943; 6603313410; 57208923689","Expert system for laser vulnerability analysis of aerospace structures","1992","AUTOTESTCON (Proceedings)","1992-September","","270129","79","87","8","","10.1109/AUTEST.1992.270129","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066138375&doi=10.1109%2fAUTEST.1992.270129&partnerID=40&md5=5a6f575a6da16cfe74e8cf4fb5a96f4f","The authors discuss the framework of a knowledge-based expert system for studying the survivability of the aerospace structures exposed to high energy lasers using VAASEL (vulnerability analysis of aerospace structures exposed to lasers) software. VAASEL is a synthesis tool built around NASTRAN and ASTROS programs. The knowledge base involves threat characterization, temperature distribution, failure prediction, linear and nonlinear statics, air loads and aeroelastic disciplines. A description of the VAASEL expert system modules, the graphics interface and the input data generator is included. © 1992 IEEE.","","Automatic testing; Expert systems; Aerospace structure; Failure prediction; Graphics interface; Knowledge-based expert systems; Nonlinear statics; Synthesis tool; System modules; Vulnerability analysis; High energy lasers","Conference paper","Final","","Scopus","2-s2.0-85066138375"
"Joh H.; Malaiya Y.K.","Joh, HyunChul (26648448600); Malaiya, Yashwant K. (35619571700)","26648448600; 35619571700","Seasonality in vulnerability discovery in major software systems","2008","Proceedings - International Symposium on Software Reliability Engineering, ISSRE","","","4700344","297","298","1","","10.1109/ISSRE.2008.31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67249135819&doi=10.1109%2fISSRE.2008.31&partnerID=40&md5=3600b7efb2963732dada3c50170ee9ae","Prediction of vulnerability discovery rates can be used to assess security risks and to determine the resources needed to develop patches quickly to handle vulnerabilities discovered. An examination of the vulnerability data suggests a seasonal behavior that has not been modeled by the recently proposed vulnerability discovery models. This seasonality has not been identified or examined so far. This study examines whether vulnerability discovery rates for Windows NT,IIS Server and the Internet Explorer exhibit a significant annual seasonal pattern. Actual data has been analyzed using seasonal index and autocorrelation function approaches to identify seasonality and to evaluate its statistical significance. The results for the three software systems show that there is indeed a significant annual seasonal pattern. ©2008 IEEE.","","Computer software selection and evaluation; Regression analysis; Risk assessment; Risk management; Software reliability; Windows operating system; Autocorrelation functions; Internet explorers; Seasonal patterns; Seasonality; Security risks; Software systems; Statistical significance; Vulnerability discovery; Windows NT; Security of data","Conference paper","Final","","Scopus","2-s2.0-67249135819"
"Grunske L.; Joyce D.","Grunske, Lars (8966479000); Joyce, David (24721647100)","8966479000; 24721647100","Quantitative risk-based security prediction for component-based systems with explicitly modeled attack profiles","2008","Journal of Systems and Software","81","8","","1327","1345","18","","10.1016/j.jss.2007.11.716","https://www.scopus.com/inward/record.uri?eid=2-s2.0-50049117473&doi=10.1016%2fj.jss.2007.11.716&partnerID=40&md5=c3bcc674f82a75db139082a717d7cb61","Systems and software architects require quantitative dependability evaluations, which allow them to compare the effect of their design decisions on dependability properties. For security, however, quantitative evaluations have proven difficult, especially for component-based systems. In this paper, we present a risk-based approach that creates modular attack trees for each component in the system. These modular attack trees are specified as parametric constraints, which allow quantifying the probability of security breaches that occur due to internal component vulnerabilities as well as vulnerabilities in the component's deployment environment. In the second case, attack probabilities are passed between system components as appropriate to model attacks that exploit vulnerabilities in multiple system components. The probability of a successful attack is determined with respect to a set of attack profiles that are chosen to represent potential attackers and corresponding environmental conditions. Based on these attack probabilities and the structure of the modular attack trees, risk measures can be estimated for the complete system and compared with the tolerable risk demanded by stakeholders. The practicability of this approach is demonstrated with an example that evaluates the confidentiality of a distributed document management system. © 2007 Elsevier Inc. All rights reserved.","Component-based systems engineering; Composability; Confidentiality; Model-driven security evaluation; Parametric constraints; Privacy; Quantitative evaluation; Risk; Secrecy; SysML","Information retrieval systems; Information services; Probability; Random processes; Risk perception; Security of data; Software design; Complete systems; Component-based systems; Component-based systems engineering; Composability; Confidentiality; Design decisions; Distributed document management; Environmental conditioning; Model-driven security evaluation; Parametric constraints; Privacy; Quantitative evaluation; Quantitative risk; Risk; Risk based approaches; Risk measures; Secrecy; Security breaches; SysML; System components; Systems-and-software; Risk assessment","Article","Final","","Scopus","2-s2.0-50049117473"
"Johnson G.; Shalaev R.; Oates C.; Swaszek P.F.; Hartnett R.","Johnson, Gregory (15622861600); Shalaev, Ruslan (12790997400); Oates, Christian (35569509000); Swaszek, Peter F. (7004284321); Hartnett, Richard (6603516740)","15622861600; 12790997400; 35569509000; 7004284321; 6603516740","BALOR model validity for the airport ASF mapping methodology","2006","Proceedings of the Institute of Navigation, National Technical Meeting","1","","","403","412","9","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645967144&partnerID=40&md5=252849947c97b98fd4b48f365ab126ac","In 2001, the Volpe National Transportation Systems Center completed an evaluation of the Global Positioning System (GPS) vulnerabilities and the potential impacts to transportation systems in the United States. One of the recommendations of this study was for the operation of backup system(s) to GPS; Loran C was identified as one possible backup system. The Federal Aviation Administration (FAA) has been leading a team consisting of members from industry, government, and academia to evaluate the future of Loran-C in the United States. In a recently completed Navigation Transition Study, the FAA concluded that Loran-C, as an independent radionavigation system, is theoretically the best backup for the GPS; however, in order for Loran-C to be considered a viable back-up system to GFS, it must be able to meet the requirements for non-precision approaches (NPA's) for the aviation community and the Harbor Entrance and Approach (HEA) requirements for the maritime community. A significant factor limiting the accuracy of a Loran system is the spatial and temporal variation in the times of arrival (TOAs) observed by the receiver. A significant portion of these variations is due to the signals propagating over paths of varying conductivity; these TOA corrections which compensate for propagating over non-seawater paths are called additional secondary factors (ASFs). Hence, a key component in evaluating the utility of Loran as a GPS backup is a better understanding of ASFs and a key goal is deciding how to mitigate the effects of ASFs to achieve more accurate Loran-C positions while ensuring that the possibility of providing hazardous and misleading information (HMI) will be no greater than 1×10-7. For an aviation receiver, the approach to mitigate propagation issues under study is to use a single set of ASF values (one for each Loran tower) for a given airport This value may have seasonal adjustments applied to it. The Loran receiver will use this set of static ASF values to improve position accuracy when conducting a non-precision approach (NPA). A Working Group is currently developing the procedures to be used to ""map"" the ASF values for an airport The output of the Working Group will be a set of tested and documented procedures for conducting an airport survey; these procedures can then be followed to survey airports nationwide. The draft procedure has been tested during data collection efforts at airports in Maine, Ohio, and New Jersey. A key component of the proposed procedure is the use of the BALOR ASF prediction software to reduce the number of field measurements. ASF measurements made on the ground along the airport approaches and in the air on long baselines to and from several Loran towers are used to compare to the BALOR predictions to determine the validity of the BALOR model. This paper discusses the results of this data collection: how well the measured spatial variations match the BALOR model predictions, how well the proposed mapping procedure works, and results of the position accuracy obtained by the aircraft flying approaches when using the airport ASF values.","","Airports; Aviation; Navigation systems; Public policy; Signal processing; Signal receivers; Social aspects; Transportation; Federal Aviation Administration (FAA); Mapping methodology; Potential impacts; Volpe National Transportation Systems; Global positioning system","Conference paper","Final","","Scopus","2-s2.0-33645967144"
"Alguliyev R.; Imamverdiyev Y.; Aliyev E.A.","Alguliyev, Rasim (58093951600); Imamverdiyev, Yadigar (35731194800); Aliyev, Elchin A. (57190688003)","58093951600; 35731194800; 57190688003","Thesis for concepts of forming the population and migration segment of national information space; [Milli informasiya minverted letter ekaninin inverted letter ehali vinverted letter e miqrasiya üzre seqmentinin formalaşdirilmasi konsepsiyasi üçün tezislinverted letter er]","2009","2009 International Conference on Application of Information and Communication Technologies, AICT 2009","","","5372529","","","","","10.1109/ICAICT.2009.5372529","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749311303&doi=10.1109%2fICAICT.2009.5372529&partnerID=40&md5=7e5a3efd6242c41035bbaf0a691bef31","National information space consists of the segments which are consist of the followings: distributed information resources which has different software-technical platform; telecommunication infrastructure and methods of coherency. These segments are being formed, coordinated and continuously developed by the scientific-technical conception. This process is the task of national security. On population and migration segment the personal data is gathered by the government, private and social sector. The following tasks are defined in conception: gathering personal data, protection, general principles and requirements; formation the segment, outline of coordination and development models; problems to be being solved and dependencies between them; coherency nodes of information resources and requirement for the new standards of the segment. ©2009 IEEE.","""Virtual migration""; Coherency-routing procedure; Commissioner on protection of personal data; Hierarchal registry of information resources; Intelligent e-service; Migration technologies; Multi-parametric analysis apparatus; Prediction and monitoring of migration; Transaction and query ""nodes""; Vulnerabilities of law on normative acts","Information science; Standardization; Technology; Commissioner on protection of personal data; E-services; Information resource; Intelligent e-service; Parametric analysis; Personal data; Population statistics","Conference paper","Final","","Scopus","2-s2.0-77749311303"
"Shin Y.; Williams L.","Shin, Yonghee (24492397200); Williams, Laurie (35565101900)","24492397200; 35565101900","Can traditional fault prediction models be used for vulnerability prediction?","2013","Empirical Software Engineering","18","1","","25","59","34","","10.1007/s10664-011-9190-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872295305&doi=10.1007%2fs10664-011-9190-8&partnerID=40&md5=fb57a3a14dfe639fc197c1f4434dba6e","Finding security vulnerabilities requires a different mindset than finding general faults in software - thinking like an attacker. Therefore, security engineers looking to prioritize security inspection and testing efforts may be better served by a prediction model that indicates security vulnerabilities rather than faults. At the same time, faults and vulnerabilities have commonalities that may allow development teams to use traditional fault prediction models and metrics for vulnerability prediction. The goal of our study is to determine whether fault prediction models can be used for vulnerability prediction or if specialized vulnerability prediction models should be developed when both models are built with traditional metrics of complexity, code churn, and fault history. We have performed an empirical study on a widely-used, large open source project, the Mozilla Firefox web browser, where 21% of the source code files have faults and only 3% of the files have vulnerabilities. Both the fault prediction model and the vulnerability prediction model provide similar ability in vulnerability prediction across a wide range of classification thresholds. For example, the fault prediction model provided recall of 83% and precision of 11% at classification threshold 0.6 and the vulnerability prediction model provided recall of 83% and precision of 12% at classification threshold 0.5. Our results suggest that fault prediction models based upon traditional metrics can substitute for specialized vulnerability prediction models. However, both fault prediction and vulnerability prediction models require significant improvement to reduce false positives while providing high recall. © 2011 Springer Science+Business Media, LLC.","Automated text classification; Complexity metrics; Fault prediction; Open source project; Software metrics; Vulnerability prediction","Classification (of information); Forecasting; Open systems; Security of data; Web browsers; Complexity metrics; Fault prediction; Open source projects; Software metrics; Text classification; Mathematical models","Article","Final","","Scopus","2-s2.0-84872295305"
"Fairweather R.G.","Fairweather, R. Graham (6603699704)","6603699704","Gas emissions and fuel economy of the light duty diesel truck","1977","SAE Technical Papers","","","","","","","","10.4271/770256","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072424269&doi=10.4271%2f770256&partnerID=40&md5=3123730de970043e26f70c97a40b3b74","A computer program is used to predict the effect of engine emissions, vehicle gearing, test weight and driving technique on light duty diesel truck emissions levels. The predictions are correlated with actual tests and the vulnerability of using steady state instead of instantaneous emission data studied. An outline of the development of an engine to meet the latest E.P.A. emission level requirement is given. It is concluded that the degree of emissions control required of an engine in order to meet specified legislation depends significantly on whether that engine is to be installed in a truck or a car. Copyright © Society of Automotive Engineers, Inc. 1977 All rights reserved.","","Automobile testing; Automotive engineering; Engines; Fuel economy; Software testing; Trucks; Degree of emissions; Diesel truck; Diesel truck emissions; Driving techniques; Emission data; Emission level; Engine emission; Steady state; Gas emissions","Conference paper","Final","","Scopus","2-s2.0-85072424269"
"Schmidt A.G.; Walters J.P.; Zick K.M.; French M.; Keymeulen D.; Aranki N.; Klimesh M.; Kiely A.","Schmidt, Andrew G. (24470156800); Walters, John Paul (16306085600); Zick, Kenneth M. (26653914500); French, Matthew (7202595245); Keymeulen, Didier (6701790152); Aranki, Nazeeh (6507642635); Klimesh, Matthew (6603431783); Kiely, Aaron (6603663407)","24470156800; 16306085600; 26653914500; 7202595245; 6701790152; 6507642635; 6603431783; 6603663407","Applying radiation hardening by software to fast lossless compression prediction on FPGAs","2012","IEEE Aerospace Conference Proceedings","","","6187254","","","","","10.1109/AERO.2012.6187254","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861144334&doi=10.1109%2fAERO.2012.6187254&partnerID=40&md5=b59fdd672141fd7bba55b60cf941b636","As scientists endeavor to learn more about the world's ecosystems, engineers are pushed to develop more sophisticated instruments. With these advancements comes an increase in the amount of data generated. For satellite based instruments the additional data requires sufficient bandwidth be available to transmit the data. Alternatively, compression algorithms can be employed to reduce the bandwidth requirements. This work is motivated by the proposed HyspIRI mission, which includes two imaging spectrometers measuring from visible to short wave infrared (VSWIR) and thermal infrared (TIR) that saturate the projected bandwidth allocations. We present a novel investigation into the capability of using FPGAs integrated with embedded PowerPC processors to adequately perform the predictor function of the Fast Lossless (FL) compression algorithm for multispectral and hyperspectral imagery. Furthermore, our design includes a multi-PowerPC implementation which incorporates recently developed Radiation Hardening by Software (RHBSW) techniques to provide software-based fault tolerance to commercial FPGA devices. Our results show low performance overhead (4-8%) while achieving a speedup of 1.97x when utilizing both PowerPCs. Finally, the evaluation of the proposed system includes resource utilization, performance metrics, and an analysis of the vulnerability to Single Event Upsets (SEU) through the use of a hardware based fault injector. © 2012 IEEE.","","Algorithms; Bandwidth; Computer control systems; Fault tolerance; Image compression; Infrared radiation; Radiation hardening; Remote sensing; Bandwidth requirement; Compression algorithms; Fault injector; FPGA devices; Hyperspectral imagery; Imaging spectrometers; Lossless; Lossless compression; Multi-spectral; Performance metrics; PowerPC processors; Resource utilizations; Short wave infrared; Single event upsets; Software-based; Thermal infrared; Bandwidth compression","Conference paper","Final","","Scopus","2-s2.0-84861144334"
"Dunn P.E.; Hacker W.L.; Hikida S.","Dunn, Peter E. (7401710275); Hacker, William L. (7004518867); Hikida, Shuichi (6701344303)","7401710275; 7004518867; 6701344303","Blast methodology in the modular effectiveness vulnerability assessment (MEVA) code","1997","American Society of Mechanical Engineers, Pressure Vessels and Piping Division (Publication) PVP","351","","","127","134","7","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030710313&partnerID=40&md5=869c07b1b1be432ab93226c97473f167","MEVA is a computer program which assesses the vulnerability of buildings subjected to conventional weapon attack. An airblast methodology has been developed for implementation within MEVA which captures the features of multiple reflections off walls, ceilings, and floors and venting through doorways without the computational expense of hydrocode calculations. This airblast methodology uses the TNT standard for modeling free-field airblast propagation, non-linear shock addition rules from the Low Altitude Multiple Burst (LAMB) code, and shock diffraction algorithms from the CHAMBER code. ""Particle propagation,"" an approach which places tracer volumes throughout the target volume to track the geometric progress of the airblast shock front, is used to recognize interaction with the building geometry. Predictions from the TNT standard are compared to experimental results from Dipole West Shot 6. Results from a 750 1b. tritonol explosive test external to a three-story, reinforced concrete building will be used to verify this airblast methodology.","","Algorithms; Blast resistance; Computer aided analysis; Computer simulation; Computer software; Risk assessment; Shock waves; Structural analysis; Software package LAMB; Software package MEVA; Buildings","Article","Final","","Scopus","2-s2.0-0030710313"
"Zhai Y.; Ouyang Q.","Zhai, Yongmei (42361750700); Ouyang, Qianwen (56181220100)","42361750700; 56181220100","Application of GIS and RS techniques in rapid seismic damage prediction","2013","Communications in Computer and Information Science","398 PART I","","","88","94","6","","10.1007/978-3-642-45025-9_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901498819&doi=10.1007%2f978-3-642-45025-9_10&partnerID=40&md5=21069b695cf062d032c10a4df7d1c29c","In order to achieve rapid large-scale seismic damage prediction, this paper studied on relevant application of high-resolution remote sensing (RS) and GIS. As this technique takes RS image as information source, investigation and improvement of existing attribute information extraction methods has been done to realize automatic extraction and precise intelligent interpretation. Moreover, software platform integration of RS and GIS is also accomplished by secondary development for seamless data transmission and further operation in GIS. Furthermore, structural vulnerability analysis method has been introduced to GIS system for calculating damage degrees of various structure forms. Thus, intuitionistic prediction results are obtained in GIS interface. In addition, this paper took a case study on a county of Henan province to verify its reliability. © Springer-Verlag Berlin Heidelberg 2013.","GIS; Integration; Rapid seismic damage prediction; RS","Ecosystems; Geographic information systems; Integration; Natural resources management; Resource allocation; Structural analysis; Attribute information; Automatic extraction; High resolution remote sensing; Intelligent interpretation; RS; Secondary development; Seismic damage prediction; Structural vulnerability; Forecasting","Conference paper","Final","","Scopus","2-s2.0-84901498819"
"Aciiçmez O.; Gueron S.; Seifert J.-P.","Aciiçmez, Onur (14033907600); Gueron, Shay (35229308700); Seifert, Jean-Pierre (56227746000)","14033907600; 35229308700; 56227746000","New branch prediction vulnerabilities in OpenSSL and necessary software countermeasures","2007","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","4887 LNCS","","","185","203","18","","10.1007/978-3-540-77272-9_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149014637&doi=10.1007%2f978-3-540-77272-9_12&partnerID=40&md5=5268e73da519a23c08d5311cb9ce3c0f","Software based side-channel attacks allow an unprivileged spy process to extract secret information from a victim (cryptosystem) process by exploiting some indirect leakage of ""side-channel"" information. It has been realized that some components of modern computer microarchitectures leak certain sidechannel information and can create unforeseen security risks. An example of such MicroArchitectural Side-Channel Analysis is the Cache Attack -a group of attacks that exploit information leaks from cache latencies [4,7,13,15,18]. Public awareness of Cache Attack vulnerabilities lead software writers of OpenSSL (version 0.9.8a and subsequent versions) to incorporate countermeasures for preventing these attacks. In this paper, we present a new and yet unforeseen side channel attack that is enabled by the recently published Simple Branch Prediction Analysis (SBPA) which is another type of MicroArchitectural Analysis, cf. [2,3]. We show that modular inversion -a critical primitive in public key cryptography -is a natural target of SBPA attacks because it typically uses the Binary Extended Euclidean algorithm whose nature is an input-centric sequence of conditional branches. Our results show that SBPA can be used to extract secret parameters during the execution of the Binary Extended Euclidean algorithm. This poses a new potential risk to crypto-applications such as OpenSSL, which already employs Cache Attack countermeasures. Thus, it is necessary to develop new software mitigation techniques for BPA and incorporate them with cache analysis countermeasures in security applications. To mitigate this new risk in full generality, we apply a security-aware algorithm design methodology and propose some changes to the CRT-RSA algorithm flow. These changes either avoid some of the steps that require modular inversion, or remove the critical information leak from this procedure. In addition, we also show by example that, independently of the required changes in the algorithms, careful software analysis is also required in order to assure that the software implementation does not inadvertently introduce branches that may expose the application to SBPA attacks. These offer several simple ways for modifying OpenSSL in order to mitigate Branch Prediction Attacks. © Springer-Verlag Berlin Heidelberg 2007.","Binary extended euclidean algorithm; Branch prediction attacks; Cache eviction attacks; CRT; Modular inversion; OpenSSL; RSA; Side channel attacks; Software mitigation methods","Algorithms; Computer architecture; Computer software; Information dissemination; Security of data; Binary extended euclidean algorithm; Branch prediction attacks; Cache eviction attacks; Modular inversion; Side channel attacks; Software mitigation methods; Computer crime","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-38149014637"
"Chowdhury I.; Zulkernine M.","Chowdhury, Istehad (25722951700); Zulkernine, Mohammad (6506457317)","25722951700; 6506457317","Using complexity, coupling, and cohesion metrics as early indicators of vulnerabilities","2011","Journal of Systems Architecture","57","3","","294","313","19","","10.1016/j.sysarc.2010.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952574726&doi=10.1016%2fj.sysarc.2010.06.003&partnerID=40&md5=ddebef120b05b872d21e8545303f7883","Software security failures are common and the problem is growing. A vulnerability is a weakness in the software that, when exploited, causes a security failure. It is difficult to detect vulnerabilities until they manifest themselves as security failures in the operational stage of software, because security concerns are often not addressed or known sufficiently early during the software development life cycle. Numerous studies have shown that complexity, coupling, and cohesion (CCC) related structural metrics are important indicators of the quality of software architecture, and software architecture is one of the most important and early design decisions that influences the final quality of the software system. Although these metrics have been successfully employed to indicate software faults in general, there are no systematic guidelines on how to use these metrics to predict vulnerabilities in software. If CCC metrics can be used to indicate vulnerabilities, these metrics could aid in the conception of more secured architecture, leading to more secured design and code and eventually better software. In this paper, we present a framework to automatically predict vulnerabilities based on CCC metrics. To empirically validate the framework and prediction accuracy, we conduct a large empirical study on fifty-two releases of Mozilla Firefox developed over a period of four years. To build vulnerability predictors, we consider four alternative data mining and statistical techniques - C4.5 Decision Tree, Random Forests, Logistic Regression, and Naïve-Bayes - and compare their prediction performances. We are able to correctly predict majority of the vulnerability-prone files in Mozilla Firefox, with tolerable false positive rates. Moreover, the predictors built from the past releases can reliably predict the likelihood of having vulnerabilities in the future releases. The experimental results indicate that structural information from the non-security realm such as complexity, coupling, and cohesion are useful in vulnerability prediction. © 2010 Elsevier B.V. All rights reserved.","Cohesion; Complexity; Coupling; Software metrics; Vulnerability prediction","Adhesion; Computer software selection and evaluation; Decision trees; Forecasting; Software architecture; Software design; Web browsers; Cohesion; Cohesion metrics; Complexity; Design decisions; Empirical studies; False positive rates; Firefox; Logistic regressions; Mozilla; Operational stages; Prediction accuracy; Prediction performance; Random forests; Security failure; Software development life cycle; Software fault; Software metrics; Software security; Software systems; Statistical techniques; Structural information; Structural metrics; Vulnerability prediction; Security of data","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-79952574726"
"Firouzi F.; Azarpeyvand A.; Salehi M.E.; Fakhraie S.M.","Firouzi, Farshad (36056471000); Azarpeyvand, Ali (36056132800); Salehi, Mostafa E. (57194140135); Fakhraie, Sied Mehdi (6603636269)","36056471000; 36056132800; 57194140135; 6603636269","Adaptive fault-tolerant DVFS with dynamic online AVF prediction","2012","Microelectronics Reliability","52","6","","1197","1208","11","","10.1016/j.microrel.2012.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861529968&doi=10.1016%2fj.microrel.2012.01.005&partnerID=40&md5=411911b0807f73ff95a9eb881dd8cea1","Advances in silicon technology and shrinking the feature size to nanometer levels make random variations and low reliability of nano-devices the most important concern for fault-tolerant design. Design of reliable and fault-tolerant embedded processors is mostly based on developing techniques that compensate reliability shortcomings by adding hardware or software redundancy. The recently-proposed redundancy adding techniques are generally applied uniformly to all parts of a system and lead to heavy overheads and inefficiencies in terms of performance, power, and area. Efficient employment of non-uniform redundancy becomes possible when a quantitative analysis of a system behavior while encountering transient faults is provided. In this work, we present a quantitative analysis of the behavior of an embedded processor regarding transient faults and propose a new approach that accurately predicts the architecture vulnerability factor (AVF) in real-time. Another critical concern in design of new-silicon processors is power consumption issue. Dynamic voltage and frequency scaling (DVFS) is an effective method for controlling both energy consumption and performance of a system. Since rate of radiation-induced transient faults depends on operating frequency and supply voltage, DVFS techniques are recently shown to have compromising effects on electronic system reliability. Therefore, ignoring the effects of voltage scaling on fault rate could considerably degrade the system reliability. Here, by exploiting the proposed online AVF prediction methodology and based on analytic derivation, we propose a reliability-aware adaptive dynamic voltage and frequency scaling (DVFS) approach in case study of Multi-Processor System on Chip (MPSoC) with Multiple Clock Domain (MCD) pipeline architectures in which the frequency and voltage are scaled by simultaneously considering all three of power consumption, reliability, and performance. Comparing to the traditional methods of reliability-aware DVFS systems, the proposed reliability-aware DVFS method yields 50% better power saving at the same reliability level. © 2011 Elsevier Ltd. All rights reserved.","","Computer architecture; Design; Energy utilization; Microprocessor chips; Network architecture; Power quality; Radiation effects; Software reliability; Adaptive dynamics; Dynamic voltage and frequency scaling; Electronic systems; Embedded processors; Fault rates; Fault-tolerant; Feature sizes; Multiple clock domains; Multiprocessor-system; Nano-devices; Nanometer level; Operating frequency; Pipeline architecture; Power savings; Prediction methodology; Radiation-induced; Random variation; Reliability level; Silicon Technologies; Supply voltages; System behaviors; System reliability; Transient faults; Voltage-scaling; Redundancy","Article","Final","","Scopus","2-s2.0-84861529968"
"Shar L.K.; Tan H.B.K.","Shar, Lwin Khin (36802571200); Tan, Hee Beng Kuan (7403011293)","36802571200; 7403011293","Mining input sanitization patterns for predicting SQL injection and cross site scripting vulnerabilities","2012","Proceedings - International Conference on Software Engineering","","","6227096","1293","1296","3","","10.1109/ICSE.2012.6227096","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864239564&doi=10.1109%2fICSE.2012.6227096&partnerID=40&md5=74a7648ecf132e70b1f38ee871c84af7","Static code attributes such as lines of code and cyclomatic complexity have been shown to be useful indicators of defects in software modules. As web applications adopt input sanitization routines to prevent web security risks, static code attributes that represent the characteristics of these routines may be useful for predicting web application vulnerabilities. In this paper, we classify various input sanitization methods into different types and propose a set of static code attributes that represent these types. Then we use data mining methods to predict SQL injection and cross site scripting vulnerabilities in web applications. Preliminary experiments show that our proposed attributes are important indicators of such vulnerabilities. © 2012 IEEE.","data mining; defect prediction; input sanitization; static code attributes; web security vulnerabilities","Data mining; Defects; Forecasting; Security of data; Software engineering; Cross site scripting; Cyclomatic complexity; Data mining methods; Defect prediction; Lines of code; Sanitization; Software modules; SQL injection; Static codes; WEB application; Web application vulnerability; WEB security; World Wide Web","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84864239564"
"Santos J.C.M.; Yunsi F.","Santos, Juan Carlos Martinez (26325154200); Yunsi, Fei (7103059457)","26325154200; 7103059457","Leveraging speculative architectures for run-time program validation","2008","26th IEEE International Conference on Computer Design 2008, ICCD","","","4751907","498","505","7","","10.1109/ICCD.2008.4751907","https://www.scopus.com/inward/record.uri?eid=2-s2.0-62349095013&doi=10.1109%2fICCD.2008.4751907&partnerID=40&md5=2a4db42003bb5b6696105eeb6458f3d8","Program execution can be tampered by malicious attackers through exploiting software vulnerabilities. Changing the program behavior by compromising control data and decision data has become the most serious threat to computer systems security. Although several hardware approaches have been presented to validate program execution, they mostly suffer great hardware area or poor ambiguity handling. In this paper, we propose a new hardware-based approach by leveraging the existing speculative architectures for run-time program validation. The on-chip branch target buffer (BTB) is utilized as a cache of the legitimate control flow transfers stored in a secure memory region. In addition, the BTB is extended to store the correct program path information. At each indirect branch site, the BTB is used to validate the decision history of conditional branches before it, and more information about the future decision path is fetched to monitor the execution path at run-time. Implementation of this approach is transparent to the upper operating system and programs. Thus, it is applicable to legacy code. Due to good code locality of the executable programs and effectiveness of branch prediction, the frequency of run-time control flow validations against the secure off-chip memory is low. Our experimental results show a negligible performance penalty and small storage overhead with ambiguity reduced. © 2008 IEEE.","","Data storage equipment; Soil structure interactions; Web services; Branch predictions; Branch target buffers; Conditional branches; Control datum; Control flows; Decision datum; Decision paths; Executable programs; Execution paths; Good codes; Hardware-based approaches; Indirect branches; Legacy codes; Memory regions; Off-chip memories; On chips; Operating systems; Performance penalties; Program behaviors; Program executions; Program validations; Run-time; Software vulnerabilities; Storage overheads; Concurrency control","Conference paper","Final","","Scopus","2-s2.0-62349095013"
"Shin Y.; Williams L.","Shin, Yonghee (24492397200); Williams, Laurie (35565101900)","24492397200; 35565101900","Is complexity really the enemy of software security?","2008","Proceedings of the ACM Conference on Computer and Communications Security","","","","47","50","3","","10.1145/1456362.1456372","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349263324&doi=10.1145%2f1456362.1456372&partnerID=40&md5=bbdf5c57078989e57156860909465e65","Software complexity is often hypothesized to be the enemy of software security. We performed statistical analysis on nine code complexity metrics from the JavaScript Engine in the Mozilla application framework to investigate if this hypothesis is true. Our initial results show that the nine complexity measures have weak correlation (ρ=0.30 at best) with security problems for Mozilla JavaScript Engine. The study should be replicated on more products with design and code-level metrics. It may be necessary to create new complexity metrics to embody the type of complexity that leads to security problems. Copyright 2008 ACM.","Fault prediction; Reliability; Security metrics; Software complexity; Software metrics; Vulnerability prediction","Computer software selection and evaluation; High level languages; Metric system; Nonlinear control systems; Software engineering; Software reliability; Fault prediction; Security metrics; Software complexity; Software metrics; Vulnerability prediction; Security of data","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-70349263324"
"Schmidt A.G.; French M.","Schmidt, Andrew G. (24470156800); French, Matthew (7202595245)","24470156800; 7202595245","Fast lossless image compression with Radiation Hardening by hardware/software co-design on platform FPGAs","2013","Proceedings of the International Conference on Application-Specific Systems, Architectures and Processors","","","6567560","103","106","3","","10.1109/ASAP.2013.6567560","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883330784&doi=10.1109%2fASAP.2013.6567560&partnerID=40&md5=b230689c209ce8335a939188ee1bc373","Motivated by the proposed NASA HyspIRI mission, our work improves existing Radiation Hardening by Software (RHBSW) techniques with FPGA Fabric Checkpoint/Restart (F2CPR) to bring enhanced hardware/software co-designed fault tolerance to commercial FPGA devices. We evaluate our approach on Fast Lossless (FL) image compression prediction for hyperspectral imagery in order to meet real-time performance requirements that cannot be achieved with aging radiation hardened devices. We report results across several metrics including resource utilization, performance, and an analysis of the vulnerability to Single Event Upsets (SEU) through the use of a hardware based fault injector. Results show low performance overhead (4-8%) achieving a speedup of 11.28× with a hardware accelerated implementation. © 2013 IEEE.","","Computer architecture; Computer control systems; Hardware; Image compression; NASA; Radiation hardening; Spectroscopy; Hardware-accelerated; Hardware/software co-design; Hyper-spectral imageries; Lossless image compression; Radiation hardening by softwares; Radiation-hardened devices; Real time performance; Resource utilizations; Field programmable gate arrays (FPGA)","Conference paper","Final","","Scopus","2-s2.0-84883330784"
"Rahimi S.; Zargham M.","Rahimi, Sanaz (7004058733); Zargham, Mehdi (6701798931)","7004058733; 6701798931","Vulnerability scrying method for software vulnerability discovery prediction without a vulnerability database","2013","IEEE Transactions on Reliability","62","2","6502762","395","407","12","","10.1109/TR.2013.2257052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878706990&doi=10.1109%2fTR.2013.2257052&partnerID=40&md5=ab7c8d6a8ac3d1ef27d87c0387837a00","Predicting software vulnerability discovery trends can help improve secure deployment of software applications and facilitate backup provisioning, disaster recovery, diversity planning, and maintenance scheduling. Vulnerability discovery models (VDMs) have been studied in the literature as a means to capture the underlying stochastic process. Based on the VDMs, a few vulnerability prediction schemes have been proposed. Unfortunately, all these schemes suffer from the same weaknesses: they require a large amount of historical vulnerability data from a database (hence they are not applicable to a newly released software application), their precision depends on the amount of training data, and they have significant amount of error in their estimates. In this work, we propose vulnerability scrying, a new paradigm for vulnerability discovery prediction based on code properties. Using compiler-based static analysis of a codebase, we extract code properties such as code complexity (cyclomatic complexity), and more importantly code quality (compliance with secure coding rules), from the source code of a software application. Then we propose a stochastic model which uses code properties as its parameters to predict vulnerability discovery. We have studied the impact of code properties on the vulnerability discovery trends by performing static analysis on the source code of four real-world software applications. We have used our scheme to predict vulnerability discovery in three other software applications. The results show that even though we use no historical data in our prediction, vulnerability scrying can predict vulnerability discovery with better precision and less divergence over time. © 2012 IEEE.","Code security; static analysis; vulnerability discovery model; vulnerability prediction","Application programs; Forecasting; Network security; Random processes; Code security; Cyclomatic complexity; Maintenance scheduling; Prediction schemes; Software applications; Software vulnerabilities; Vulnerability database; Vulnerability discovery; Static analysis","Article","Final","","Scopus","2-s2.0-84878706990"
"Zimmermann T.; Nagappan N.; Williams L.","Zimmermann, Thomas (16308551800); Nagappan, Nachiappan (8261920700); Williams, Laurie (35565101900)","16308551800; 8261920700; 35565101900","Searching for a needle in a haystack: Predicting security vulnerabilities for Windows Vista","2010","ICST 2010 - 3rd International Conference on Software Testing, Verification and Validation","","","5477059","421","428","7","","10.1109/ICST.2010.32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954485601&doi=10.1109%2fICST.2010.32&partnerID=40&md5=0891cbffaa7765c1491c944865649fa6","Many factors are believed to increase the vulnerability of software system; for example, the more widely deployed or popular is a software system the more likely it is to be attacked. Early identification of defects has been a widely investigated topic in software engineering research. Early identification of software vulnerabilities can help mitigate these attacks to a large degree by focusing better security verification efforts in these components. Predicting vulnerabilities is complicated by the fact that vulnerabilities are, most often, few in number and introduce significant bias by creating a sparse dataset in the population. As a result, vulnerability prediction can be thought of us preverbally ""searching for a needle in a haystack."" In this paper, we present a large-scale empirical study on Windows Vista, where we empirically evaluate the efficacy of classical metrics like complexity, churn, coverage, dependency measures, and organizational structure of the company to predict vulnerabilities and assess how well these software measures correlate with vulnerabilities. We observed in our experiments that classical software measures predict vulnerabilities with a high precision but low recall values. The actual dependencies, however, predict vulnerabilities with a lower precision but substantially higher recall. © 2010 IEEE.","Churn; Complexity; Coverage; Dependencies; Metrics; Organizational structure; Prediction; Vulnerabilities","Computer operating systems; Computer software selection and evaluation; Forecasting; Needles; Population statistics; Software testing; Verification; Data sets; Empirical studies; High precision; Lower precision; Organizational structures; Security verification; Security vulnerabilities; Software measures; Software systems; Software vulnerabilities; Windows Vista; Security of data","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-77954485601"
